{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/scar3crow/Dropbox/WorkStation-Subrata/python/models/research/object_detection\n"
     ]
    }
   ],
   "source": [
    "cd models/research/object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tqdm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import *\n",
    "\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications import InceptionResNetV2\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = [480, 480]\n",
    "target_w = 480 # target sizes of image in model input\n",
    "target_h = 480 #target sizes of image in model input\n",
    "\n",
    "grid_size = [15, 15]\n",
    "grid_y_axis = 15  # each image is to be segmented to 13 x 13 grid\n",
    "grid_x_axis = 15  # # each image is to be segmented to 13 x 13 grid\n",
    "\n",
    "grid_w = target_w / grid_x_axis  # grid cell width\n",
    "grid_h = target_h / grid_y_axis  # grid cell height\n",
    "\n",
    "channels = 3\n",
    "num_anchors = 3\n",
    "class_num = 5 # vendor, invoice, inv_date, po, buyer\n",
    "info = 5 + class_num    # pc, x, y, h, w, and class probabilities\n",
    "\n",
    "categories = ['vendor', 'invoice', 'inv_date', 'po', 'buyer'] # details of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images =  36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/scar3crow/Downloads/8-6-new-scan/50a.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a list of image path\n",
    "\n",
    "inv_directory = '/home/scar3crow/Downloads/8-6-new-scan'  ## 'invoices' is a zip file of jpg images in ...../Downloads \n",
    "                                                        \n",
    "inv_new_image = ['/home/scar3crow/Downloads/8-6-new-scan/{}'.format(i) for i in os.listdir(inv_directory)] # making the list\n",
    "inv_new_image.sort() # Sorting the list\n",
    "\n",
    "num_images = len(inv_new_image)\n",
    "\n",
    "print('Number of images = ', num_images)\n",
    "inv_new_image[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_serial</th>\n",
       "      <th>rows</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/101a.jpg</td>\n",
       "      <td>160</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/102a.jpg</td>\n",
       "      <td>406</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/103a.jpg</td>\n",
       "      <td>260</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      image_serial  rows  columns\n",
       "0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg   160      416\n",
       "1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg   406      870\n",
       "2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   260      416"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check sizes of exiting images & Create a Dataframe with image id and height(row) and width(column):\n",
    "\n",
    "rows = []\n",
    "columns = []\n",
    "image_sl = []\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "for i in range(len(inv_new_image)):\n",
    "    image = cv2.imread(inv_new_image[i]) ## Loading image\n",
    "    height, width, _ = image.shape\n",
    "    rows.append(height)\n",
    "    columns.append(width)\n",
    "    image_sl.append(inv_new_image[i])\n",
    "    \n",
    "row_values = pd.Series(rows)\n",
    "col_values = pd.Series(columns)\n",
    "image_num = pd.Series(image_sl)\n",
    "\n",
    "\n",
    "df_new.insert(loc=0, column='image_serial', value=image_num)\n",
    "df_new.insert(loc=1, column='rows', value=row_values)\n",
    "df_new.insert(loc=2, column='columns', value=col_values)\n",
    "\n",
    "df_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes =  5\n",
      "Number of unique images =  36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#filename</th>\n",
       "      <th>region_shape_attributes</th>\n",
       "      <th>region_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>63a.jpg</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":211,\"y\":64,\"width\":76,\"heig...</td>\n",
       "      <td>{\"text\":\"po\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>63a.jpg</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":2,\"y\":68,\"width\":165,\"heigh...</td>\n",
       "      <td>{\"text\":\"buyer\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>101a.jpg</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":6,\"y\":23,\"width\":119,\"heigh...</td>\n",
       "      <td>{\"text\":\"vendor\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #filename                            region_shape_attributes  \\\n",
       "58   63a.jpg  {\"name\":\"rect\",\"x\":211,\"y\":64,\"width\":76,\"heig...   \n",
       "59   63a.jpg  {\"name\":\"rect\",\"x\":2,\"y\":68,\"width\":165,\"heigh...   \n",
       "60  101a.jpg  {\"name\":\"rect\",\"x\":6,\"y\":23,\"width\":119,\"heigh...   \n",
       "\n",
       "    region_attributes  \n",
       "58      {\"text\":\"po\"}  \n",
       "59   {\"text\":\"buyer\"}  \n",
       "60  {\"text\":\"vendor\"}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading output of VGG Image Annotation tool and create a dataframe\n",
    "\n",
    "r_new_data = pd.read_csv('/home/scar3crow/Downloads/via_new_data.csv')\n",
    "num_obj = r_new_data['region_count'][0] # number of objects in each photo\n",
    "r_new_data.drop(r_new_data.columns[[1, 2, 3, 4]], axis=1, inplace=True) # reduce unnecessary columns\n",
    "r_new_data.sort_values(by=['#filename'], ascending=True) # Sorting based on image-id\n",
    "num_images = r_new_data[\"#filename\"].nunique() # Find out number of unique images\n",
    "\n",
    "print('Number of classes = ', num_obj)\n",
    "print('Number of unique images = ', num_images)\n",
    "r_new_data[58:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>img_idx</th>\n",
       "      <th>i_path</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>obj_class</th>\n",
       "      <th>img_wd</th>\n",
       "      <th>img_ht</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50a.jpg</td>\n",
       "      <td>24</td>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/50a.jpg</td>\n",
       "      <td>221</td>\n",
       "      <td>59</td>\n",
       "      <td>103</td>\n",
       "      <td>24</td>\n",
       "      <td>po</td>\n",
       "      <td>416</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50a.jpg</td>\n",
       "      <td>24</td>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/50a.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>206</td>\n",
       "      <td>56</td>\n",
       "      <td>buyer</td>\n",
       "      <td>416</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51a.jpg</td>\n",
       "      <td>25</td>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/51a.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>56</td>\n",
       "      <td>vendor</td>\n",
       "      <td>416</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    img_id  img_idx                                          i_path    x   y  \\\n",
       "3  50a.jpg       24  /home/scar3crow/Downloads/8-6-new-scan/50a.jpg  221  59   \n",
       "4  50a.jpg       24  /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5  57   \n",
       "5  51a.jpg       25  /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    5   0   \n",
       "\n",
       "   width  height obj_class  img_wd  img_ht  \n",
       "3    103      24        po     416     209  \n",
       "4    206      56     buyer     416     209  \n",
       "5    120      56    vendor     416     194  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a dataframe for Image_id, x, y, width, height, class, image_width and image_height\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "width = []\n",
    "height = []\n",
    "obj_class = []\n",
    "i_width = []\n",
    "i_height = []\n",
    "img_path = []\n",
    "img_index = []\n",
    "\n",
    "for i in range(len(r_new_data)):\n",
    "    \n",
    "    r_size = r_new_data.values[i, 1][1:(len(r_new_data.values[i, 1])-1)]\n",
    "    r_size_par = r_size.split(\",\")\n",
    "    \n",
    "    x.append(int(\"\".join(filter(str.isdigit, r_size_par[1]))))\n",
    "    y.append(int(\"\".join(filter(str.isdigit, r_size_par[2]))))\n",
    "    width.append(int(\"\".join(filter(str.isdigit, r_size_par[3]))))\n",
    "    height.append(int(\"\".join(filter(str.isdigit, r_size_par[4]))))\n",
    "    \n",
    "    r_attribs = r_new_data.values[i, 2][1:(len(r_new_data.values[i, 2])-1)]\n",
    "    r_attribs_par = r_attribs.split(':')[1]\n",
    "    obj_class.append(r_attribs_par[1:(len(r_attribs_par)-1)])\n",
    "    \n",
    "    foto_id = r_new_data['#filename'][i]\n",
    "    i_path = '/home/scar3crow/Downloads/8-6-new-scan/' + foto_id\n",
    "    foto_index = int(df_new[df_new['image_serial'] == i_path].index[0])\n",
    "    foto_width = df_new['columns'][foto_index]\n",
    "    foto_height = df_new['rows'][foto_index]\n",
    "    i_width.append(foto_width)\n",
    "    i_height.append(foto_height)\n",
    "    img_path.append(i_path)\n",
    "    img_index.append(foto_index)\n",
    "    \n",
    "x_values = pd.Series(x)\n",
    "y_values = pd.Series(y)\n",
    "width_values = pd.Series(width)\n",
    "height_values = pd.Series(height)\n",
    "class_values = pd.Series(obj_class)\n",
    "i_width_values = pd.Series(i_width)\n",
    "i_height_values = pd.Series(i_height)\n",
    "img_path_values = pd.Series(img_path)\n",
    "img_index_values = pd.Series(img_index)\n",
    "\n",
    "r_new_data.insert(loc=1, column='img_idx', value=img_index_values)\n",
    "r_new_data.insert(loc=2, column='i_path', value=img_path_values)\n",
    "r_new_data.insert(loc=3, column='x', value=x_values)\n",
    "r_new_data.insert(loc=4, column='y', value=y_values)\n",
    "r_new_data.insert(loc=5, column='width', value=width_values)\n",
    "r_new_data.insert(loc=6, column='height', value=height_values)\n",
    "r_new_data.insert(loc=7, column='obj_class', value=class_values)\n",
    "r_new_data.insert(loc=8, column='img_wd', value=i_width_values)\n",
    "r_new_data.insert(loc=9, column='img_ht', value=i_height_values)\n",
    "\n",
    "r_new_data.drop(r_new_data.columns[[10, 11]], axis=1, inplace=True) # reduce unnecessary columns\n",
    "\n",
    "r_new_data.rename({'#filename': 'img_id'}, axis=1, inplace=True) # changing column name\n",
    "\n",
    "r_new_data[3:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images =  36\n",
      "Number of classes in diff. categories =  buyer      38\n",
      "invoice    36\n",
      "vendor     36\n",
      "date       36\n",
      "po         33\n",
      "order       1\n",
      "Name: obj_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique images = ', r_new_data['img_id'].nunique())  # print total no, of unique images\n",
    "print('Number of classes in diff. categories = ', r_new_data['obj_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[       img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to correct above :\n",
    "\n",
    "# To find smallest width & height boxes in 'buyer' which should be 'po'\n",
    "gb = r_new_data.groupby('obj_class')    \n",
    "[gb.get_group('buyer') for x in gb.groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images =  36\n",
      "Number of unique classes =  5\n",
      "Number of classes in diff. categories =  inv_date    36\n",
      "po          36\n",
      "invoice     36\n",
      "vendor      36\n",
      "buyer       36\n",
      "Name: obj_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Correcting above wrong spelling & converting buyer to po of object classes and rechecking\n",
    "\n",
    "id_1 = r_new_data.index[r_new_data['obj_class'] == 'order'] # Finding the index\n",
    "id_2 = r_new_data.index[r_new_data['obj_class'] == 'date'] # to change 'date' to 'inv_date' to be consistent with old data\n",
    "\n",
    "r_new_data.at[id_1, 'obj_class'] = 'po' # writing the correct spelling \n",
    "r_new_data.at[88, 'obj_class'] = 'po' # # 'buyer' to 'po'\n",
    "r_new_data.at[163, 'obj_class'] = 'po' # # 'buyer' to 'po'\n",
    "r_new_data.at[id_2, 'obj_class'] = 'inv_date' # # 'date' to 'inv_date'\n",
    "\n",
    "print('Number of unique images = ', r_new_data['img_id'].nunique())  # print total no, of unique images\n",
    "print('Number of unique classes = ', r_new_data['obj_class'].nunique())\n",
    "print('Number of classes in diff. categories = ', r_new_data['obj_class'].value_counts())\n",
    "\n",
    "# r_new_data.drop(r_new_data.columns[[0]], axis=1, inplace=True) # reduce unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each image, we have to find : (a) line_index = integer, (b) img_path = string, (c) boxes = shape [N, 4], \n",
    "## N is the ground truth count, elements in the second dimension are [x_min, y_min, x_max, y_max] (d) labels = shape\n",
    "## [N]. class index. (e) img_width = int.  =f) img_height = int\n",
    "\n",
    "def single_image_info(lines):\n",
    "    \n",
    "    ## lines will be a dataframe like, for i in range(num_images), lines = r_new_data[i*5:(i+1)*5]\n",
    "    \n",
    "    line_idx = lines.iat[0, 1]\n",
    "    pic_path = lines.iat[0, 2]\n",
    "    img_width = lines.iat[0, 8]\n",
    "    img_height = lines.iat[0, 9]\n",
    "    \n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for i in range(len(lines)):\n",
    "        label, x_min, y_min, x_max, y_max = int(i), float(lines.iat[i,3]), float(lines.iat[i,4]), float(lines.iat[i,3]+lines.iat[i,5]), float(lines.iat[i,4]+lines.iat[i,6])\n",
    "        boxes.append([x_min, y_min, x_max, y_max])\n",
    "        labels.append(label)\n",
    "        \n",
    "    boxes = np.asarray(boxes, np.float32)\n",
    "    labels = np.asarray(labels, np.int64)\n",
    "    \n",
    "    return line_idx, pic_path, boxes, labels, img_width, img_height  ## boxes are in format xmin, ymin, xmax, ymax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "[25, '/home/scar3crow/Downloads/8-6-new-scan/51a.jpg', array([[  5.,   0., 125.,  56.],\n",
      "       [239.,   1., 279.,  20.],\n",
      "       [328.,   1., 382.,  21.],\n",
      "       [238.,  51., 302.,  74.],\n",
      "       [  4.,  53., 156., 117.]], dtype=float32), array([0, 1, 2, 3, 4]), 416, 194]\n"
     ]
    }
   ],
   "source": [
    "## Creating the complete data set :\n",
    "\n",
    "all_image_line = []\n",
    "for i in range(num_images):\n",
    "    image_line = []\n",
    "    limit_lower = i*5\n",
    "    limit_upper = limit_lower+5\n",
    "    lines = r_new_data[limit_lower:limit_upper]\n",
    "    line_idx, pic_path, boxes, labels, img_width, img_height = single_image_info(lines)\n",
    "    image_line.append(line_idx)\n",
    "    image_line.append(pic_path)\n",
    "    image_line.append(boxes)\n",
    "    image_line.append(labels)\n",
    "    image_line.append(img_width)\n",
    "    image_line.append(img_height)\n",
    "    all_image_line.append(image_line)\n",
    "    \n",
    "print(len(all_image_line))\n",
    "print(all_image_line[1])   ##  boxes are in format xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 140 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scar3crow/Dropbox/WorkStation-Subrata/python/venv1/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train and Test split\n",
    "\n",
    "data_train, data_val = train_test_split(all_image_line, train_size = 0.8 , shuffle = True)\n",
    "\n",
    "num_all_bbox = len(all_image_line) * len(all_image_line[0][2])\n",
    "num_bb_train = len(data_train) * len(data_train[0][2])\n",
    "num_bb_val = len(data_val) * len(data_val[0][2])\n",
    "print(num_all_bbox, num_bb_train, num_bb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating anchors from true boundary boxes :\n",
    "\n",
    "def iou_kmeans(box, clusters):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) between a box and k clusters.\n",
    "    :param box: tuple or array, shifted to the origin (i. e. width and height)\n",
    "    :param clusters: numpy array of shape (k, 2) where k is the number of clusters\n",
    "    :return: numpy array of shape (k, 0) where k is the number of clusters\n",
    "    \"\"\"\n",
    "    x = np.minimum(clusters[:, 0], box[0])\n",
    "    y = np.minimum(clusters[:, 1], box[1])\n",
    "    if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
    "        raise ValueError(\"Box has no area\")\n",
    "\n",
    "    intersection = x * y\n",
    "    box_area = box[0] * box[1]\n",
    "    cluster_area = clusters[:, 0] * clusters[:, 1]\n",
    "\n",
    "    iou = intersection / (box_area + cluster_area - intersection)\n",
    "\n",
    "    return iou\n",
    "\n",
    "def kmeans(boxes, k, dist=np.median):\n",
    "    \"\"\"\n",
    "    Calculates k-means clustering with the Intersection over Union (IoU) metric.\n",
    "    :param boxes: numpy array of shape (r, 2), where r is the number of rows\n",
    "    :param k: number of clusters\n",
    "    :param dist: distance function\n",
    "    :return: numpy array of shape (k, 2)\n",
    "    \"\"\"\n",
    "    rows = boxes.shape[0]\n",
    "\n",
    "    distances = np.empty((rows, k))\n",
    "    last_clusters = np.zeros((rows,))\n",
    "\n",
    "    np.random.seed()\n",
    "\n",
    "    # the Forgy method will fail if the whole array contains the same rows\n",
    "    clusters = boxes[np.random.choice(rows, k, replace=False)]\n",
    "\n",
    "\n",
    "    while True:\n",
    "        for row in range(rows):\n",
    "            distances[row] = 1 - iou_kmeans(boxes[row], clusters)\n",
    "\n",
    "        nearest_clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        if (last_clusters == nearest_clusters).all():\n",
    "            break\n",
    "\n",
    "        for cluster in range(k):\n",
    "            clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=0)\n",
    "\n",
    "        last_clusters = nearest_clusters\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[129.23076923  37.92592593]\n",
      " [ 68.07692308  65.30612245]\n",
      " [189.80769231 160.36697248]]\n"
     ]
    }
   ],
   "source": [
    "## Finding out anchors :\n",
    "## Firstly, converting true boundary box width, height to width & height with respect to target image :\n",
    "## finaly find anchors. Anchors here are in absolute size w.r.t. target image but not as % of target image or \n",
    "## as multiple of unit grids.\n",
    "\n",
    "# num_all_bb = len(r_new_data) # if no. of bboxes varies for images, this formula should be used \n",
    "\n",
    "anchors_wrt_target = np.zeros((3,2))\n",
    "\n",
    "num_all_bb = len(all_image_line) * len(all_image_line[0][2])  ## from all image line data\n",
    "\n",
    "b_box_wrt_target = np.zeros((num_all_bb,2))\n",
    "\n",
    "for i in range(num_all_bb):\n",
    "    \n",
    "    image_w = r_new_data['img_wd'][i]\n",
    "    image_h = r_new_data['img_ht'][i]\n",
    "\n",
    "    x_ratio = target_w / image_w \n",
    "    y_ratio = target_h / image_h\n",
    "    \n",
    "    anchor_w = r_new_data['width'][i] * x_ratio\n",
    "    anchor_h = r_new_data['height'][i] * y_ratio\n",
    "    b_box_wrt_target[i, 0] = anchor_w\n",
    "    b_box_wrt_target[i, 1] = anchor_h\n",
    "    \n",
    "anchors_wrt_target = kmeans(b_box_wrt_target, num_anchors)\n",
    "\n",
    "print(anchors_wrt_target.shape)\n",
    "print(anchors_wrt_target)     ## anchors wrt target image in abs. value and in format width, height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.0384617 1.1851852]\n",
      " [2.1274037 2.0408163]\n",
      " [5.9314904 5.011468 ]]\n"
     ]
    }
   ],
   "source": [
    "## Anchors w.r.t target image but in terms of no. of grids and in format width and height\n",
    "\n",
    "grid_stride = target_size[0] / grid_size[0]\n",
    "anchors_wrt_ti = (anchors_wrt_target / grid_stride).astype(np.float32)\n",
    "print(anchors_wrt_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing the original data to get y_true :\n",
    "\n",
    "def process_box(ori_boxes, ori_img_width, ori_img_height, labels, target_size, class_num, anchors_wrt_target):\n",
    "    '''\n",
    "    Generate the y_true label, i.e. the ground truth feature_map.\n",
    "    params:\n",
    "        boxes: [N, 5] shape, float32 dtype. `x_min, y_min, x_max, y_mix, mixup_weight`.\n",
    "        labels: [N] shape, int64 dtype.\n",
    "        class_num: int64 num.\n",
    "        anchors: [3,2] shape, float32 dtype.\n",
    "    '''\n",
    "    \n",
    "    img_width = ori_img_width\n",
    "    img_height = ori_img_height\n",
    "    boxes = ori_boxes           ## boxes in format xmin, ymin, xmax, ymax in absolute value\n",
    "    anchors = anchors_wrt_target\n",
    "    \n",
    "    x_ratio = target_size[1] / img_width\n",
    "    y_ratio = target_size[0] / img_height\n",
    "    \n",
    "    boxes_wrt_target = np.zeros((5,4)).astype(np.float32)\n",
    "    box_centers_target = np.zeros((5,2)).astype(np.float32)\n",
    "\n",
    "    boxes_wrt_target[:,0] = boxes[:,0] * x_ratio  # xmin absolute value wrt target image\n",
    "    boxes_wrt_target[:,1] = boxes[:,1] * y_ratio  # ymin absolute value wrt target image\n",
    "    boxes_wrt_target[:,2] = boxes[:,2] * x_ratio  # xmax absolute value wrt target image\n",
    "    boxes_wrt_target[:,3] = boxes[:,3] * y_ratio  # ymax absolute value wrt target image\n",
    "    \n",
    "    # In above, boxes_wrt_target shape is (5, 4), now this will be taken to (5. 5) by adding 1 at end\n",
    "#    boxes_wrt_target = np.concatenate((boxes_wrt_target, np.full(shape=(boxes_wrt_target.shape[0], 1), fill_value=1., dtype=np.float32)), axis=-1)\n",
    "    box_centers_target = (boxes_wrt_target[:, 0:2] + boxes_wrt_target[:, 2:4]) / 2  ## centers wrt target, abs values\n",
    "    \n",
    "    box_sizes = boxes[:, 2:4] - boxes[:, 0:2]  #xmax-xmin = width and ymax-ymin = height wrt original image\n",
    "    box_sizes[:,0] = box_sizes[:,0] * x_ratio  # width w.r.t target image in absolute value\n",
    "    box_sizes[:,1] = box_sizes[:,1] * y_ratio  # width w.r.t target image in absolute value\n",
    "    \n",
    "#    y_true_13 = np.zeros((target_size[1] // 32, target_size[0] // 32, 3, 6 + class_num), np.float32)\n",
    "    y_true_13 = np.zeros((target_size[1] // 32, target_size[0] // 32, 3, 5 + class_num), np.float32)\n",
    "\n",
    "#    y_true = [y_true_13]\n",
    "    \n",
    "    box_sizes_exp = np.expand_dims(box_sizes, 1)\n",
    "    mins = np.maximum(- box_sizes_exp / 2, - anchors / 2)\n",
    "    maxs = np.minimum(box_sizes_exp / 2, anchors / 2)\n",
    "    whs = maxs - mins\n",
    "\n",
    "    iou = (whs[:, :, 0] * whs[:, :, 1]) / (\n",
    "                box_sizes_exp[:, :, 0] * box_sizes_exp[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :,\n",
    "                                                                                                         1] + 1e-10)\n",
    "    best_match_idx = np.argmax(iou, axis=1)\n",
    "\n",
    "    anchor_mask = np.zeros((target_size[1] // 32, target_size[0] // 32, 3))\n",
    "\n",
    "    grid_stride = 32  ## = targetsize / no. of grid cells\n",
    "    \n",
    "    for i, idx in enumerate(best_match_idx):\n",
    "        \n",
    "        c_x = box_centers_target[i, 0] / grid_stride\n",
    "        c_y = box_centers_target[i, 1] / grid_stride\n",
    "        \n",
    "        b_w = box_sizes[i, 0] / grid_stride\n",
    "        b_h = box_sizes[i, 1] / grid_stride\n",
    "\n",
    "        x = int(c_x)\n",
    "        y = int(c_y)\n",
    "        k = int(idx)\n",
    "        c = int(labels[i])\n",
    "\n",
    "        print(x, y, k, c)\n",
    "\n",
    "# Very Imp : Now preparing y_true: all values x_center, y_cemter, width & height are being taken to % of target image\n",
    "\n",
    "        c_x_x = c_x - x\n",
    "        if c_x_x == 0.:\n",
    "            c_x_x = (c_x - x) + .0000001\n",
    "            \n",
    "        c_y_y = c_y - y\n",
    "        \n",
    "        if c_y_y == 0.:\n",
    "            c_y_y = (c_y - y) + .0000001\n",
    "\n",
    "\n",
    "        y_true_13[y, x, k, 0] = np.log((c_x_x / (1 - c_x_x)))\n",
    "        y_true_13[y, x, k, 1] = np.log((c_y_y / (1 - c_y_y)))\n",
    "        \n",
    "        y_true_13[y, x, k, 2] = np.log(b_w / anchors[k,0])\n",
    "        y_true_13[y, x, k, 3] = np.log(b_h / anchors[k,1])\n",
    "\n",
    "        y_true_13[y, x, k, 4] = 1.\n",
    "        y_true_13[y, x, k, 5 + c] = 1.\n",
    "\n",
    "        anchor_mask[y, x, k] = 1\n",
    "\n",
    "    return y_true_13, anchor_mask  ## all data are  w,r,t, grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single image-wise image/boundary box preprocessing:\n",
    "\n",
    "def parse_data(line, class_num, target_size, anchors):   ## (mode, letterbox_resize):\n",
    "    '''\n",
    "    param:\n",
    "        line: a line from the training/test txt file\n",
    "        class_num: totol class nums.\n",
    "        target_size: the size of image to be resized to. [width, height] format.\n",
    "        anchors: anchors.\n",
    "        mode: 'train' or 'val'. When set to 'train', data_augmentation will be applied.\n",
    "        letterbox_resize: whether to use the letterbox resize, i.e., keep the original aspect ratio in the resized image.\n",
    "    '''\n",
    "    \n",
    "    img_idx, pic_path, boxes, labels,img_width, img_height = line  # boxes in format xmin, ymin, xmax, ymax\n",
    "    img = cv2.imread(pic_path)\n",
    "    img_resized = cv2.resize(img,(target_size[0], target_size[1]))\n",
    "    \n",
    "    # expand the 2nd dimension, mix up weight default to 1.\n",
    "    boxes = np.concatenate((boxes, np.full(shape=(boxes.shape[0], 1), fill_value=1., dtype=np.float32)), axis=-1)\n",
    "\n",
    "    img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "\n",
    "    # the input of yolo_v3 should be in range 0~1, lets change to -0.5 to +0.5\n",
    "    \n",
    "#    y = (x - min) / (max - min)\n",
    "    \n",
    "    img_resized = (img_resized - 127.5)/ 255.\n",
    "\n",
    "    y_true_13, anchor_mask = process_box(boxes, img_width, img_height, labels, target_size, class_num, anchors)\n",
    "\n",
    "    return img_idx, img_resized, y_true_13, anchor_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "9 5 0 3\n",
      "3 6 2 4\n",
      "3 6 2 0\n",
      "9 5 1 1\n",
      "12 5 1 2\n",
      "9 8 0 3\n",
      "3 9 2 4\n",
      "2 5 2 0\n",
      "8 3 1 1\n",
      "12 3 1 2\n",
      "9 8 1 3\n",
      "3 11 2 4\n",
      "4 3 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "9 5 1 3\n",
      "3 10 2 4\n",
      "7 2 2 0\n",
      "11 9 0 1\n",
      "11 10 0 2\n",
      "11 12 0 3\n",
      "4 11 2 4\n",
      "3 2 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "9 5 1 3\n",
      "2 6 2 4\n",
      "2 3 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "10 7 1 3\n",
      "3 10 2 4\n",
      "2 2 2 0\n",
      "9 0 1 1\n",
      "12 0 1 2\n",
      "9 3 1 3\n",
      "3 6 2 4\n",
      "11 3 2 0\n",
      "9 9 1 1\n",
      "13 9 0 2\n",
      "10 12 0 3\n",
      "4 11 2 4\n",
      "5 2 2 0\n",
      "2 5 0 1\n",
      "2 4 0 2\n",
      "2 7 0 3\n",
      "10 12 2 4\n",
      "7 5 2 0\n",
      "10 10 0 1\n",
      "10 11 0 2\n",
      "11 13 0 3\n",
      "4 12 2 4\n",
      "3 2 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "9 5 1 3\n",
      "3 6 2 4\n",
      "2 2 2 0\n",
      "9 0 1 1\n",
      "12 0 1 2\n",
      "9 4 1 3\n",
      "2 6 2 4\n",
      "4 3 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "9 4 1 3\n",
      "3 10 2 4\n",
      "2 2 2 0\n",
      "8 1 1 1\n",
      "12 1 1 2\n",
      "8 6 1 3\n",
      "3 7 2 4\n",
      "2 3 0 0\n",
      "9 5 0 1\n",
      "9 7 0 2\n",
      "9 12 0 3\n",
      "3 9 2 4\n",
      "2 3 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "10 7 1 3\n",
      "3 11 2 4\n",
      "2 2 2 0\n",
      "9 1 1 1\n",
      "12 0 1 2\n",
      "9 5 1 3\n",
      "3 6 2 4\n",
      "2 4 2 0\n",
      "9 3 1 1\n",
      "12 2 1 2\n",
      "10 7 1 3\n",
      "2 8 2 4\n",
      "2 3 2 0\n",
      "8 1 1 1\n",
      "12 1 1 2\n",
      "9 7 1 3\n",
      "2 10 2 4\n",
      "2 2 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "9 6 1 3\n",
      "2 9 2 4\n",
      "2 2 2 0\n",
      "8 1 1 1\n",
      "12 1 1 2\n",
      "9 6 1 3\n",
      "3 7 2 4\n",
      "2 3 0 0\n",
      "9 5 0 1\n",
      "9 7 0 2\n",
      "9 12 0 3\n",
      "3 10 2 4\n",
      "3 2 2 0\n",
      "9 0 1 1\n",
      "12 0 1 2\n",
      "10 5 1 3\n",
      "3 6 2 4\n",
      "4 2 2 0\n",
      "9 1 1 1\n",
      "12 0 1 2\n",
      "10 4 1 3\n",
      "3 7 2 4\n",
      "2 3 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "10 8 1 3\n",
      "2 9 2 4\n",
      "2 3 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "9 8 1 3\n",
      "3 10 2 4\n",
      "7 3 2 0\n",
      "10 9 0 1\n",
      "11 11 0 2\n",
      "11 12 0 3\n",
      "4 11 2 4\n",
      "2 3 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "10 7 1 3\n",
      "3 9 2 4\n",
      "6 2 2 0\n",
      "2 11 0 1\n",
      "2 12 0 2\n",
      "7 11 0 3\n",
      "10 8 2 4\n",
      "5 1 2 0\n",
      "2 4 0 1\n",
      "2 4 0 2\n",
      "2 6 0 3\n",
      "10 12 2 4\n",
      "2 2 2 0\n",
      "9 2 1 1\n",
      "12 2 1 2\n",
      "9 8 1 3\n",
      "2 9 2 4\n",
      "11 4 2 0\n",
      "9 9 0 1\n",
      "13 9 0 2\n",
      "9 12 0 3\n",
      "3 12 2 4\n",
      "2 3 2 0\n",
      "9 1 1 1\n",
      "12 0 1 2\n",
      "9 6 1 3\n",
      "3 9 2 4\n",
      "2 2 2 0\n",
      "9 1 1 1\n",
      "12 1 1 2\n",
      "9 5 1 3\n",
      "3 7 2 4\n",
      "2 2 2 0\n",
      "9 1 1 1\n",
      "12 2 1 2\n",
      "10 7 1 3\n",
      "2 7 2 4\n"
     ]
    }
   ],
   "source": [
    "## Making the data ready for entering into network :\n",
    "\n",
    "anchors = anchors_wrt_target\n",
    "image_index = []\n",
    "image_resized = []\n",
    "image_y_true = []\n",
    "image_anchor_mask = []\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "\n",
    "    line = data_train[i]\n",
    "    \n",
    "    img_idx, img_resized, y_true, anchor_mask = parse_data(line, class_num, target_size, anchors)\n",
    "    \n",
    "    \n",
    "    image_index.append(img_idx)\n",
    "    image_resized.append(img_resized)\n",
    "    image_y_true.append(y_true)\n",
    "    image_anchor_mask.append(anchor_mask)\n",
    "    \n",
    "train_image_index = image_index\n",
    "X_train = np.array(image_resized).astype(np.float32)\n",
    "Y_train = np.array(image_y_true).astype(np.float32)\n",
    "train_anchor_mask = np.array(image_anchor_mask).astype(np.float32)\n",
    "\n",
    "image_index = []\n",
    "image_resized = []\n",
    "image_y_true = []\n",
    "image_anchor_mask = []\n",
    "\n",
    "for i in range(len(data_val)):\n",
    "    line = data_val[i]\n",
    "    \n",
    "    img_idx, img_resized, y_true, anchor_mask = parse_data(line, class_num, target_size, anchors)\n",
    "    image_index.append(img_idx)\n",
    "    image_resized.append(img_resized)\n",
    "    image_y_true.append(y_true)\n",
    "    image_anchor_mask.append(anchor_mask)\n",
    "val_image_index = image_index\n",
    "X_val = np.array(image_resized).astype(np.float32)\n",
    "Y_val = np.array(image_y_true).astype(np.float32)\n",
    "val_anchor_mask = np.array(image_anchor_mask).astype(np.float32)\n",
    "\n",
    "image_index = []\n",
    "image_resized = []\n",
    "image_y_true = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ,  0.       ,  0.       ,  0.       ],\n",
       "       [ 0.6466272, -2.0660553, -3.5179217, -3.6778758,  1.       ,\n",
       "         0.       ,  0.       ,  1.       ,  0.       ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ,  0.       ,  0.       ,  0.       ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0,1, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " '/home/scar3crow/Downloads/8-6-new-scan/114a.jpg',\n",
       " array([[  3.,  14., 145.,  78.],\n",
       "        [218.,  21., 264.,  45.],\n",
       "        [320.,  16., 379.,  41.],\n",
       "        [217.,  61., 310.,  90.],\n",
       "        [  6.,  83., 195., 129.]], dtype=float32),\n",
       " array([0, 1, 2, 3, 4]),\n",
       " 416,\n",
       " 138]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_line[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.,  14., 145.,  78.],\n",
       "       [218.,  21., 264.,  45.],\n",
       "       [320.,  16., 379.,  41.],\n",
       "       [217.,  61., 310.,  90.],\n",
       "       [  6.,  83., 195., 129.]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_box = all_image_line[25][2]\n",
    "true_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 5 2 0\n",
      "8 3 1 1\n",
      "12 3 1 2\n",
      "9 8 1 3\n",
      "3 11 2 4\n"
     ]
    }
   ],
   "source": [
    "true_y, _ = process_box(true_box, 416., 138., labels, target_size, class_num, anchors_wrt_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "          0.       ,   0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "          0.       ,   0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.7003671, -16.118095 ,  -3.6128194,  -3.1377852,   1.       ,\n",
       "          1.       ,   0.       ,   0.       ,   0.       ,   0.       ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_y[5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28 28 8 8 8\n",
      "[24, 2, 13, 19, 21, 26, 30, 31, 10, 7, 9, 11, 25, 18, 35, 12, 22, 16, 0, 33, 4, 20, 23, 27, 17, 32, 34, 28]\n",
      "(28, 480, 480, 3) (28, 15, 15, 3, 10)\n",
      "24 (480, 480, 3) (15, 15, 3, 10)\n",
      "(28, 15, 15, 3) and (15, 15, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(Y_train), len(train_image_index), len(X_val), len(Y_val), len(val_image_index))\n",
    "print(train_image_index)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(train_image_index[0], X_train[0].shape, Y_train[0].shape)\n",
    "print(train_anchor_mask.shape, 'and', train_anchor_mask[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ethanyanjiali/deep-vision/blob/master/YOLO/tensorflow/utils.py\n",
    "\n",
    "def xywh_to_x1y1x2y2(box):\n",
    "    xy = box[..., 0:2]\n",
    "    wh = box[..., 2:4]\n",
    "\n",
    "    x1y1 = xy - wh / 2\n",
    "    x2y2 = xy + wh / 2\n",
    "\n",
    "    y_box = K.concatenate([x1y1, x2y2], axis=-1)\n",
    "    return y_box\n",
    "\n",
    "def broadcast_iou(box_a, box_b):\n",
    "    \"\"\"\n",
    "    calculate iou between box_a and multiple box_b in a broadcast way\n",
    "    inputs: box_a: a tensor full of boxes, eg. (B, N, 4), box is in x1y1x2y2\n",
    "            box_b: another tensor full of boxes, eg. (B, M, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # (B, N, 1, 4)\n",
    "    box_a = tf.expand_dims(box_a, -2)\n",
    "    # (B, 1, M, 4)\n",
    "    box_b = tf.expand_dims(box_b, -3)\n",
    "    # (B, N, M, 4)\n",
    "    new_shape = tf.broadcast_dynamic_shape(tf.shape(box_a), tf.shape(box_b))\n",
    "\n",
    "    # (B, N, M, 4)\n",
    "    # (B, N, M, 4)\n",
    "    box_a = tf.broadcast_to(box_a, new_shape)\n",
    "    box_b = tf.broadcast_to(box_b, new_shape)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    al, at, ar, ab = tf.split(box_a, 4, -1)\n",
    "    bl, bt, br, bb = tf.split(box_b, 4, -1)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    left = tf.math.maximum(al, bl)\n",
    "    right = tf.math.minimum(ar, br)\n",
    "    top = tf.math.maximum(at, bt)\n",
    "    bot = tf.math.minimum(ab, bb)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    iw = tf.clip_by_value(right - left, 0, 1)\n",
    "    ih = tf.clip_by_value(bot - top, 0, 1)\n",
    "    i = iw * ih\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    area_a = (ar - al) * (ab - at)\n",
    "    area_b = (br - bl) * (bb - bt)\n",
    "    union = area_a + area_b - i\n",
    "\n",
    "    # (B, N, M)\n",
    "    iou = tf.squeeze(i / (union + 1e-7), axis=-1)\n",
    "\n",
    "    return iou\n",
    "\n",
    "## https://github.com/ethanyanjiali/deep-vision/blob/master/YOLO/tensorflow/yolov3.py#L213\n",
    "\n",
    "def calc_ignore_mask(ignore_thresh, true_box, pred_box):\n",
    "    \n",
    "        # YOLOv3:\n",
    "        # \"If the bounding box prior is not the best but does overlap a ground\n",
    "        # truth object by more than some threshold we ignore the prediction,\n",
    "        # following [17]. We use the threshold of .5.\"\n",
    "        # calculate the iou for each pair of pred bbox and true bbox, then find the best among them\n",
    "\n",
    "        # (None, 13, 13, 3, 4)\n",
    "        \n",
    "        true_box_reorganised = xywh_to_x1y1x2y2(true_box)  # reorganised to x1, y1, x2, y2\n",
    "        pred_box_reorganised = xywh_to_x1y1x2y2(pred_box)\n",
    "        \n",
    "        true_box_shape = tf.shape(true_box_reorganised)  \n",
    "        # (None, 13, 13, 3, 4)\n",
    "        pred_box_shape = tf.shape(pred_box_reorganised)  \n",
    "        # (None, 507, 4)\n",
    "        true_box_reorganised = tf.reshape(true_box_reorganised, [true_box_shape[0], -1, 4])\n",
    "        # sort true_box to have non-zero boxes rank first\n",
    "        true_box_reorganised = tf.sort(true_box_reorganised, axis=1, direction=\"DESCENDING\")\n",
    "        # (None, 100, 4)\n",
    "        # only use maximum 100 boxes per groundtruth to calcualte IOU, otherwise\n",
    "        # GPU emory comsumption would explode for a matrix like (16, 52*52*3, 52*52*3, 4)\n",
    "        true_box_reorganised = true_box_reorganised[:, 0:100, :]\n",
    "        # (None, 507, 4)\n",
    "        pred_box_reorganised = tf.reshape(pred_box_reorganised, [pred_box_shape[0], -1, 4])\n",
    "\n",
    "        # https://github.com/dmlc/gluon-cv/blob/06bb7ec2044cdf3f433721be9362ab84b02c5a90/gluoncv/model_zoo/yolo/yolo_target.py#L198\n",
    "        # (None, 507, 507)\n",
    "        iou = broadcast_iou(pred_box_reorganised, true_box_reorganised)\n",
    "        # (None, 507)\n",
    "        best_iou = tf.reduce_max(iou, axis=-1)\n",
    "        # (None, 13, 13, 3)\n",
    "        best_iou = tf.reshape(best_iou, [pred_box_shape[0], pred_box_shape[1], pred_box_shape[2], pred_box_shape[3]])\n",
    "        # ignore_mask = 1 => don't ignore\n",
    "        # ignore_mask = 0 => should ignore\n",
    "        ignore_mask = tf.cast(best_iou < ignore_thresh, tf.float32)\n",
    "        # (None, 13, 13, 3, 1)\n",
    "        ignore_mask = tf.expand_dims(ignore_mask, axis=-1)\n",
    "        \n",
    "        return ignore_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## made on 16/7/2020 at 8:48 pm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "anchors = anchors_wrt_target\n",
    "\n",
    "def my_custom_loss(y_true, y_pred):\n",
    "    \n",
    "#    def pre_loss(my_custom_loss, anchors):\n",
    "        \n",
    "    num_anchors = len(anchors)\n",
    "    num_classes = 5\n",
    "    ignore_thresh = 0.5\n",
    "    grid_size = [15., 15.]\n",
    "    grid_stride = 480. / grid_size[0]\n",
    "    batch_size = 4\n",
    "    \n",
    "#    batch_shape = y_pred.get_shape()\n",
    "#    batch_size = batch_shape[0]\n",
    "    \n",
    "    \n",
    "#        scaled_anchors = anchors / grid_stride\n",
    "    \n",
    "    Lambda_Coord = 5.0\n",
    "    Lambda_no_obj = 0.5\n",
    "    \n",
    "#    grid_x = np.arange(grid_size[1])\n",
    "#    grid_y = np.arange(grid_size[0])\n",
    "    \n",
    "#        a = np.array(np.meshgrid(grid_x, grid_y))\n",
    "#        b = np.array(np.meshgrid(grid_x, grid_y))\n",
    "#        c = np.array(np.meshgrid(grid_x, grid_y))\n",
    "#        d = np.concatenate((a,b,c), axis = 0)\n",
    "#        e = d.transpose(2, 1, 0)\n",
    "#        grid_final = np.reshape(e,[1,15,15,3,2])\n",
    "    \n",
    "    tot_loss = tf.zeros(1, dtype='float32')\n",
    "\n",
    "    obj_mask = y_true[..., 4:5]\n",
    "    no_obj_mask = 1. - obj_mask\n",
    "\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "    t_x_pred = y_pred[..., 0:1]\n",
    "    t_y_pred = y_pred[..., 1:2]\n",
    "    t_w_pred = y_pred[..., 2:3]\n",
    "    t_h_pred = y_pred[..., 3:4]\n",
    "        \n",
    "    t_x_true = y_true[..., 0:1]\n",
    "    t_y_true = y_true[..., 1:2]\n",
    "    t_w_true = y_true[..., 2:3]\n",
    "    t_h_true = y_true[..., 3:4]\n",
    "        \n",
    "    box_loss = K.square(t_x_pred - t_x_true) + K.square(t_y_pred - t_y_true) + K.square(t_w_pred - t_w_true) + K.square(t_h_pred - t_h_true)\n",
    "    box_loss = Lambda_Coord * K.sum(box_loss * obj_mask) / batch_size\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "    t_obj_pred = K.sigmoid(y_pred[..., 4:5])  # shape = 28, 15, 15, 3, 1\n",
    "    \n",
    "    obj_loss = K.sum((-K.log(t_obj_pred)) * obj_mask) / batch_size\n",
    "        \n",
    "    noobj_loss = Lambda_no_obj * K.sum((-K.log(1-t_obj_pred)) * no_obj_mask) / batch_size\n",
    "                                          \n",
    "        \n",
    "        \n",
    "#        true_box_wrt_ti = K.concatenate([true_box_xy_wrt_target_image, true_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "#        pred_box_wrt_ti = K.concatenate([pred_box_xy_wrt_target_image, pred_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "    \n",
    "#        ignore_mask = calc_ignore_mask(ignore_thresh, true_box_wrt_ti, pred_box_wrt_ti)\n",
    "        \n",
    "#        bce = tf.keras.losses.BinaryCrossentropy()        \n",
    "#        obj_loss = K.sum(bce(obj_mask, pred_obj_mask) * obj_mask)\n",
    "\n",
    "##    obj_loss_arr = K.square(t_obj_pred - obj_mask)\n",
    "##    obj_loss = K.sum(obj_loss_arr * obj_mask) / batch_size\n",
    "    \n",
    "#        no_obj_mask = 1. - obj_mask\n",
    "                \n",
    "##    noobj_loss_arr = Lambda_no_obj * K.square(t_obj_pred - obj_mask)\n",
    "##    noobj_loss = K.sum(noobj_loss_arr * no_obj_mask) / batch_size\n",
    "    \n",
    "#    noobj_loss = K.sum(noobj_loss_arr * no_obj_mask * ignore_mask) / batch_size\n",
    "        \n",
    "        \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    true_classes = y_true[..., 5:10]\n",
    "    \n",
    "    pred_classes = K.sigmoid(y_pred[..., 5:10]) * t_obj_pred\n",
    "        \n",
    "        \n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    class_loss = K.sum(bce(pred_classes, true_classes) * obj_mask) / batch_size\n",
    "        \n",
    "        \n",
    "#        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "#        class_loss = K.sum(bce(true_classes, pred_classes) * obj_mask)\n",
    "\n",
    "#        class_loss_arr = K.square(true_classes - pred_classes)\n",
    "#        class_loss = K.sum(class_loss_arr * obj_mask) / batch_size\n",
    "\n",
    "    tot_loss = box_loss + obj_loss + noobj_loss + class_loss\n",
    "\n",
    "##    return box_loss\n",
    "        \n",
    "    return tot_loss\n",
    "    \n",
    "#    loss = pre_loss(my_custom_loss, anchors)\n",
    "    \n",
    "#    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block(inp, convs, skip=True):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    \n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        \n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
    "        x = Conv2D(conv['filter'], \n",
    "                   conv['kernel'], \n",
    "                   strides=conv['stride'], \n",
    "                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
    "                   name='conv_' + str(conv['layer_idx']), \n",
    "                   use_bias=False if conv['bnorm'] else True)(x)\n",
    "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
    "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "\n",
    "    return add([skip_connection, x]) if skip else x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yolov3_model():\n",
    "    input_image = Input(shape=(480, 480, 3))\n",
    "\n",
    "    # Layer  0 => 4\n",
    "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
    "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
    "\n",
    "    # Layer  5 => 8\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
    "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
    "\n",
    "    # Layer  9 => 11\n",
    "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
    "\n",
    "    # Layer 12 => 15\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
    "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
    "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
    "\n",
    "    # Layer 16 => 36\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
    "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
    "        \n",
    "    skip_36 = x\n",
    "        \n",
    "    # Layer 37 => 40\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
    "\n",
    "    # Layer 41 => 61\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
    "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
    "        \n",
    "    skip_61 = x\n",
    "        \n",
    "#    # Layer 62 => 65\n",
    "#    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "#                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "#                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "    \n",
    "    # Layer 62 => 65\n",
    "    x = _conv_block(x, [{'filter':  256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "                        {'filter':  256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "\n",
    "\n",
    "#    # Layer 66 => 74\n",
    "#    for i in range(3):\n",
    "#        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "#                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "    \n",
    "     # Layer 66 => 74\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x, [{'filter':  128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "                            {'filter':  256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "\n",
    "#    # Layer 75 => 79\n",
    "#    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "#                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "#                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "#                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "#                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "    \n",
    "    # Layer 75 => 79\n",
    "    x = _conv_block(x, [{'filter':  128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "                        {'filter':  256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "                        {'filter':  128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "                        {'filter':  256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "                        {'filter':  128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "    \n",
    "    # Layer 80 => 82\n",
    "    yolo_82 = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
    "                              {'filter':  30, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
    "\n",
    "#    # Layer 83 => 86\n",
    "#    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
    "#    x = UpSampling2D(2)(x)\n",
    "#    x = concatenate([x, skip_61])\n",
    "\n",
    "#    # Layer 87 => 91\n",
    "#    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
    "#                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
    "#                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
    "#                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
    "#                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
    "\n",
    "#    # Layer 92 => 94\n",
    "#    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
    "#                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
    "\n",
    "#    # Layer 95 => 98\n",
    "#    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
    "#    x = UpSampling2D(2)(x)\n",
    "#    x = concatenate([x, skip_36])\n",
    "\n",
    "#    # Layer 99 => 106\n",
    "#    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
    "#                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
    "#                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
    "#                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
    "#                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
    "#                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
    "#                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
    "\n",
    "#    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n",
    "\n",
    "    final = Reshape((grid_y_axis,grid_x_axis,num_anchors,info))(yolo_82)\n",
    "    model = Model(input_image, final)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 480, 480, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv2D)                 (None, 480, 480, 32) 864         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_0 (BatchNormalization)    (None, 480, 480, 32) 128         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_0 (LeakyReLU)             (None, 480, 480, 32) 0           bnorm_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_41 (ZeroPadding2 (None, 481, 481, 32) 0           leaky_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 240, 240, 64) 18432       zero_padding2d_41[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_1 (BatchNormalization)    (None, 240, 240, 64) 256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_1 (LeakyReLU)             (None, 240, 240, 64) 0           bnorm_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 240, 240, 32) 2048        leaky_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_2 (BatchNormalization)    (None, 240, 240, 32) 128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_2 (LeakyReLU)             (None, 240, 240, 32) 0           bnorm_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 240, 240, 64) 18432       leaky_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_3 (BatchNormalization)    (None, 240, 240, 64) 256         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_3 (LeakyReLU)             (None, 240, 240, 64) 0           bnorm_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_185 (Add)                   (None, 240, 240, 64) 0           leaky_1[0][0]                    \n",
      "                                                                 leaky_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_42 (ZeroPadding2 (None, 241, 241, 64) 0           add_185[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 120, 120, 128 73728       zero_padding2d_42[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_5 (BatchNormalization)    (None, 120, 120, 128 512         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_5 (LeakyReLU)             (None, 120, 120, 128 0           bnorm_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 120, 120, 64) 8192        leaky_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_6 (BatchNormalization)    (None, 120, 120, 64) 256         conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_6 (LeakyReLU)             (None, 120, 120, 64) 0           bnorm_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, 120, 120, 128 73728       leaky_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_7 (BatchNormalization)    (None, 120, 120, 128 512         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_7 (LeakyReLU)             (None, 120, 120, 128 0           bnorm_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_186 (Add)                   (None, 120, 120, 128 0           leaky_5[0][0]                    \n",
      "                                                                 leaky_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, 120, 120, 64) 8192        add_186[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_9 (BatchNormalization)    (None, 120, 120, 64) 256         conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_9 (LeakyReLU)             (None, 120, 120, 64) 0           bnorm_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, 120, 120, 128 73728       leaky_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_10 (BatchNormalization)   (None, 120, 120, 128 512         conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_10 (LeakyReLU)            (None, 120, 120, 128 0           bnorm_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_187 (Add)                   (None, 120, 120, 128 0           add_186[0][0]                    \n",
      "                                                                 leaky_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_43 (ZeroPadding2 (None, 121, 121, 128 0           add_187[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_12 (Conv2D)                (None, 60, 60, 256)  294912      zero_padding2d_43[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_12 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_12 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_13 (Conv2D)                (None, 60, 60, 128)  32768       leaky_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_13 (BatchNormalization)   (None, 60, 60, 128)  512         conv_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_13 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_14 (Conv2D)                (None, 60, 60, 256)  294912      leaky_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_14 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_14 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_188 (Add)                   (None, 60, 60, 256)  0           leaky_12[0][0]                   \n",
      "                                                                 leaky_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 60, 60, 128)  32768       add_188[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_16 (BatchNormalization)   (None, 60, 60, 128)  512         conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_16 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 60, 60, 256)  294912      leaky_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_17 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_17 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_189 (Add)                   (None, 60, 60, 256)  0           add_188[0][0]                    \n",
      "                                                                 leaky_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 60, 60, 128)  32768       add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_19 (BatchNormalization)   (None, 60, 60, 128)  512         conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_19 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 60, 60, 256)  294912      leaky_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_20 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_20 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_190 (Add)                   (None, 60, 60, 256)  0           add_189[0][0]                    \n",
      "                                                                 leaky_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 60, 60, 128)  32768       add_190[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_22 (BatchNormalization)   (None, 60, 60, 128)  512         conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_22 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 60, 60, 256)  294912      leaky_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_23 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_23 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_191 (Add)                   (None, 60, 60, 256)  0           add_190[0][0]                    \n",
      "                                                                 leaky_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_25 (Conv2D)                (None, 60, 60, 128)  32768       add_191[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_25 (BatchNormalization)   (None, 60, 60, 128)  512         conv_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_25 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_26 (Conv2D)                (None, 60, 60, 256)  294912      leaky_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_26 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_26 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_192 (Add)                   (None, 60, 60, 256)  0           add_191[0][0]                    \n",
      "                                                                 leaky_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_28 (Conv2D)                (None, 60, 60, 128)  32768       add_192[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_28 (BatchNormalization)   (None, 60, 60, 128)  512         conv_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_28 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_29 (Conv2D)                (None, 60, 60, 256)  294912      leaky_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_29 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_29 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_193 (Add)                   (None, 60, 60, 256)  0           add_192[0][0]                    \n",
      "                                                                 leaky_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_31 (Conv2D)                (None, 60, 60, 128)  32768       add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_31 (BatchNormalization)   (None, 60, 60, 128)  512         conv_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_31 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_32 (Conv2D)                (None, 60, 60, 256)  294912      leaky_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_32 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_32 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_194 (Add)                   (None, 60, 60, 256)  0           add_193[0][0]                    \n",
      "                                                                 leaky_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_34 (Conv2D)                (None, 60, 60, 128)  32768       add_194[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_34 (BatchNormalization)   (None, 60, 60, 128)  512         conv_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_34 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_35 (Conv2D)                (None, 60, 60, 256)  294912      leaky_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_35 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_35 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_195 (Add)                   (None, 60, 60, 256)  0           add_194[0][0]                    \n",
      "                                                                 leaky_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_44 (ZeroPadding2 (None, 61, 61, 256)  0           add_195[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_37 (Conv2D)                (None, 30, 30, 512)  1179648     zero_padding2d_44[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_37 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_37 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_38 (Conv2D)                (None, 30, 30, 256)  131072      leaky_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_38 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_38 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_39 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_39 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_39 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_196 (Add)                   (None, 30, 30, 512)  0           leaky_37[0][0]                   \n",
      "                                                                 leaky_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_41 (Conv2D)                (None, 30, 30, 256)  131072      add_196[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_41 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_41 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_42 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_42 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_42 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_197 (Add)                   (None, 30, 30, 512)  0           add_196[0][0]                    \n",
      "                                                                 leaky_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_44 (Conv2D)                (None, 30, 30, 256)  131072      add_197[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_44 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_44 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_45 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_45 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_45 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_198 (Add)                   (None, 30, 30, 512)  0           add_197[0][0]                    \n",
      "                                                                 leaky_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_47 (Conv2D)                (None, 30, 30, 256)  131072      add_198[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_47 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_47 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_48 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_48 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_48 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_199 (Add)                   (None, 30, 30, 512)  0           add_198[0][0]                    \n",
      "                                                                 leaky_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_50 (Conv2D)                (None, 30, 30, 256)  131072      add_199[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_50 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_50 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_51 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_51 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_51 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_200 (Add)                   (None, 30, 30, 512)  0           add_199[0][0]                    \n",
      "                                                                 leaky_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_53 (Conv2D)                (None, 30, 30, 256)  131072      add_200[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_53 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_53 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_54 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_54 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_54 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_201 (Add)                   (None, 30, 30, 512)  0           add_200[0][0]                    \n",
      "                                                                 leaky_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_56 (Conv2D)                (None, 30, 30, 256)  131072      add_201[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_56 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_56 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_57 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_57 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_57[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_57 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_202 (Add)                   (None, 30, 30, 512)  0           add_201[0][0]                    \n",
      "                                                                 leaky_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_59 (Conv2D)                (None, 30, 30, 256)  131072      add_202[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_59 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_59 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_60 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_60 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_60 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_203 (Add)                   (None, 30, 30, 512)  0           add_202[0][0]                    \n",
      "                                                                 leaky_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_45 (ZeroPadding2 (None, 31, 31, 512)  0           add_203[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_62 (Conv2D)                (None, 15, 15, 256)  1179648     zero_padding2d_45[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_62 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_62 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_63 (Conv2D)                (None, 15, 15, 512)  131072      leaky_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_63 (BatchNormalization)   (None, 15, 15, 512)  2048        conv_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_63 (LeakyReLU)            (None, 15, 15, 512)  0           bnorm_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_64 (Conv2D)                (None, 15, 15, 256)  1179648     leaky_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_64 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_64 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_204 (Add)                   (None, 15, 15, 256)  0           leaky_62[0][0]                   \n",
      "                                                                 leaky_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_66 (Conv2D)                (None, 15, 15, 128)  32768       add_204[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_66 (BatchNormalization)   (None, 15, 15, 128)  512         conv_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_66 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_67 (Conv2D)                (None, 15, 15, 256)  294912      leaky_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_67 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_67 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_205 (Add)                   (None, 15, 15, 256)  0           add_204[0][0]                    \n",
      "                                                                 leaky_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_69 (Conv2D)                (None, 15, 15, 128)  32768       add_205[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_69 (BatchNormalization)   (None, 15, 15, 128)  512         conv_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_69 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_70 (Conv2D)                (None, 15, 15, 256)  294912      leaky_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_70 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_70[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_70 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_206 (Add)                   (None, 15, 15, 256)  0           add_205[0][0]                    \n",
      "                                                                 leaky_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_72 (Conv2D)                (None, 15, 15, 128)  32768       add_206[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_72 (BatchNormalization)   (None, 15, 15, 128)  512         conv_72[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_72 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_73 (Conv2D)                (None, 15, 15, 256)  294912      leaky_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_73 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_73[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_73 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_207 (Add)                   (None, 15, 15, 256)  0           add_206[0][0]                    \n",
      "                                                                 leaky_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_75 (Conv2D)                (None, 15, 15, 128)  32768       add_207[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_75 (BatchNormalization)   (None, 15, 15, 128)  512         conv_75[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_75 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_76 (Conv2D)                (None, 15, 15, 256)  294912      leaky_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_76 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_76[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_76 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_77 (Conv2D)                (None, 15, 15, 128)  32768       leaky_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_77 (BatchNormalization)   (None, 15, 15, 128)  512         conv_77[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_77 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_78 (Conv2D)                (None, 15, 15, 256)  294912      leaky_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_78 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_78[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_78 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_79 (Conv2D)                (None, 15, 15, 128)  32768       leaky_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_79 (BatchNormalization)   (None, 15, 15, 128)  512         conv_79[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_79 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_80 (Conv2D)                (None, 15, 15, 256)  294912      leaky_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_80 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_80[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_80 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_81 (Conv2D)                (None, 15, 15, 30)   7710        leaky_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 15, 15, 3, 10 0           conv_81[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,379,326\n",
      "Trainable params: 19,351,294\n",
      "Non-trainable params: 28,032\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_size = (target_w, target_h, 3)\n",
    "\n",
    "\n",
    "my_model_1 = make_yolov3_model()\n",
    "my_model_2 = make_yolov3_model()\n",
    "my_model_3 = make_yolov3_model()\n",
    "my_model_4 = make_yolov3_model()\n",
    "\n",
    "print(my_model_1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "my_model_1.compile(optimizer= opt, loss = my_custom_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/scar3crow/Dropbox/WorkStation-Subrata/python/venv1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 177s 6s/step - loss: nan - accuracy: 0.6060 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 90s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 91s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 89s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 95s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f58ba7550>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_1.fit(X_train ,Y_train, epochs= 5, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.0001, momentum = 0.9)\n",
    "\n",
    "my_model_2.compile(optimizer= opt, loss = my_custom_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 94s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 94s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 96s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 93s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 95s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f54866048>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_1.fit(X_train ,Y_train, epochs= 5, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 96s 3s/step - loss: nan - accuracy: 0.8728 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 90s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 91s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 93s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 93s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f5454f588>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 5, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "my_model_3.compile(optimizer= opt, loss = my_custom_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 102s 4s/step - loss: nan - accuracy: 0.4965 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 90s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f50d78be0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_3.fit(X_train ,Y_train, epochs= 2, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 15, 15, 3, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan],\n",
       "          [nan, nan, nan, ..., nan, nan, nan]]]]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_1 = my_model_3.predict(X_train)\n",
    "print(pred_1.shape)\n",
    "pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "my_model_1.compile(optimizer= opt, loss = my_custom_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 104s 4s/step - loss: nan - accuracy: 0.4441 - val_loss: nan - val_accuracy: 0.9943\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 91s 3s/step - loss: nan - accuracy: 0.9944 - val_loss: nan - val_accuracy: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f3f704c50>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_1.fit(X_train ,Y_train, epochs= 2, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "my_model_3.compile(optimizer= opt, loss = my_custom_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 248s 9s/step - loss: 962.6547 - accuracy: 0.1080 - val_loss: 816.9245 - val_accuracy: 0.0209\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 93s 3s/step - loss: 685.9572 - accuracy: 0.0885 - val_loss: 815.4125 - val_accuracy: 5.5556e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f9e1ac710>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_3.fit(X_train ,Y_train, epochs= 2, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "my_yolo_invoice_model.compile(optimizer= opt, loss = my_custom_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/2\n",
      "28/28 [==============================] - 172s 6s/step - loss: 1224.4514 - accuracy: 0.0471 - val_loss: 1069.1140 - val_accuracy: 0.0035\n",
      "Epoch 2/2\n",
      "28/28 [==============================] - 90s 3s/step - loss: 929.2804 - accuracy: 0.0508 - val_loss: 1064.6971 - val_accuracy: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f22b23668>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_yolo_invoice_model.fit(X_train ,Y_train, epochs= 2, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 93s 3s/step - loss: 826.8909 - accuracy: 0.0566 - val_loss: 1056.2383 - val_accuracy: 0.0067\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 95s 3s/step - loss: 586.1638 - accuracy: 0.0650 - val_loss: 1043.3456 - val_accuracy: 0.0183\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 94s 3s/step - loss: 489.8834 - accuracy: 0.0770 - val_loss: 1026.7545 - val_accuracy: 0.0072\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 94s 3s/step - loss: 429.9064 - accuracy: 0.0916 - val_loss: 1012.4221 - val_accuracy: 0.0237\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 360.7981 - accuracy: 0.0989 - val_loss: 1000.7332 - val_accuracy: 0.0226\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 326.6782 - accuracy: 0.1090 - val_loss: 983.2865 - val_accuracy: 0.0285\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 281.7925 - accuracy: 0.1194 - val_loss: 965.4844 - val_accuracy: 0.0537\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 244.1436 - accuracy: 0.1223 - val_loss: 943.5254 - val_accuracy: 0.0678\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 236.3964 - accuracy: 0.1298 - val_loss: 928.2658 - val_accuracy: 0.0907\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 229.2505 - accuracy: 0.1455 - val_loss: 923.6724 - val_accuracy: 0.0944\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 212.4128 - accuracy: 0.1500 - val_loss: 915.9168 - val_accuracy: 0.0944\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 214.8413 - accuracy: 0.1605 - val_loss: 905.2801 - val_accuracy: 0.0965\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 205.5526 - accuracy: 0.1695 - val_loss: 875.4111 - val_accuracy: 0.1022\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 190.1678 - accuracy: 0.1737 - val_loss: 864.0676 - val_accuracy: 0.1111\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 163.6713 - accuracy: 0.1770 - val_loss: 866.8800 - val_accuracy: 0.1148\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 158.5129 - accuracy: 0.1853 - val_loss: 856.0121 - val_accuracy: 0.1252\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 151.8726 - accuracy: 0.1892 - val_loss: 823.8322 - val_accuracy: 0.1491\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 143.6994 - accuracy: 0.1982 - val_loss: 807.3480 - val_accuracy: 0.1969\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 143.9588 - accuracy: 0.2016 - val_loss: 791.7431 - val_accuracy: 0.2709\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 128.3405 - accuracy: 0.2061 - val_loss: 785.9633 - val_accuracy: 0.2794\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 128.5858 - accuracy: 0.2108 - val_loss: 777.5719 - val_accuracy: 0.2743\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 118.6895 - accuracy: 0.2154 - val_loss: 727.6503 - val_accuracy: 0.2933\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 110.4417 - accuracy: 0.2170 - val_loss: 692.4131 - val_accuracy: 0.3181\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 111.0517 - accuracy: 0.2196 - val_loss: 681.3207 - val_accuracy: 0.3154\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 102.3331 - accuracy: 0.2256 - val_loss: 683.7508 - val_accuracy: 0.3046\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 93.7133 - accuracy: 0.2275 - val_loss: 697.5600 - val_accuracy: 0.2911\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 99.2674 - accuracy: 0.2312 - val_loss: 687.3079 - val_accuracy: 0.2854\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 92.2379 - accuracy: 0.2307 - val_loss: 663.6496 - val_accuracy: 0.2835\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 100.7727 - accuracy: 0.2324 - val_loss: 655.8083 - val_accuracy: 0.2798\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 90.5625 - accuracy: 0.2389 - val_loss: 641.7548 - val_accuracy: 0.2815\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 81.0065 - accuracy: 0.2369 - val_loss: 620.6379 - val_accuracy: 0.2904\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 74.4671 - accuracy: 0.2355 - val_loss: 597.9076 - val_accuracy: 0.3030\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 85.7177 - accuracy: 0.2396 - val_loss: 586.7969 - val_accuracy: 0.3035\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 71.6769 - accuracy: 0.2424 - val_loss: 562.6176 - val_accuracy: 0.3044\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 69.3962 - accuracy: 0.2430 - val_loss: 545.1321 - val_accuracy: 0.3015\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 66.0875 - accuracy: 0.2441 - val_loss: 528.2744 - val_accuracy: 0.3007\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 72.3739 - accuracy: 0.2443 - val_loss: 500.5304 - val_accuracy: 0.3124\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 67.8069 - accuracy: 0.2470 - val_loss: 483.3296 - val_accuracy: 0.3137\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 66.1918 - accuracy: 0.2478 - val_loss: 475.3977 - val_accuracy: 0.2980\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 59.7362 - accuracy: 0.2475 - val_loss: 473.7980 - val_accuracy: 0.2887\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 69.6764 - accuracy: 0.2527 - val_loss: 463.4374 - val_accuracy: 0.2926\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 63.1769 - accuracy: 0.2510 - val_loss: 433.9866 - val_accuracy: 0.3228\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 59.3628 - accuracy: 0.2573 - val_loss: 403.8999 - val_accuracy: 0.3387\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 55.2618 - accuracy: 0.2524 - val_loss: 382.9838 - val_accuracy: 0.3404\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 55.4289 - accuracy: 0.2535 - val_loss: 384.2691 - val_accuracy: 0.3269\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 55.9223 - accuracy: 0.2593 - val_loss: 392.8479 - val_accuracy: 0.3150\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 54.3860 - accuracy: 0.2637 - val_loss: 383.1729 - val_accuracy: 0.3233\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 52.6673 - accuracy: 0.2608 - val_loss: 383.2134 - val_accuracy: 0.3100\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 51.1533 - accuracy: 0.2595 - val_loss: 371.0771 - val_accuracy: 0.3191\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 46.2902 - accuracy: 0.2630 - val_loss: 360.9781 - val_accuracy: 0.3265\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 47.6752 - accuracy: 0.2665 - val_loss: 358.2807 - val_accuracy: 0.3270\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 47.4238 - accuracy: 0.2640 - val_loss: 354.1961 - val_accuracy: 0.3359\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 45.4998 - accuracy: 0.2676 - val_loss: 349.8583 - val_accuracy: 0.3333\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 49.2674 - accuracy: 0.2689 - val_loss: 350.9033 - val_accuracy: 0.3261\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 57.1934 - accuracy: 0.2668 - val_loss: 348.0984 - val_accuracy: 0.3311\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 42.6065 - accuracy: 0.2662 - val_loss: 339.3997 - val_accuracy: 0.3354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 39.6982 - accuracy: 0.2640 - val_loss: 332.1734 - val_accuracy: 0.3350\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 39.8875 - accuracy: 0.2688 - val_loss: 329.8260 - val_accuracy: 0.3333\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 38.7406 - accuracy: 0.2701 - val_loss: 328.3312 - val_accuracy: 0.3280\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 39.4365 - accuracy: 0.2701 - val_loss: 329.3074 - val_accuracy: 0.3326\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 38.7977 - accuracy: 0.2680 - val_loss: 329.1261 - val_accuracy: 0.3348\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 39.2005 - accuracy: 0.2724 - val_loss: 327.5367 - val_accuracy: 0.3348\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 64.9860 - accuracy: 0.2668 - val_loss: 329.7888 - val_accuracy: 0.3356\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 39.8464 - accuracy: 0.2675 - val_loss: 332.5818 - val_accuracy: 0.3313\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 38.3363 - accuracy: 0.2701 - val_loss: 331.2086 - val_accuracy: 0.3226\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 35.9944 - accuracy: 0.2728 - val_loss: 329.6303 - val_accuracy: 0.3231\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.8965 - accuracy: 0.2699 - val_loss: 327.9898 - val_accuracy: 0.3257\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 35.1415 - accuracy: 0.2717 - val_loss: 325.4418 - val_accuracy: 0.3252\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 35.0636 - accuracy: 0.2698 - val_loss: 322.4973 - val_accuracy: 0.3246\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 33.1550 - accuracy: 0.2723 - val_loss: 317.6975 - val_accuracy: 0.3285\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 32.7746 - accuracy: 0.2704 - val_loss: 315.6045 - val_accuracy: 0.3183\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 32.7632 - accuracy: 0.2689 - val_loss: 311.4435 - val_accuracy: 0.3141\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 33.3682 - accuracy: 0.2714 - val_loss: 305.8890 - val_accuracy: 0.3196\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 32.6717 - accuracy: 0.2696 - val_loss: 302.9958 - val_accuracy: 0.3231\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 40.5458 - accuracy: 0.2692 - val_loss: 305.9614 - val_accuracy: 0.3098\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 40.5194 - accuracy: 0.2722 - val_loss: 307.6413 - val_accuracy: 0.3031\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 32.2290 - accuracy: 0.2747 - val_loss: 305.8719 - val_accuracy: 0.3080\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 30.2088 - accuracy: 0.2732 - val_loss: 299.5454 - val_accuracy: 0.3096\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 28.5344 - accuracy: 0.2764 - val_loss: 293.2835 - val_accuracy: 0.3094\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 41.3545 - accuracy: 0.2720 - val_loss: 292.6147 - val_accuracy: 0.3074\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.5654 - accuracy: 0.2729 - val_loss: 286.1808 - val_accuracy: 0.2996\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 31.3489 - accuracy: 0.2751 - val_loss: 283.0719 - val_accuracy: 0.3039\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 30.7104 - accuracy: 0.2797 - val_loss: 278.9966 - val_accuracy: 0.3117\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 27.7815 - accuracy: 0.2797 - val_loss: 276.1596 - val_accuracy: 0.3107\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 29.1366 - accuracy: 0.2767 - val_loss: 280.2306 - val_accuracy: 0.3072\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 27.7268 - accuracy: 0.2763 - val_loss: 282.7634 - val_accuracy: 0.3050\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 24.7519 - accuracy: 0.2784 - val_loss: 281.3676 - val_accuracy: 0.3050\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 26.1641 - accuracy: 0.2770 - val_loss: 279.6717 - val_accuracy: 0.2967\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 26.0136 - accuracy: 0.2737 - val_loss: 281.5035 - val_accuracy: 0.2924\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 25.4292 - accuracy: 0.2726 - val_loss: 288.7182 - val_accuracy: 0.2898\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 25.0137 - accuracy: 0.2751 - val_loss: 287.8857 - val_accuracy: 0.2933\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 26.1175 - accuracy: 0.2735 - val_loss: 288.1250 - val_accuracy: 0.2889\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 26.7755 - accuracy: 0.2810 - val_loss: 298.0863 - val_accuracy: 0.2865\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 24.2722 - accuracy: 0.2803 - val_loss: 306.1282 - val_accuracy: 0.2859\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 22.6217 - accuracy: 0.2760 - val_loss: 303.7193 - val_accuracy: 0.2806\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 23.7055 - accuracy: 0.2756 - val_loss: 306.2906 - val_accuracy: 0.2802\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 21.3599 - accuracy: 0.2781 - val_loss: 307.4538 - val_accuracy: 0.2819\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 22.8889 - accuracy: 0.2767 - val_loss: 306.7919 - val_accuracy: 0.2819\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 23.4992 - accuracy: 0.2784 - val_loss: 309.0769 - val_accuracy: 0.2778\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 22.6529 - accuracy: 0.2792 - val_loss: 314.4291 - val_accuracy: 0.2735\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 21.5809 - accuracy: 0.2799 - val_loss: 315.9620 - val_accuracy: 0.2750\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 21.9630 - accuracy: 0.2765 - val_loss: 318.6597 - val_accuracy: 0.2767\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 21.3623 - accuracy: 0.2788 - val_loss: 315.6752 - val_accuracy: 0.2761\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 22.1555 - accuracy: 0.2771 - val_loss: 314.9544 - val_accuracy: 0.2735\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 24.6823 - accuracy: 0.2780 - val_loss: 321.6977 - val_accuracy: 0.2761\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 19.6247 - accuracy: 0.2804 - val_loss: 316.9325 - val_accuracy: 0.2789\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 20.0463 - accuracy: 0.2792 - val_loss: 315.5301 - val_accuracy: 0.2794\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 19.1969 - accuracy: 0.2810 - val_loss: 315.6947 - val_accuracy: 0.2770\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 18.9593 - accuracy: 0.2783 - val_loss: 315.3206 - val_accuracy: 0.2769\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 19.1018 - accuracy: 0.2830 - val_loss: 319.2006 - val_accuracy: 0.2743\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 19.6765 - accuracy: 0.2786 - val_loss: 317.4119 - val_accuracy: 0.2759\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 17.9163 - accuracy: 0.2833 - val_loss: 312.8822 - val_accuracy: 0.2781\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 102s 4s/step - loss: 19.2310 - accuracy: 0.2817 - val_loss: 316.1345 - val_accuracy: 0.2817\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 19.1610 - accuracy: 0.2848 - val_loss: 310.0303 - val_accuracy: 0.2796\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 17.7916 - accuracy: 0.2806 - val_loss: 307.9184 - val_accuracy: 0.2813\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 17.4299 - accuracy: 0.2805 - val_loss: 311.9798 - val_accuracy: 0.2789\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 17.5824 - accuracy: 0.2819 - val_loss: 311.8983 - val_accuracy: 0.2767\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 17.5321 - accuracy: 0.2821 - val_loss: 310.6434 - val_accuracy: 0.2761\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 16.1008 - accuracy: 0.2860 - val_loss: 307.0182 - val_accuracy: 0.2754\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 17.1594 - accuracy: 0.2820 - val_loss: 304.3211 - val_accuracy: 0.2746\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 17.1118 - accuracy: 0.2810 - val_loss: 310.4255 - val_accuracy: 0.2752\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 20.8865 - accuracy: 0.2804 - val_loss: 310.3071 - val_accuracy: 0.2739\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 17.3611 - accuracy: 0.2842 - val_loss: 307.2302 - val_accuracy: 0.2724\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 16.7194 - accuracy: 0.2835 - val_loss: 307.5136 - val_accuracy: 0.2720\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 16.2658 - accuracy: 0.2830 - val_loss: 308.6022 - val_accuracy: 0.2754\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 17.0385 - accuracy: 0.2817 - val_loss: 314.3176 - val_accuracy: 0.2754\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 15.9139 - accuracy: 0.2834 - val_loss: 319.6216 - val_accuracy: 0.2759\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 15.7661 - accuracy: 0.2855 - val_loss: 313.2406 - val_accuracy: 0.2756\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 15.1893 - accuracy: 0.2846 - val_loss: 315.7698 - val_accuracy: 0.2765\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 15.9572 - accuracy: 0.2812 - val_loss: 311.6420 - val_accuracy: 0.2739\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 15.4000 - accuracy: 0.2822 - val_loss: 313.4575 - val_accuracy: 0.2711\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 16.0677 - accuracy: 0.2797 - val_loss: 312.9890 - val_accuracy: 0.2728\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 15.4820 - accuracy: 0.2830 - val_loss: 310.6146 - val_accuracy: 0.2744\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 16.1713 - accuracy: 0.2831 - val_loss: 310.4265 - val_accuracy: 0.2746\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 14.8947 - accuracy: 0.2832 - val_loss: 314.2751 - val_accuracy: 0.2769\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 111s 4s/step - loss: 14.3079 - accuracy: 0.2799 - val_loss: 314.1312 - val_accuracy: 0.2769\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 15.3798 - accuracy: 0.2840 - val_loss: 312.1794 - val_accuracy: 0.2787\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 16.6057 - accuracy: 0.2801 - val_loss: 317.8318 - val_accuracy: 0.2769\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 18.8873 - accuracy: 0.2826 - val_loss: 317.7103 - val_accuracy: 0.2763\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 16.7234 - accuracy: 0.2790 - val_loss: 317.7051 - val_accuracy: 0.2752\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 14.3862 - accuracy: 0.2820 - val_loss: 312.7560 - val_accuracy: 0.2831\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 16.8803 - accuracy: 0.2836 - val_loss: 300.8338 - val_accuracy: 0.2811\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 16.0659 - accuracy: 0.2802 - val_loss: 307.2412 - val_accuracy: 0.2787\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 16.3525 - accuracy: 0.2825 - val_loss: 320.1673 - val_accuracy: 0.2837\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 15.2991 - accuracy: 0.2839 - val_loss: 312.7861 - val_accuracy: 0.2781\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 14.7538 - accuracy: 0.2802 - val_loss: 320.3294 - val_accuracy: 0.2774\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 15.9501 - accuracy: 0.2810 - val_loss: 324.9444 - val_accuracy: 0.2774\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 14.6014 - accuracy: 0.2814 - val_loss: 321.4102 - val_accuracy: 0.2713\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 13.4409 - accuracy: 0.2784 - val_loss: 328.6263 - val_accuracy: 0.2757\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 12.6490 - accuracy: 0.2841 - val_loss: 325.1081 - val_accuracy: 0.2754\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 13.1506 - accuracy: 0.2874 - val_loss: 313.9630 - val_accuracy: 0.2702\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 13.6784 - accuracy: 0.2859 - val_loss: 315.5846 - val_accuracy: 0.2681\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 16.5681 - accuracy: 0.2832 - val_loss: 325.1637 - val_accuracy: 0.2702\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 15.3768 - accuracy: 0.2863 - val_loss: 316.0552 - val_accuracy: 0.2707\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 13.6661 - accuracy: 0.2829 - val_loss: 310.5654 - val_accuracy: 0.2756\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 13.1607 - accuracy: 0.2857 - val_loss: 322.2340 - val_accuracy: 0.2785\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 13.7184 - accuracy: 0.2826 - val_loss: 325.3522 - val_accuracy: 0.2831\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 14.1457 - accuracy: 0.2846 - val_loss: 328.0563 - val_accuracy: 0.2872\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 14.4215 - accuracy: 0.2859 - val_loss: 319.6816 - val_accuracy: 0.2859\n",
      "Epoch 160/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 13.8209 - accuracy: 0.2861 - val_loss: 310.2162 - val_accuracy: 0.2793\n",
      "Epoch 161/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 12.3603 - accuracy: 0.2849 - val_loss: 317.8448 - val_accuracy: 0.2815\n",
      "Epoch 162/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 12.2050 - accuracy: 0.2815 - val_loss: 314.5682 - val_accuracy: 0.2830\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 13.1256 - accuracy: 0.2850 - val_loss: 317.4653 - val_accuracy: 0.2885\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 15.7930 - accuracy: 0.2792 - val_loss: 319.2688 - val_accuracy: 0.2785\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 14.4869 - accuracy: 0.2821 - val_loss: 317.4214 - val_accuracy: 0.2798\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 12.4463 - accuracy: 0.2847 - val_loss: 319.5293 - val_accuracy: 0.2807\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 12.1836 - accuracy: 0.2879 - val_loss: 328.8870 - val_accuracy: 0.2798\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 11.3659 - accuracy: 0.2874 - val_loss: 318.4139 - val_accuracy: 0.2789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 16.0524 - accuracy: 0.2789 - val_loss: 315.7791 - val_accuracy: 0.2763\n",
      "Epoch 170/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 11.2329 - accuracy: 0.2868 - val_loss: 327.8529 - val_accuracy: 0.2776\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 13.0153 - accuracy: 0.2912 - val_loss: 328.1692 - val_accuracy: 0.2735\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 14.7763 - accuracy: 0.2831 - val_loss: 326.0393 - val_accuracy: 0.2709\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 14.0411 - accuracy: 0.2858 - val_loss: 327.6120 - val_accuracy: 0.2761\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 12.3114 - accuracy: 0.2829 - val_loss: 323.6874 - val_accuracy: 0.2752\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 13.3153 - accuracy: 0.2880 - val_loss: 320.8290 - val_accuracy: 0.2763\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 10.6029 - accuracy: 0.2900 - val_loss: 318.5331 - val_accuracy: 0.2839\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 14.4198 - accuracy: 0.2890 - val_loss: 314.9274 - val_accuracy: 0.2828\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 15.2442 - accuracy: 0.2889 - val_loss: 313.2019 - val_accuracy: 0.2856\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 12.3051 - accuracy: 0.2853 - val_loss: 307.5829 - val_accuracy: 0.2883\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 14.4903 - accuracy: 0.2873 - val_loss: 308.7155 - val_accuracy: 0.2859\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 11.3059 - accuracy: 0.2878 - val_loss: 319.9068 - val_accuracy: 0.2933\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 14.6256 - accuracy: 0.2843 - val_loss: 310.5178 - val_accuracy: 0.2828\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 13.4697 - accuracy: 0.2821 - val_loss: 319.8697 - val_accuracy: 0.2772\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 12.6810 - accuracy: 0.2875 - val_loss: 328.0964 - val_accuracy: 0.2791\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 12.0809 - accuracy: 0.2885 - val_loss: 319.6621 - val_accuracy: 0.2794\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 10.7145 - accuracy: 0.2884 - val_loss: 315.7792 - val_accuracy: 0.2781\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 10.5023 - accuracy: 0.2892 - val_loss: 317.1833 - val_accuracy: 0.2804\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 10.3874 - accuracy: 0.2873 - val_loss: 312.7984 - val_accuracy: 0.2769\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 9.5227 - accuracy: 0.2894 - val_loss: 307.8321 - val_accuracy: 0.2759\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 10.0565 - accuracy: 0.2872 - val_loss: 303.4358 - val_accuracy: 0.2744\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 9.0319 - accuracy: 0.2842 - val_loss: 303.4561 - val_accuracy: 0.2813\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 9.4106 - accuracy: 0.2878 - val_loss: 307.3943 - val_accuracy: 0.2813\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 9.8812 - accuracy: 0.2892 - val_loss: 306.7970 - val_accuracy: 0.2830\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 10.0959 - accuracy: 0.2875 - val_loss: 309.9147 - val_accuracy: 0.2811\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 8.5198 - accuracy: 0.2889 - val_loss: 309.7886 - val_accuracy: 0.2781\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 9.4591 - accuracy: 0.2843 - val_loss: 311.7891 - val_accuracy: 0.2811\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 9.7229 - accuracy: 0.2857 - val_loss: 306.9007 - val_accuracy: 0.2817\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 13.3525 - accuracy: 0.2824 - val_loss: 307.9430 - val_accuracy: 0.2807\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 8.9546 - accuracy: 0.2844 - val_loss: 308.8378 - val_accuracy: 0.2854\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 9.6047 - accuracy: 0.2883 - val_loss: 302.6732 - val_accuracy: 0.2817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f18cc3c88>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_yolo_invoice_model.fit(X_train ,Y_train, epochs= 200, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 91s 3s/step - loss: 10.7827 - accuracy: 0.2874 - val_loss: 305.0119 - val_accuracy: 0.2798\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 9.7764 - accuracy: 0.2890 - val_loss: 303.7448 - val_accuracy: 0.2776\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 8.7450 - accuracy: 0.2893 - val_loss: 302.9899 - val_accuracy: 0.2769\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 9.4488 - accuracy: 0.2889 - val_loss: 305.0806 - val_accuracy: 0.2772\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 8.5985 - accuracy: 0.2880 - val_loss: 303.1087 - val_accuracy: 0.2785\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 10.2100 - accuracy: 0.2892 - val_loss: 306.9197 - val_accuracy: 0.2774\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 9.2474 - accuracy: 0.2846 - val_loss: 312.5334 - val_accuracy: 0.2763\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 8.3449 - accuracy: 0.2866 - val_loss: 308.0275 - val_accuracy: 0.2806\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 8.6690 - accuracy: 0.2884 - val_loss: 303.3714 - val_accuracy: 0.2789\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 9.6917 - accuracy: 0.2861 - val_loss: 310.0408 - val_accuracy: 0.2822\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 10.6755 - accuracy: 0.2862 - val_loss: 303.5798 - val_accuracy: 0.2802\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 7.9035 - accuracy: 0.2865 - val_loss: 309.2538 - val_accuracy: 0.2759\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 8.8741 - accuracy: 0.2893 - val_loss: 314.0266 - val_accuracy: 0.2833\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 9.1938 - accuracy: 0.2849 - val_loss: 304.7474 - val_accuracy: 0.2865\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 8.3916 - accuracy: 0.2879 - val_loss: 303.1023 - val_accuracy: 0.2907\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 8.4798 - accuracy: 0.2883 - val_loss: 301.7811 - val_accuracy: 0.2874\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 10.5586 - accuracy: 0.2885 - val_loss: 304.9473 - val_accuracy: 0.2850\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 10.1562 - accuracy: 0.2874 - val_loss: 302.1228 - val_accuracy: 0.2844\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 8.8848 - accuracy: 0.2921 - val_loss: 304.9796 - val_accuracy: 0.2824\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 8.7648 - accuracy: 0.2887 - val_loss: 301.8660 - val_accuracy: 0.2820\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 7.8394 - accuracy: 0.2907 - val_loss: 301.9965 - val_accuracy: 0.2831\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 8.5172 - accuracy: 0.2864 - val_loss: 302.7452 - val_accuracy: 0.2824\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 9.4743 - accuracy: 0.2888 - val_loss: 306.1465 - val_accuracy: 0.2819\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 8.1574 - accuracy: 0.2903 - val_loss: 309.4730 - val_accuracy: 0.2835\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 7.8972 - accuracy: 0.2893 - val_loss: 303.1358 - val_accuracy: 0.2822\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 9.9366 - accuracy: 0.2869 - val_loss: 306.3344 - val_accuracy: 0.2819\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 7.0180 - accuracy: 0.2886 - val_loss: 307.6800 - val_accuracy: 0.2822\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 7.6622 - accuracy: 0.2872 - val_loss: 302.8929 - val_accuracy: 0.2835\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 7.2723 - accuracy: 0.2896 - val_loss: 305.1090 - val_accuracy: 0.2833\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 7.0909 - accuracy: 0.2870 - val_loss: 303.7517 - val_accuracy: 0.2830\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 6.7869 - accuracy: 0.2893 - val_loss: 300.9318 - val_accuracy: 0.2804\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 8.3998 - accuracy: 0.2870 - val_loss: 303.2329 - val_accuracy: 0.2778\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 7.4221 - accuracy: 0.2849 - val_loss: 305.5618 - val_accuracy: 0.2781\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 7.9791 - accuracy: 0.2859 - val_loss: 306.2085 - val_accuracy: 0.2781\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 8.6407 - accuracy: 0.2886 - val_loss: 303.7735 - val_accuracy: 0.2774\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 7.4949 - accuracy: 0.2866 - val_loss: 300.8055 - val_accuracy: 0.2800\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 6.6915 - accuracy: 0.2905 - val_loss: 294.4681 - val_accuracy: 0.2819\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 7.1261 - accuracy: 0.2869 - val_loss: 295.4897 - val_accuracy: 0.2841\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 7.6467 - accuracy: 0.2886 - val_loss: 294.4901 - val_accuracy: 0.2835\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 7.1893 - accuracy: 0.2879 - val_loss: 299.4739 - val_accuracy: 0.2815\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 6.9365 - accuracy: 0.2856 - val_loss: 301.3618 - val_accuracy: 0.2800\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 7.3093 - accuracy: 0.2875 - val_loss: 306.0556 - val_accuracy: 0.2826\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 6.8064 - accuracy: 0.2896 - val_loss: 305.6112 - val_accuracy: 0.2856\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 7.5745 - accuracy: 0.2898 - val_loss: 299.9618 - val_accuracy: 0.2870\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 9.3438 - accuracy: 0.2863 - val_loss: 299.2692 - val_accuracy: 0.2850\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 8.7074 - accuracy: 0.2867 - val_loss: 300.6124 - val_accuracy: 0.2854\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 6.1052 - accuracy: 0.2852 - val_loss: 300.1940 - val_accuracy: 0.2869\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 7.4726 - accuracy: 0.2897 - val_loss: 303.7331 - val_accuracy: 0.2881\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 6.6402 - accuracy: 0.2873 - val_loss: 299.4157 - val_accuracy: 0.2848\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 6.8496 - accuracy: 0.2865 - val_loss: 296.0695 - val_accuracy: 0.2867\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 6.8419 - accuracy: 0.2854 - val_loss: 300.5549 - val_accuracy: 0.2874\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 6.7550 - accuracy: 0.2860 - val_loss: 302.1045 - val_accuracy: 0.2863\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 7.2602 - accuracy: 0.2861 - val_loss: 301.2569 - val_accuracy: 0.2831\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 9.3276 - accuracy: 0.2843 - val_loss: 313.1978 - val_accuracy: 0.2857\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 8.1725 - accuracy: 0.2919 - val_loss: 324.8872 - val_accuracy: 0.2806\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 11.1113 - accuracy: 0.2835 - val_loss: 307.0952 - val_accuracy: 0.2665\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 100s 4s/step - loss: 7.7975 - accuracy: 0.2865 - val_loss: 304.5408 - val_accuracy: 0.2769\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 7.8631 - accuracy: 0.2910 - val_loss: 309.1048 - val_accuracy: 0.2794\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 6.6601 - accuracy: 0.2890 - val_loss: 306.8216 - val_accuracy: 0.2770\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 7.7269 - accuracy: 0.2907 - val_loss: 307.0088 - val_accuracy: 0.2830\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 7.9092 - accuracy: 0.2862 - val_loss: 296.4494 - val_accuracy: 0.2787\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 7.5478 - accuracy: 0.2847 - val_loss: 303.0287 - val_accuracy: 0.2802\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 6.6783 - accuracy: 0.2887 - val_loss: 309.5240 - val_accuracy: 0.2831\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 9.7828 - accuracy: 0.2860 - val_loss: 295.5842 - val_accuracy: 0.2702\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 6.5525 - accuracy: 0.2862 - val_loss: 293.6416 - val_accuracy: 0.2752\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 6.7637 - accuracy: 0.2885 - val_loss: 310.1164 - val_accuracy: 0.2770\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 7.0702 - accuracy: 0.2887 - val_loss: 308.3715 - val_accuracy: 0.2717\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 6.6750 - accuracy: 0.2899 - val_loss: 296.6431 - val_accuracy: 0.2754\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 6.4729 - accuracy: 0.2850 - val_loss: 295.3454 - val_accuracy: 0.2776\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 6.6640 - accuracy: 0.2870 - val_loss: 293.1905 - val_accuracy: 0.2787\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 5.9927 - accuracy: 0.2875 - val_loss: 285.7530 - val_accuracy: 0.2787\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 6.7104 - accuracy: 0.2856 - val_loss: 288.8067 - val_accuracy: 0.2776\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 7.1664 - accuracy: 0.2857 - val_loss: 296.9723 - val_accuracy: 0.2754\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 7.4206 - accuracy: 0.2860 - val_loss: 299.0610 - val_accuracy: 0.2715\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 6.2384 - accuracy: 0.2846 - val_loss: 294.6173 - val_accuracy: 0.2763\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 6.1210 - accuracy: 0.2823 - val_loss: 290.6943 - val_accuracy: 0.2759\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 6.2729 - accuracy: 0.2831 - val_loss: 298.4612 - val_accuracy: 0.2789\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 6.4770 - accuracy: 0.2850 - val_loss: 296.8439 - val_accuracy: 0.2807\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 5.9672 - accuracy: 0.2850 - val_loss: 289.6581 - val_accuracy: 0.2756\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 5.3508 - accuracy: 0.2847 - val_loss: 301.6159 - val_accuracy: 0.2831\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 5.5385 - accuracy: 0.2879 - val_loss: 291.0736 - val_accuracy: 0.2819\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 5.3078 - accuracy: 0.2869 - val_loss: 289.2935 - val_accuracy: 0.2824\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 5.3464 - accuracy: 0.2895 - val_loss: 290.2095 - val_accuracy: 0.2824\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 4.7552 - accuracy: 0.2859 - val_loss: 290.6059 - val_accuracy: 0.2820\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 5.8657 - accuracy: 0.2888 - val_loss: 287.4861 - val_accuracy: 0.2824\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 5.0301 - accuracy: 0.2852 - val_loss: 282.9655 - val_accuracy: 0.2785\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 5.0086 - accuracy: 0.2850 - val_loss: 286.3891 - val_accuracy: 0.2819\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 5.8482 - accuracy: 0.2871 - val_loss: 281.7945 - val_accuracy: 0.2833\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 4.5455 - accuracy: 0.2887 - val_loss: 285.5700 - val_accuracy: 0.2820\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 5.5278 - accuracy: 0.2866 - val_loss: 291.9313 - val_accuracy: 0.2809\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 5.5140 - accuracy: 0.2872 - val_loss: 284.9159 - val_accuracy: 0.2820\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 5.2983 - accuracy: 0.2880 - val_loss: 280.4777 - val_accuracy: 0.2878\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 5.5088 - accuracy: 0.2872 - val_loss: 284.8332 - val_accuracy: 0.2915\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 5.9906 - accuracy: 0.2885 - val_loss: 275.7645 - val_accuracy: 0.2872\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 6.3946 - accuracy: 0.2860 - val_loss: 274.2074 - val_accuracy: 0.2867\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 5.2368 - accuracy: 0.2886 - val_loss: 278.8615 - val_accuracy: 0.2852\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 5.1477 - accuracy: 0.2856 - val_loss: 275.0349 - val_accuracy: 0.2867\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 5.2468 - accuracy: 0.2875 - val_loss: 278.0965 - val_accuracy: 0.2852\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 6.4152 - accuracy: 0.2878 - val_loss: 276.2861 - val_accuracy: 0.2859\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 7.5611 - accuracy: 0.2882 - val_loss: 276.4491 - val_accuracy: 0.2846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f18c56c88>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_yolo_invoice_model.fit(X_train ,Y_train, epochs= 100, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_mask = Y_train[..., 4:5]\n",
    "no_obj_mask = 1. - obj_mask\n",
    "\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "Y_pred = np.zeros((28, 6750))\n",
    "Y_pred = np.reshape(Y_pred, [28, 15, 15, 3, 10])\n",
    "\n",
    "\n",
    "t_x_pred = Y_pred[..., 0:1]\n",
    "t_y_pred = Y_pred[..., 1:2]\n",
    "t_w_pred = Y_pred[..., 2:3]\n",
    "t_h_pred = Y_pred[..., 3:4]\n",
    "        \n",
    "t_x_true = Y_train[..., 0:1]\n",
    "t_y_true = Y_train[..., 1:2]\n",
    "t_w_true = Y_train[..., 2:3]\n",
    "t_h_true = Y_train[..., 3:4]\n",
    "        \n",
    "box_loss = K.square(t_x_pred - t_x_true) + K.square(t_y_pred - t_y_true) + K.square(t_w_pred - t_w_true) + K.square(t_h_pred - t_h_true)\n",
    "box_loss = 5. * K.sum(box_loss * obj_mask) / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464.0127166977621\n"
     ]
    }
   ],
   "source": [
    "print(K.eval(K.sum(K.square(t_x_pred - t_x_true) * obj_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    }
   ],
   "source": [
    "print(K.eval(K.sum(K.square(t_y_pred - t_y_true) * obj_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1679.946553050103\n"
     ]
    }
   ],
   "source": [
    "print(K.eval(K.sum(K.square(t_w_pred - t_w_true) * obj_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657.7068980551937\n"
     ]
    }
   ],
   "source": [
    "print(K.eval(K.sum(K.square(t_h_pred - t_h_true) * obj_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.35139683],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [-2.221617  ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [       -inf]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [-1.3460248 ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.08701088]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       "\n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[2,...,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " '/home/scar3crow/Downloads/8-6-new-scan/114a.jpg',\n",
       " array([[  3.,  14., 145.,  78.],\n",
       "        [218.,  21., 264.,  45.],\n",
       "        [320.,  16., 379.,  41.],\n",
       "        [217.,  61., 310.,  90.],\n",
       "        [  6.,  83., 195., 129.]], dtype=float32),\n",
       " array([0, 1, 2, 3, 4]),\n",
       " 416,\n",
       " 138]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_line[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 480, 480, 3)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = X_train[2]\n",
    "XXX = np.expand_dims(xx, axis=0)\n",
    "XXX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = my_yolo_invoice_model.predict(XXX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "          0.       ,   0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "          0.       ,   0.       ,   0.       ,   0.       ,   0.       ],\n",
       "       [  0.7003671, -16.118095 ,  -3.6128194,  -3.1377852,   1.       ,\n",
       "          1.       ,   0.       ,   0.       ,   0.       ,   0.       ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[2, 5, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.7255685 ,  -0.74695927,  -1.9000641 ,  -1.6721799 ,\n",
       "         -5.877833  , -10.6284685 ,  -1.8506002 ,  -7.5909595 ,\n",
       "         -1.7050235 ,  -6.371971  ],\n",
       "       [ -1.2239833 ,   2.140383  ,   1.2755095 ,   0.6673165 ,\n",
       "         -9.753118  ,  -8.85055   ,  -5.78881   ,  -2.8534703 ,\n",
       "         -1.9281044 ,  -6.7498527 ],\n",
       "       [  0.8463026 , -13.92913   ,  -3.3317375 ,  -2.5032406 ,\n",
       "          2.034382  ,  -6.839702  ,  -8.581668  ,  -9.369587  ,\n",
       "         -5.6633873 ,  -6.6871963 ]], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_1[0,5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.0001)\n",
    "\n",
    "my_model_1.compile(optimizer= opt, loss = my_custom_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 515s 18s/step - loss: 1279.8291 - accuracy: 0.1161 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 114s 4s/step - loss: 772.4225 - accuracy: 0.3301 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 90s 3s/step - loss: 564.2575 - accuracy: 0.2491 - val_loss: nan - val_accuracy: 0.0130\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 442.2053 - accuracy: 0.2328 - val_loss: nan - val_accuracy: 0.2589\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 363.7374 - accuracy: 0.2224 - val_loss: 772.8564 - val_accuracy: 0.1341\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 306.5204 - accuracy: 0.2093 - val_loss: 738.4301 - val_accuracy: 0.1107\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 287.5654 - accuracy: 0.2099 - val_loss: 702.8107 - val_accuracy: 0.1430\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 223.6559 - accuracy: 0.1724 - val_loss: 674.6861 - val_accuracy: 0.0857\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 273.7436 - accuracy: 0.1657 - val_loss: 623.2585 - val_accuracy: 0.1085\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 188.2835 - accuracy: 0.1684 - val_loss: 618.8415 - val_accuracy: 0.1033\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 191.7727 - accuracy: 0.1949 - val_loss: 597.7497 - val_accuracy: 0.1637\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 132.5792 - accuracy: 0.1861 - val_loss: 567.9268 - val_accuracy: 0.2607\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 181.6801 - accuracy: 0.1855 - val_loss: 538.4476 - val_accuracy: 0.1811\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 116.5308 - accuracy: 0.1864 - val_loss: 511.7962 - val_accuracy: 0.1646\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 105.1196 - accuracy: 0.1768 - val_loss: 498.2715 - val_accuracy: 0.1404\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 93.0227 - accuracy: 0.1862 - val_loss: 510.4243 - val_accuracy: 0.2067\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 90.4306 - accuracy: 0.1757 - val_loss: 475.9307 - val_accuracy: 0.2674\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 84.5114 - accuracy: 0.1690 - val_loss: 460.9191 - val_accuracy: 0.2678\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 109.0575 - accuracy: 0.1727 - val_loss: 435.2648 - val_accuracy: 0.2737\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 72.0543 - accuracy: 0.1765 - val_loss: 424.6064 - val_accuracy: 0.2887\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 69.2263 - accuracy: 0.1841 - val_loss: 411.3962 - val_accuracy: 0.3083\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 56.1804 - accuracy: 0.1824 - val_loss: 403.4689 - val_accuracy: 0.3007\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 58.5982 - accuracy: 0.1814 - val_loss: 377.2534 - val_accuracy: 0.3459\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 39.5752 - accuracy: 0.1863 - val_loss: 381.1726 - val_accuracy: 0.3019\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 39.2847 - accuracy: 0.1806 - val_loss: 365.1185 - val_accuracy: 0.3254\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 35.8665 - accuracy: 0.1814 - val_loss: 360.6452 - val_accuracy: 0.2841\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 34.2275 - accuracy: 0.1814 - val_loss: 346.3394 - val_accuracy: 0.2933\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 33.0628 - accuracy: 0.1750 - val_loss: 341.2237 - val_accuracy: 0.2809\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 75.7313 - accuracy: 0.1720 - val_loss: 338.5725 - val_accuracy: 0.2167\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 42.8403 - accuracy: 0.1699 - val_loss: 336.5138 - val_accuracy: 0.2637\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 34.7544 - accuracy: 0.1795 - val_loss: 328.1185 - val_accuracy: 0.2730\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 34.2177 - accuracy: 0.1848 - val_loss: 328.0931 - val_accuracy: 0.2557\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 31.7036 - accuracy: 0.1808 - val_loss: 311.1652 - val_accuracy: 0.2580\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 30.6408 - accuracy: 0.1831 - val_loss: 312.0478 - val_accuracy: 0.2569\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 30.5364 - accuracy: 0.1828 - val_loss: 298.5125 - val_accuracy: 0.2646\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 31.4131 - accuracy: 0.1855 - val_loss: 294.5099 - val_accuracy: 0.2417\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 35.5053 - accuracy: 0.1826 - val_loss: 296.0683 - val_accuracy: 0.2561\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 32.4910 - accuracy: 0.1823 - val_loss: 290.2509 - val_accuracy: 0.2533\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 27.7890 - accuracy: 0.1770 - val_loss: 277.2557 - val_accuracy: 0.2439\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 90s 3s/step - loss: 27.0704 - accuracy: 0.1734 - val_loss: 273.8783 - val_accuracy: 0.2304\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 26.9433 - accuracy: 0.1819 - val_loss: 272.7263 - val_accuracy: 0.2715\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 28.7275 - accuracy: 0.1774 - val_loss: 264.0511 - val_accuracy: 0.2537\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 22.7541 - accuracy: 0.1816 - val_loss: 266.5807 - val_accuracy: 0.2369\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 26.1028 - accuracy: 0.1804 - val_loss: 258.9209 - val_accuracy: 0.2357\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 22.3857 - accuracy: 0.1778 - val_loss: 266.1066 - val_accuracy: 0.2550\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 21.6152 - accuracy: 0.1789 - val_loss: 259.1825 - val_accuracy: 0.2450\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 20.9471 - accuracy: 0.1770 - val_loss: 261.3178 - val_accuracy: 0.2394\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 21.9615 - accuracy: 0.1753 - val_loss: 263.2635 - val_accuracy: 0.2409\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 23.2872 - accuracy: 0.1729 - val_loss: 253.8181 - val_accuracy: 0.2261\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 21.0424 - accuracy: 0.1748 - val_loss: 257.8559 - val_accuracy: 0.2302\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 23.1593 - accuracy: 0.1746 - val_loss: 259.2041 - val_accuracy: 0.2365\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 27.8884 - accuracy: 0.1756 - val_loss: 258.3115 - val_accuracy: 0.2380\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 23.7539 - accuracy: 0.1790 - val_loss: 256.3113 - val_accuracy: 0.2320\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 22.1915 - accuracy: 0.1751 - val_loss: 258.1005 - val_accuracy: 0.2257\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 20.4402 - accuracy: 0.1802 - val_loss: 258.0313 - val_accuracy: 0.2267\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 21.6192 - accuracy: 0.1770 - val_loss: 259.0742 - val_accuracy: 0.2289\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 93s 3s/step - loss: 22.7000 - accuracy: 0.1726 - val_loss: 252.3158 - val_accuracy: 0.2239\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 19.5024 - accuracy: 0.1780 - val_loss: 252.7375 - val_accuracy: 0.2231\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 31.0929 - accuracy: 0.1740 - val_loss: 250.1847 - val_accuracy: 0.2087\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 20.6367 - accuracy: 0.1739 - val_loss: 251.4698 - val_accuracy: 0.2209\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 122s 4s/step - loss: 18.0151 - accuracy: 0.1741 - val_loss: 248.3019 - val_accuracy: 0.2144\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 118s 4s/step - loss: 18.0126 - accuracy: 0.1730 - val_loss: 247.1706 - val_accuracy: 0.2213\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 17.4717 - accuracy: 0.1748 - val_loss: 249.9970 - val_accuracy: 0.2267\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 19.8250 - accuracy: 0.1800 - val_loss: 250.1791 - val_accuracy: 0.2248\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 120s 4s/step - loss: 16.6674 - accuracy: 0.1757 - val_loss: 251.0729 - val_accuracy: 0.2224\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 16.1456 - accuracy: 0.1784 - val_loss: 252.0294 - val_accuracy: 0.2235\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 16.4162 - accuracy: 0.1755 - val_loss: 247.3229 - val_accuracy: 0.2204\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 18.7852 - accuracy: 0.1784 - val_loss: 247.4310 - val_accuracy: 0.2144\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 20.3607 - accuracy: 0.1779 - val_loss: 246.4209 - val_accuracy: 0.2137\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 18.0132 - accuracy: 0.1751 - val_loss: 243.6086 - val_accuracy: 0.2109\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 17.2440 - accuracy: 0.1748 - val_loss: 244.4885 - val_accuracy: 0.2189\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 15.4435 - accuracy: 0.1796 - val_loss: 244.2466 - val_accuracy: 0.2148\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 115s 4s/step - loss: 16.6724 - accuracy: 0.1741 - val_loss: 241.4795 - val_accuracy: 0.2161\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 15.2232 - accuracy: 0.1751 - val_loss: 244.3025 - val_accuracy: 0.2161\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 15.2206 - accuracy: 0.1768 - val_loss: 234.6883 - val_accuracy: 0.2161\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 13.9158 - accuracy: 0.1742 - val_loss: 237.1222 - val_accuracy: 0.2104\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 14.9998 - accuracy: 0.1731 - val_loss: 232.8546 - val_accuracy: 0.2115\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 17.5964 - accuracy: 0.1770 - val_loss: 236.4549 - val_accuracy: 0.2115\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 14.5165 - accuracy: 0.1768 - val_loss: 232.6472 - val_accuracy: 0.2143\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 13.2409 - accuracy: 0.1760 - val_loss: 232.9197 - val_accuracy: 0.2172\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 13.3336 - accuracy: 0.1777 - val_loss: 232.4573 - val_accuracy: 0.2169\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 14.5089 - accuracy: 0.1753 - val_loss: 232.1342 - val_accuracy: 0.2144\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 13.0945 - accuracy: 0.1757 - val_loss: 237.0144 - val_accuracy: 0.2137\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 13.5643 - accuracy: 0.1762 - val_loss: 239.3802 - val_accuracy: 0.2124\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 14.6888 - accuracy: 0.1762 - val_loss: 236.5032 - val_accuracy: 0.2111\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 12.4259 - accuracy: 0.1755 - val_loss: 228.2522 - val_accuracy: 0.2091\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 12.4193 - accuracy: 0.1756 - val_loss: 230.0559 - val_accuracy: 0.2143\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 12.0993 - accuracy: 0.1769 - val_loss: 228.9956 - val_accuracy: 0.2096\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 12.9327 - accuracy: 0.1777 - val_loss: 224.9182 - val_accuracy: 0.2069\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 13.7989 - accuracy: 0.1763 - val_loss: 224.7212 - val_accuracy: 0.2093\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 13.1916 - accuracy: 0.1769 - val_loss: 227.5918 - val_accuracy: 0.2069\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 92s 3s/step - loss: 13.0666 - accuracy: 0.1779 - val_loss: 225.5762 - val_accuracy: 0.2031\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 12.4637 - accuracy: 0.1800 - val_loss: 229.5429 - val_accuracy: 0.2078\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 11.9766 - accuracy: 0.1785 - val_loss: 222.2989 - val_accuracy: 0.2031\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 13.0512 - accuracy: 0.1804 - val_loss: 225.5246 - val_accuracy: 0.1991\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 12.6391 - accuracy: 0.1767 - val_loss: 220.3720 - val_accuracy: 0.1926\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 11.7100 - accuracy: 0.1765 - val_loss: 224.2268 - val_accuracy: 0.1976\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 11.8015 - accuracy: 0.1792 - val_loss: 221.7305 - val_accuracy: 0.1933\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 11.1191 - accuracy: 0.1794 - val_loss: 221.7540 - val_accuracy: 0.1976\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 12.1483 - accuracy: 0.1805 - val_loss: 221.4116 - val_accuracy: 0.1980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0f0b1968d0>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_1.fit(X_train ,Y_train, epochs= 100, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ = my_model_1.predict(XXX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.11884286,  -0.29294455,  -3.8280103 ,  -4.58905   ,\n",
       "         -7.485161  ,  -1.4699419 ,   1.3593652 ,  -2.5104914 ,\n",
       "         -0.4770757 ,   1.8778927 ],\n",
       "       [ -1.7047958 ,   1.9640722 ,  -2.9230008 ,  -4.355827  ,\n",
       "         -8.039365  ,  -1.657332  ,  -1.5517566 ,  -3.4764194 ,\n",
       "         -1.9580714 ,   1.997337  ],\n",
       "       [  0.8648745 , -15.780776  ,  -3.6722846 ,  -3.1603458 ,\n",
       "         -3.936662  ,   0.6864209 ,  -2.154814  ,   0.01957085,\n",
       "         -2.6341605 ,   4.421023  ]], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_[0,5,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
