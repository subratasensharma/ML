{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/scar3crow/Dropbox/WorkStation-Subrata/python/models/research/object_detection\n"
     ]
    }
   ],
   "source": [
    "cd models/research/object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tqdm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import *\n",
    "\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications import InceptionResNetV2\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = [480, 480]\n",
    "target_w = 480 # target sizes of image in model input\n",
    "target_h = 480 #target sizes of image in model input\n",
    "\n",
    "grid_size = [15, 15]\n",
    "grid_y_axis = 15  # each image is to be segmented to 13 x 13 grid\n",
    "grid_x_axis = 15  # # each image is to be segmented to 13 x 13 grid\n",
    "\n",
    "grid_w = target_w / grid_x_axis  # grid cell width\n",
    "grid_h = target_h / grid_y_axis  # grid cell height\n",
    "\n",
    "channels = 3\n",
    "num_anchors = 3\n",
    "class_num = 5 # vendor, invoice, inv_date, po, buyer\n",
    "info = 5 + class_num    # pc, x, y, h, w, and class probabilities\n",
    "\n",
    "categories = ['vendor', 'invoice', 'inv_date', 'po', 'buyer'] # details of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images =  36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/scar3crow/Downloads/8-6-new-scan/50a.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a list of image path\n",
    "\n",
    "inv_directory = '/home/scar3crow/Downloads/8-6-new-scan'  ## 'invoices' is a zip file of jpg images in ...../Downloads \n",
    "                                                        \n",
    "inv_new_image = ['/home/scar3crow/Downloads/8-6-new-scan/{}'.format(i) for i in os.listdir(inv_directory)] # making the list\n",
    "inv_new_image.sort() # Sorting the list\n",
    "\n",
    "num_images = len(inv_new_image)\n",
    "\n",
    "print('Number of images = ', num_images)\n",
    "inv_new_image[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_serial</th>\n",
       "      <th>rows</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/101a.jpg</td>\n",
       "      <td>160</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/102a.jpg</td>\n",
       "      <td>406</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/103a.jpg</td>\n",
       "      <td>260</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      image_serial  rows  columns\n",
       "0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg   160      416\n",
       "1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg   406      870\n",
       "2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   260      416"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check sizes of exiting images & Create a Dataframe with image id and height(row) and width(column):\n",
    "\n",
    "rows = []\n",
    "columns = []\n",
    "image_sl = []\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "for i in range(len(inv_new_image)):\n",
    "    image = cv2.imread(inv_new_image[i]) ## Loading image\n",
    "    height, width, _ = image.shape\n",
    "    rows.append(height)\n",
    "    columns.append(width)\n",
    "    image_sl.append(inv_new_image[i])\n",
    "    \n",
    "row_values = pd.Series(rows)\n",
    "col_values = pd.Series(columns)\n",
    "image_num = pd.Series(image_sl)\n",
    "\n",
    "\n",
    "df_new.insert(loc=0, column='image_serial', value=image_num)\n",
    "df_new.insert(loc=1, column='rows', value=row_values)\n",
    "df_new.insert(loc=2, column='columns', value=col_values)\n",
    "\n",
    "df_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes =  5\n",
      "Number of unique images =  36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#filename</th>\n",
       "      <th>region_shape_attributes</th>\n",
       "      <th>region_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>63a.jpg</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":211,\"y\":64,\"width\":76,\"heig...</td>\n",
       "      <td>{\"text\":\"po\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>63a.jpg</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":2,\"y\":68,\"width\":165,\"heigh...</td>\n",
       "      <td>{\"text\":\"buyer\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>101a.jpg</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":6,\"y\":23,\"width\":119,\"heigh...</td>\n",
       "      <td>{\"text\":\"vendor\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #filename                            region_shape_attributes  \\\n",
       "58   63a.jpg  {\"name\":\"rect\",\"x\":211,\"y\":64,\"width\":76,\"heig...   \n",
       "59   63a.jpg  {\"name\":\"rect\",\"x\":2,\"y\":68,\"width\":165,\"heigh...   \n",
       "60  101a.jpg  {\"name\":\"rect\",\"x\":6,\"y\":23,\"width\":119,\"heigh...   \n",
       "\n",
       "    region_attributes  \n",
       "58      {\"text\":\"po\"}  \n",
       "59   {\"text\":\"buyer\"}  \n",
       "60  {\"text\":\"vendor\"}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading output of VGG Image Annotation tool and create a dataframe\n",
    "\n",
    "r_new_data = pd.read_csv('/home/scar3crow/Downloads/via_new_data.csv')\n",
    "num_obj = r_new_data['region_count'][0] # number of objects in each photo\n",
    "r_new_data.drop(r_new_data.columns[[1, 2, 3, 4]], axis=1, inplace=True) # reduce unnecessary columns\n",
    "r_new_data.sort_values(by=['#filename'], ascending=True) # Sorting based on image-id\n",
    "num_images = r_new_data[\"#filename\"].nunique() # Find out number of unique images\n",
    "\n",
    "print('Number of classes = ', num_obj)\n",
    "print('Number of unique images = ', num_images)\n",
    "r_new_data[58:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>img_idx</th>\n",
       "      <th>i_path</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>obj_class</th>\n",
       "      <th>img_wd</th>\n",
       "      <th>img_ht</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50a.jpg</td>\n",
       "      <td>24</td>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/50a.jpg</td>\n",
       "      <td>221</td>\n",
       "      <td>59</td>\n",
       "      <td>103</td>\n",
       "      <td>24</td>\n",
       "      <td>po</td>\n",
       "      <td>416</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50a.jpg</td>\n",
       "      <td>24</td>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/50a.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>206</td>\n",
       "      <td>56</td>\n",
       "      <td>buyer</td>\n",
       "      <td>416</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51a.jpg</td>\n",
       "      <td>25</td>\n",
       "      <td>/home/scar3crow/Downloads/8-6-new-scan/51a.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>56</td>\n",
       "      <td>vendor</td>\n",
       "      <td>416</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    img_id  img_idx                                          i_path    x   y  \\\n",
       "3  50a.jpg       24  /home/scar3crow/Downloads/8-6-new-scan/50a.jpg  221  59   \n",
       "4  50a.jpg       24  /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5  57   \n",
       "5  51a.jpg       25  /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    5   0   \n",
       "\n",
       "   width  height obj_class  img_wd  img_ht  \n",
       "3    103      24        po     416     209  \n",
       "4    206      56     buyer     416     209  \n",
       "5    120      56    vendor     416     194  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a dataframe for Image_id, x, y, width, height, class, image_width and image_height\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "width = []\n",
    "height = []\n",
    "obj_class = []\n",
    "i_width = []\n",
    "i_height = []\n",
    "img_path = []\n",
    "img_index = []\n",
    "\n",
    "for i in range(len(r_new_data)):\n",
    "    \n",
    "    r_size = r_new_data.values[i, 1][1:(len(r_new_data.values[i, 1])-1)]\n",
    "    r_size_par = r_size.split(\",\")\n",
    "    \n",
    "    x.append(int(\"\".join(filter(str.isdigit, r_size_par[1]))))\n",
    "    y.append(int(\"\".join(filter(str.isdigit, r_size_par[2]))))\n",
    "    width.append(int(\"\".join(filter(str.isdigit, r_size_par[3]))))\n",
    "    height.append(int(\"\".join(filter(str.isdigit, r_size_par[4]))))\n",
    "    \n",
    "    r_attribs = r_new_data.values[i, 2][1:(len(r_new_data.values[i, 2])-1)]\n",
    "    r_attribs_par = r_attribs.split(':')[1]\n",
    "    obj_class.append(r_attribs_par[1:(len(r_attribs_par)-1)])\n",
    "    \n",
    "    foto_id = r_new_data['#filename'][i]\n",
    "    i_path = '/home/scar3crow/Downloads/8-6-new-scan/' + foto_id\n",
    "    foto_index = int(df_new[df_new['image_serial'] == i_path].index[0])\n",
    "    foto_width = df_new['columns'][foto_index]\n",
    "    foto_height = df_new['rows'][foto_index]\n",
    "    i_width.append(foto_width)\n",
    "    i_height.append(foto_height)\n",
    "    img_path.append(i_path)\n",
    "    img_index.append(foto_index)\n",
    "    \n",
    "x_values = pd.Series(x)\n",
    "y_values = pd.Series(y)\n",
    "width_values = pd.Series(width)\n",
    "height_values = pd.Series(height)\n",
    "class_values = pd.Series(obj_class)\n",
    "i_width_values = pd.Series(i_width)\n",
    "i_height_values = pd.Series(i_height)\n",
    "img_path_values = pd.Series(img_path)\n",
    "img_index_values = pd.Series(img_index)\n",
    "\n",
    "r_new_data.insert(loc=1, column='img_idx', value=img_index_values)\n",
    "r_new_data.insert(loc=2, column='i_path', value=img_path_values)\n",
    "r_new_data.insert(loc=3, column='x', value=x_values)\n",
    "r_new_data.insert(loc=4, column='y', value=y_values)\n",
    "r_new_data.insert(loc=5, column='width', value=width_values)\n",
    "r_new_data.insert(loc=6, column='height', value=height_values)\n",
    "r_new_data.insert(loc=7, column='obj_class', value=class_values)\n",
    "r_new_data.insert(loc=8, column='img_wd', value=i_width_values)\n",
    "r_new_data.insert(loc=9, column='img_ht', value=i_height_values)\n",
    "\n",
    "r_new_data.drop(r_new_data.columns[[10, 11]], axis=1, inplace=True) # reduce unnecessary columns\n",
    "\n",
    "r_new_data.rename({'#filename': 'img_id'}, axis=1, inplace=True) # changing column name\n",
    "\n",
    "r_new_data[3:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images =  36\n",
      "Number of classes in diff. categories =  buyer      38\n",
      "invoice    36\n",
      "vendor     36\n",
      "date       36\n",
      "po         33\n",
      "order       1\n",
      "Name: obj_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique images = ', r_new_data['img_id'].nunique())  # print total no, of unique images\n",
    "print('Number of classes in diff. categories = ', r_new_data['obj_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[       img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ,\n",
       "        img_id  img_idx                                           i_path    x  \\\n",
       " 4     50a.jpg       24   /home/scar3crow/Downloads/8-6-new-scan/50a.jpg    5   \n",
       " 9     51a.jpg       25   /home/scar3crow/Downloads/8-6-new-scan/51a.jpg    4   \n",
       " 14    52a.jpg       26   /home/scar3crow/Downloads/8-6-new-scan/52a.jpg    1   \n",
       " 19    53a.jpg       27   /home/scar3crow/Downloads/8-6-new-scan/53a.jpg    0   \n",
       " 24    54a.jpg       28   /home/scar3crow/Downloads/8-6-new-scan/54a.jpg   31   \n",
       " 29    55a.jpg       29   /home/scar3crow/Downloads/8-6-new-scan/55a.jpg    1   \n",
       " 34    56a.jpg       30   /home/scar3crow/Downloads/8-6-new-scan/56a.jpg    1   \n",
       " 39    59a.jpg       31   /home/scar3crow/Downloads/8-6-new-scan/59a.jpg    3   \n",
       " 44    60a.jpg       32   /home/scar3crow/Downloads/8-6-new-scan/60a.jpg    0   \n",
       " 49    61a.jpg       33   /home/scar3crow/Downloads/8-6-new-scan/61a.jpg    1   \n",
       " 54    62a.jpg       34   /home/scar3crow/Downloads/8-6-new-scan/62a.jpg    4   \n",
       " 59    63a.jpg       35   /home/scar3crow/Downloads/8-6-new-scan/63a.jpg    2   \n",
       " 64   101a.jpg        0  /home/scar3crow/Downloads/8-6-new-scan/101a.jpg    6   \n",
       " 69   102a.jpg        1  /home/scar3crow/Downloads/8-6-new-scan/102a.jpg  431   \n",
       " 74   103a.jpg        2  /home/scar3crow/Downloads/8-6-new-scan/103a.jpg   12   \n",
       " 79   104a.jpg        3  /home/scar3crow/Downloads/8-6-new-scan/104a.jpg   21   \n",
       " 84   105a.jpg        4  /home/scar3crow/Downloads/8-6-new-scan/105a.jpg    4   \n",
       " 88   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg  230   \n",
       " 89   106a.jpg        5  /home/scar3crow/Downloads/8-6-new-scan/106a.jpg    1   \n",
       " 94   107a.jpg        6  /home/scar3crow/Downloads/8-6-new-scan/107a.jpg    2   \n",
       " 99   108a.jpg        7  /home/scar3crow/Downloads/8-6-new-scan/108a.jpg  219   \n",
       " 104  109a.jpg        8  /home/scar3crow/Downloads/8-6-new-scan/109a.jpg  220   \n",
       " 109  110a.jpg        9  /home/scar3crow/Downloads/8-6-new-scan/110a.jpg   22   \n",
       " 114  111a.jpg       10  /home/scar3crow/Downloads/8-6-new-scan/111a.jpg    8   \n",
       " 119  112a.jpg       11  /home/scar3crow/Downloads/8-6-new-scan/112a.jpg    2   \n",
       " 124  113a.jpg       12  /home/scar3crow/Downloads/8-6-new-scan/113a.jpg    4   \n",
       " 129  114a.jpg       13  /home/scar3crow/Downloads/8-6-new-scan/114a.jpg    6   \n",
       " 134  115a.jpg       14  /home/scar3crow/Downloads/8-6-new-scan/115a.jpg    2   \n",
       " 139  116a.jpg       15  /home/scar3crow/Downloads/8-6-new-scan/116a.jpg    4   \n",
       " 144  117a.jpg       16  /home/scar3crow/Downloads/8-6-new-scan/117a.jpg    2   \n",
       " 149  118a.jpg       17  /home/scar3crow/Downloads/8-6-new-scan/118a.jpg    1   \n",
       " 154  119a.jpg       18  /home/scar3crow/Downloads/8-6-new-scan/119a.jpg    0   \n",
       " 159  120a.jpg       19  /home/scar3crow/Downloads/8-6-new-scan/120a.jpg    1   \n",
       " 163  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg  211   \n",
       " 164  121a.jpg       20  /home/scar3crow/Downloads/8-6-new-scan/121a.jpg    1   \n",
       " 169  122a.jpg       21  /home/scar3crow/Downloads/8-6-new-scan/122a.jpg   25   \n",
       " 174  123a.jpg       22  /home/scar3crow/Downloads/8-6-new-scan/123a.jpg    1   \n",
       " 179  124a.jpg       23  /home/scar3crow/Downloads/8-6-new-scan/124a.jpg    1   \n",
       " \n",
       "        y  width  height obj_class  img_wd  img_ht  \n",
       " 4     57    206      56     buyer     416     209  \n",
       " 9     53    152      64     buyer     416     194  \n",
       " 14    50    161      74     buyer     416     188  \n",
       " 19    50    177      76     buyer     416     194  \n",
       " 24   103    186      61     buyer     416     168  \n",
       " 29    56    183      74     buyer     416     144  \n",
       " 34    56    166      62     buyer     416     123  \n",
       " 39    58    175      62     buyer     416     200  \n",
       " 44    44    165      52     buyer     416     106  \n",
       " 49    56    155      63     buyer     416     121  \n",
       " 54    58    163      61     buyer     416     123  \n",
       " 59    68    165      55     buyer     416     191  \n",
       " 64    66    142      47     buyer     416     160  \n",
       " 69   140    307     164     buyer     870     406  \n",
       " 74   126    154      68     buyer     416     260  \n",
       " 79   249    431     152     buyer     911     405  \n",
       " 84    53    158      80     buyer     416     147  \n",
       " 88    63     89      22     buyer     416     134  \n",
       " 89    53    154      72     buyer     416     134  \n",
       " 94    47    144      50     buyer     416     140  \n",
       " 99   155    141      71     buyer     416     228  \n",
       " 104  142    153      73     buyer     416     218  \n",
       " 109  144    188      60     buyer     416     211  \n",
       " 114   87    206      76     buyer     416     166  \n",
       " 119   59    177      62     buyer     416     206  \n",
       " 124  112    190      40     buyer     416     201  \n",
       " 129   83    189      46     buyer     416     138  \n",
       " 134   57    191      65     buyer     416     190  \n",
       " 139   59    175      71     buyer     416     145  \n",
       " 144   52    172      72     buyer     416     191  \n",
       " 149   56    188      65     buyer     416     168  \n",
       " 154   62    169      78     buyer     416     146  \n",
       " 159   59    171      78     buyer     416     147  \n",
       " 163   66     79      26     buyer     416     192  \n",
       " 164   68    168      58     buyer     416     192  \n",
       " 169   98    192      74     buyer     416     174  \n",
       " 174   57    171      64     buyer     416     121  \n",
       " 179  117    190      42     buyer     416     203  ]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to correct above :\n",
    "\n",
    "# To find smallest width & height boxes in 'buyer' which should be 'po'\n",
    "gb = r_new_data.groupby('obj_class')    \n",
    "[gb.get_group('buyer') for x in gb.groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images =  36\n",
      "Number of unique classes =  5\n",
      "Number of classes in diff. categories =  invoice     36\n",
      "inv_date    36\n",
      "vendor      36\n",
      "po          36\n",
      "buyer       36\n",
      "Name: obj_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Correcting above wrong spelling & converting buyer to po of object classes and rechecking\n",
    "\n",
    "id_1 = r_new_data.index[r_new_data['obj_class'] == 'order'] # Finding the index\n",
    "id_2 = r_new_data.index[r_new_data['obj_class'] == 'date'] # to change 'date' to 'inv_date' to be consistent with old data\n",
    "\n",
    "r_new_data.at[id_1, 'obj_class'] = 'po' # writing the correct spelling \n",
    "r_new_data.at[88, 'obj_class'] = 'po' # # 'buyer' to 'po'\n",
    "r_new_data.at[163, 'obj_class'] = 'po' # # 'buyer' to 'po'\n",
    "r_new_data.at[id_2, 'obj_class'] = 'inv_date' # # 'date' to 'inv_date'\n",
    "\n",
    "print('Number of unique images = ', r_new_data['img_id'].nunique())  # print total no, of unique images\n",
    "print('Number of unique classes = ', r_new_data['obj_class'].nunique())\n",
    "print('Number of classes in diff. categories = ', r_new_data['obj_class'].value_counts())\n",
    "\n",
    "# r_new_data.drop(r_new_data.columns[[0]], axis=1, inplace=True) # reduce unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each image, we have to find : (a) line_index = integer, (b) img_path = string, (c) boxes = shape [N, 4], \n",
    "## N is the ground truth count, elements in the second dimension are [x_min, y_min, x_max, y_max] (d) labels = shape\n",
    "## [N]. class index. (e) img_width = int.  =f) img_height = int\n",
    "\n",
    "def single_image_info(lines):\n",
    "    \n",
    "    ## lines will be a dataframe like, for i in range(num_images), lines = r_new_data[i*5:(i+1)*5]\n",
    "    \n",
    "    line_idx = lines.iat[0, 1]\n",
    "    pic_path = lines.iat[0, 2]\n",
    "    img_width = lines.iat[0, 8]\n",
    "    img_height = lines.iat[0, 9]\n",
    "    \n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for i in range(len(lines)):\n",
    "        label, x_min, y_min, x_max, y_max = int(i), float(lines.iat[i,3]), float(lines.iat[i,4]), float(lines.iat[i,3]+lines.iat[i,5]), float(lines.iat[i,4]+lines.iat[i,6])\n",
    "        boxes.append([x_min, y_min, x_max, y_max])\n",
    "        labels.append(label)\n",
    "        \n",
    "    boxes = np.asarray(boxes, np.float32)\n",
    "    labels = np.asarray(labels, np.int64)\n",
    "    \n",
    "    return line_idx, pic_path, boxes, labels, img_width, img_height  ## boxes are in format xmin, ymin, xmax, ymax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "[25, '/home/scar3crow/Downloads/8-6-new-scan/51a.jpg', array([[  5.,   0., 125.,  56.],\n",
      "       [239.,   1., 279.,  20.],\n",
      "       [328.,   1., 382.,  21.],\n",
      "       [238.,  51., 302.,  74.],\n",
      "       [  4.,  53., 156., 117.]], dtype=float32), array([0, 1, 2, 3, 4]), 416, 194]\n"
     ]
    }
   ],
   "source": [
    "## Creating the complete data set :\n",
    "\n",
    "all_image_line = []\n",
    "for i in range(num_images):\n",
    "    image_line = []\n",
    "    limit_lower = i*5\n",
    "    limit_upper = limit_lower+5\n",
    "    lines = r_new_data[limit_lower:limit_upper]\n",
    "    line_idx, pic_path, boxes, labels, img_width, img_height = single_image_info(lines)\n",
    "    image_line.append(line_idx)\n",
    "    image_line.append(pic_path)\n",
    "    image_line.append(boxes)\n",
    "    image_line.append(labels)\n",
    "    image_line.append(img_width)\n",
    "    image_line.append(img_height)\n",
    "    all_image_line.append(image_line)\n",
    "    \n",
    "print(len(all_image_line))\n",
    "print(all_image_line[1])   ##  boxes are in format xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 140 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scar3crow/Dropbox/WorkStation-Subrata/python/venv1/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train and Test split\n",
    "\n",
    "data_train, data_val = train_test_split(all_image_line, train_size = 0.8 , shuffle = True)\n",
    "\n",
    "num_all_bbox = len(all_image_line) * len(all_image_line[0][2])\n",
    "num_bb_train = len(data_train) * len(data_train[0][2])\n",
    "num_bb_val = len(data_val) * len(data_val[0][2])\n",
    "print(num_all_bbox, num_bb_train, num_bb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating anchors from true boundary boxes :\n",
    "\n",
    "def iou_kmeans(box, clusters):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) between a box and k clusters.\n",
    "    :param box: tuple or array, shifted to the origin (i. e. width and height)\n",
    "    :param clusters: numpy array of shape (k, 2) where k is the number of clusters\n",
    "    :return: numpy array of shape (k, 0) where k is the number of clusters\n",
    "    \"\"\"\n",
    "    x = np.minimum(clusters[:, 0], box[0])\n",
    "    y = np.minimum(clusters[:, 1], box[1])\n",
    "    if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:\n",
    "        raise ValueError(\"Box has no area\")\n",
    "\n",
    "    intersection = x * y\n",
    "    box_area = box[0] * box[1]\n",
    "    cluster_area = clusters[:, 0] * clusters[:, 1]\n",
    "\n",
    "    iou = intersection / (box_area + cluster_area - intersection)\n",
    "\n",
    "    return iou\n",
    "\n",
    "def kmeans(boxes, k, dist=np.median):\n",
    "    \"\"\"\n",
    "    Calculates k-means clustering with the Intersection over Union (IoU) metric.\n",
    "    :param boxes: numpy array of shape (r, 2), where r is the number of rows\n",
    "    :param k: number of clusters\n",
    "    :param dist: distance function\n",
    "    :return: numpy array of shape (k, 2)\n",
    "    \"\"\"\n",
    "    rows = boxes.shape[0]\n",
    "\n",
    "    distances = np.empty((rows, k))\n",
    "    last_clusters = np.zeros((rows,))\n",
    "\n",
    "    np.random.seed()\n",
    "\n",
    "    # the Forgy method will fail if the whole array contains the same rows\n",
    "    clusters = boxes[np.random.choice(rows, k, replace=False)]\n",
    "\n",
    "\n",
    "    while True:\n",
    "        for row in range(rows):\n",
    "            distances[row] = 1 - iou_kmeans(boxes[row], clusters)\n",
    "\n",
    "        nearest_clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        if (last_clusters == nearest_clusters).all():\n",
    "            break\n",
    "\n",
    "        for cluster in range(k):\n",
    "            clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=0)\n",
    "\n",
    "        last_clusters = nearest_clusters\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[ 68.07692308  65.30612245]\n",
      " [129.23076923  37.92592593]\n",
      " [189.80769231 160.36697248]]\n"
     ]
    }
   ],
   "source": [
    "## Finding out anchors :\n",
    "## Firstly, converting true boundary box width, height to width & height with respect to target image :\n",
    "## finaly find anchors. Anchors here are in absolute size w.r.t. target image but not as % of target image or \n",
    "## as multiple of unit grids.\n",
    "\n",
    "# num_all_bb = len(r_new_data) # if no. of bboxes varies for images, this formula should be used \n",
    "\n",
    "anchors_wrt_target = np.zeros((3,2))\n",
    "\n",
    "num_all_bb = len(all_image_line) * len(all_image_line[0][2])  ## from all image line data\n",
    "\n",
    "b_box_wrt_target = np.zeros((num_all_bb,2))\n",
    "\n",
    "for i in range(num_all_bb):\n",
    "    \n",
    "    image_w = r_new_data['img_wd'][i]\n",
    "    image_h = r_new_data['img_ht'][i]\n",
    "\n",
    "    x_ratio = target_w / image_w \n",
    "    y_ratio = target_h / image_h\n",
    "    \n",
    "    anchor_w = r_new_data['width'][i] * x_ratio\n",
    "    anchor_h = r_new_data['height'][i] * y_ratio\n",
    "    b_box_wrt_target[i, 0] = anchor_w\n",
    "    b_box_wrt_target[i, 1] = anchor_h\n",
    "    \n",
    "anchors_wrt_target = kmeans(b_box_wrt_target, num_anchors)\n",
    "\n",
    "print(anchors_wrt_target.shape)\n",
    "print(anchors_wrt_target)     ## anchors wrt target image in abs. value and in format width, height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing the original data to get y_true :\n",
    "\n",
    "def process_box(ori_boxes, ori_img_width, ori_img_height, labels, target_size, class_num, anchors):\n",
    "    '''\n",
    "    Generate the y_true label, i.e. the ground truth feature_map.\n",
    "    params:\n",
    "        boxes: [N, 5] shape, float32 dtype. `x_min, y_min, x_max, y_mix, mixup_weight`.\n",
    "        labels: [N] shape, int64 dtype.\n",
    "        class_num: int64 num.\n",
    "        anchors: [3,2] shape, float32 dtype.\n",
    "    '''\n",
    "    \n",
    "    img_width = ori_img_width\n",
    "    img_height = ori_img_height\n",
    "    boxes = ori_boxes           ## boxes in format xmin, ymin, xmax, ymax\n",
    "    \n",
    "    x_ratio = target_size[1] / img_width\n",
    "    y_ratio = target_size[0] / img_height\n",
    "    \n",
    "    boxes_wrt_target = np.zeros((5,4))\n",
    "    box_centers_target = np.zeros((5,2))\n",
    "\n",
    "    boxes_wrt_target[:,0] = boxes[:,0] * x_ratio  # xmin absolute value wrt target image\n",
    "    boxes_wrt_target[:,1] = boxes[:,1] * y_ratio  # ymin absolute value wrt target image\n",
    "    boxes_wrt_target[:,2] = boxes[:,2] * x_ratio  # xmax absolute value wrt target image\n",
    "    boxes_wrt_target[:,3] = boxes[:,3] * y_ratio  # ymax absolute value wrt target image\n",
    "    \n",
    "    # In above, boxes_wrt_target shape is (5, 4), now this will be taken to (5. 5) by adding 1 at end\n",
    "#    boxes_wrt_target = np.concatenate((boxes_wrt_target, np.full(shape=(boxes_wrt_target.shape[0], 1), fill_value=1., dtype=np.float32)), axis=-1)\n",
    "    box_centers_target = (boxes_wrt_target[:, 0:2] + boxes_wrt_target[:, 2:4]) / 2  ## centers wrt target, abs values\n",
    "    \n",
    "    box_sizes = boxes[:, 2:4] - boxes[:, 0:2]  #xmax-xmin = width and ymax-ymin = height wrt original image\n",
    "    box_sizes[:,0] = box_sizes[:,0] * x_ratio  # width w.r.t target image in absolute value\n",
    "    box_sizes[:,1] = box_sizes[:,1] * y_ratio  # width w.r.t target image in absolute value\n",
    "    \n",
    "#    y_true_13 = np.zeros((target_size[1] // 32, target_size[0] // 32, 3, 6 + class_num), np.float32)\n",
    "    y_true_13 = np.zeros((target_size[1] // 32, target_size[0] // 32, 3, 5 + class_num), np.float32)\n",
    "\n",
    "#    y_true = [y_true_13]\n",
    "    \n",
    "    box_sizes = np.expand_dims(box_sizes, 1)\n",
    "    mins = np.maximum(- box_sizes / 2, - anchors / 2)\n",
    "    maxs = np.minimum(box_sizes / 2, anchors / 2)\n",
    "    whs = maxs - mins\n",
    "\n",
    "    iou = (whs[:, :, 0] * whs[:, :, 1]) / (\n",
    "                box_sizes[:, :, 0] * box_sizes[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :,\n",
    "                                                                                                         1] + 1e-10)\n",
    "    best_match_idx = np.argmax(iou, axis=1)\n",
    "\n",
    "    anchor_mask = np.zeros((target_size[1] // 32, target_size[0] // 32, 3))\n",
    "\n",
    "    cell_size = 32  ## = targetsize / no. of grid cells\n",
    "    \n",
    "    for i, idx in enumerate(best_match_idx):\n",
    "\n",
    "        x = int(np.floor(box_centers_target[i, 0] / cell_size))\n",
    "        y = int(np.floor(box_centers_target[i, 1] / cell_size))\n",
    "        k = int(idx)\n",
    "        c = int(labels[i])\n",
    "\n",
    "        print(x, y, k, c)\n",
    "\n",
    "# Very Imp : Now preparing y_true: all values x_center, y_cemter, width & height are being taken to % of target image\n",
    "        \n",
    "        y_true_13[y, x, k, :2] = box_centers_target[i] / target_size[0] #since target_size[0] = target_size[1]\n",
    "        y_true_13[y, x, k, 2:4] = box_sizes[i] / target_size[0]\n",
    "        y_true_13[y, x, k, 4] = 1.\n",
    "        y_true_13[y, x, k, 5 + c] = 1.\n",
    "#        y_true[0][y, x, k, -1] = boxes_wrt_target[i, -1]\n",
    "        anchor_mask[y, x, k] = 1\n",
    "\n",
    "    return y_true_13, anchor_mask  ## all data are w.r.to target image in % of target image and NOT w,r,t, grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single image-wise image/boundary box preprocessing:\n",
    "\n",
    "def parse_data(line, class_num, target_size, anchors):   ## (mode, letterbox_resize):\n",
    "    '''\n",
    "    param:\n",
    "        line: a line from the training/test txt file\n",
    "        class_num: totol class nums.\n",
    "        target_size: the size of image to be resized to. [width, height] format.\n",
    "        anchors: anchors.\n",
    "        mode: 'train' or 'val'. When set to 'train', data_augmentation will be applied.\n",
    "        letterbox_resize: whether to use the letterbox resize, i.e., keep the original aspect ratio in the resized image.\n",
    "    '''\n",
    "    \n",
    "    img_idx, pic_path, boxes, labels,img_width, img_height = line  # boxes in format xmin, ymin, xmax, ymax\n",
    "    img = cv2.imread(pic_path)\n",
    "    img_resized = cv2.resize(img,(target_size[0], target_size[1]))\n",
    "    \n",
    "    # expand the 2nd dimension, mix up weight default to 1.\n",
    "    boxes = np.concatenate((boxes, np.full(shape=(boxes.shape[0], 1), fill_value=1., dtype=np.float32)), axis=-1)\n",
    "\n",
    "    img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "\n",
    "    # the input of yolo_v3 should be in range 0~1, lets change to -0.5 to +0.5\n",
    "    img_resized = (img_resized - 127.5)/ 255.\n",
    "\n",
    "    y_true_13, anchor_mask = process_box(boxes, img_width, img_height, labels, target_size, class_num, anchors)\n",
    "\n",
    "    return img_idx, img_resized, y_true_13, anchor_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2 0\n",
      "9 0 0 1\n",
      "12 0 0 2\n",
      "9 3 0 3\n",
      "3 6 2 4\n",
      "6 2 2 0\n",
      "2 11 1 1\n",
      "2 12 1 2\n",
      "7 11 1 3\n",
      "10 8 2 4\n",
      "2 2 2 0\n",
      "8 1 0 1\n",
      "12 1 0 2\n",
      "9 6 0 3\n",
      "3 7 2 4\n",
      "2 2 2 0\n",
      "8 1 0 1\n",
      "12 1 0 2\n",
      "8 6 0 3\n",
      "3 7 2 4\n",
      "2 3 1 0\n",
      "9 5 1 1\n",
      "9 7 1 2\n",
      "9 12 1 3\n",
      "3 10 2 4\n",
      "2 2 2 0\n",
      "9 0 0 1\n",
      "12 0 0 2\n",
      "9 4 0 3\n",
      "2 6 2 4\n",
      "2 3 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "10 7 0 3\n",
      "3 11 2 4\n",
      "4 3 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "9 4 0 3\n",
      "3 10 2 4\n",
      "3 2 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "9 5 0 3\n",
      "2 6 2 4\n",
      "11 4 2 0\n",
      "9 9 1 1\n",
      "13 9 1 2\n",
      "9 12 1 3\n",
      "3 12 2 4\n",
      "3 2 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "9 5 0 3\n",
      "3 6 2 4\n",
      "5 2 2 0\n",
      "2 5 1 1\n",
      "2 4 1 2\n",
      "2 7 1 3\n",
      "10 12 2 4\n",
      "2 3 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "10 7 0 3\n",
      "3 9 2 4\n",
      "5 1 2 0\n",
      "2 4 1 1\n",
      "2 4 1 2\n",
      "2 6 1 3\n",
      "10 12 2 4\n",
      "3 6 2 0\n",
      "9 5 0 1\n",
      "12 5 0 2\n",
      "9 8 1 3\n",
      "3 9 2 4\n",
      "4 3 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "9 5 0 3\n",
      "3 10 2 4\n",
      "3 2 2 0\n",
      "9 0 0 1\n",
      "12 0 0 2\n",
      "10 5 0 3\n",
      "3 6 2 4\n",
      "4 2 2 0\n",
      "9 1 0 1\n",
      "12 0 0 2\n",
      "10 4 0 3\n",
      "3 7 2 4\n",
      "2 3 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "10 8 0 3\n",
      "2 9 2 4\n",
      "2 3 2 0\n",
      "9 1 0 1\n",
      "12 0 0 2\n",
      "9 6 0 3\n",
      "3 9 2 4\n",
      "11 3 2 0\n",
      "9 9 0 1\n",
      "13 9 1 2\n",
      "10 12 1 3\n",
      "4 11 2 4\n",
      "2 2 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "9 5 0 3\n",
      "3 7 2 4\n",
      "2 4 2 0\n",
      "9 3 0 1\n",
      "12 2 0 2\n",
      "10 7 0 3\n",
      "2 8 2 4\n",
      "2 3 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "10 7 0 3\n",
      "3 10 2 4\n",
      "7 5 2 0\n",
      "10 10 1 1\n",
      "10 11 1 2\n",
      "11 13 1 3\n",
      "4 12 2 4\n",
      "3 2 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "9 5 1 3\n",
      "3 6 2 4\n",
      "2 2 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "9 6 0 3\n",
      "2 9 2 4\n",
      "2 5 2 0\n",
      "8 3 0 1\n",
      "12 3 0 2\n",
      "9 8 0 3\n",
      "3 11 2 4\n",
      "2 2 2 0\n",
      "9 1 0 1\n",
      "12 0 0 2\n",
      "9 5 0 3\n",
      "3 6 2 4\n",
      "2 3 1 0\n",
      "9 5 1 1\n",
      "9 7 1 2\n",
      "9 12 1 3\n",
      "3 9 2 4\n",
      "2 2 2 0\n",
      "9 2 0 1\n",
      "12 2 0 2\n",
      "9 8 0 3\n",
      "2 9 2 4\n",
      "7 3 2 0\n",
      "10 9 1 1\n",
      "11 11 1 2\n",
      "11 12 1 3\n",
      "4 11 2 4\n",
      "2 3 2 0\n",
      "9 1 0 1\n",
      "12 1 0 2\n",
      "9 8 0 3\n",
      "3 10 2 4\n",
      "2 3 2 0\n",
      "8 1 0 1\n",
      "12 1 0 2\n",
      "9 7 0 3\n",
      "2 10 2 4\n",
      "2 2 2 0\n",
      "9 1 0 1\n",
      "12 2 0 2\n",
      "10 7 0 3\n",
      "2 7 2 4\n",
      "7 2 2 0\n",
      "11 9 1 1\n",
      "11 10 1 2\n",
      "11 12 1 3\n",
      "4 11 2 4\n"
     ]
    }
   ],
   "source": [
    "## Making the data ready for entering into network :\n",
    "\n",
    "anchors = anchors_wrt_target\n",
    "image_index = []\n",
    "image_resized = []\n",
    "image_y_true = []\n",
    "image_anchor_mask = []\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "\n",
    "    line = data_train[i]\n",
    "    \n",
    "    img_idx, img_resized, y_true, anchor_mask = parse_data(line, class_num, target_size, anchors)\n",
    "    \n",
    "    \n",
    "    image_index.append(img_idx)\n",
    "    image_resized.append(img_resized)\n",
    "    image_y_true.append(y_true)\n",
    "    image_anchor_mask.append(anchor_mask)\n",
    "    \n",
    "train_image_index = image_index\n",
    "X_train = np.array(image_resized)\n",
    "Y_train = np.array(image_y_true)\n",
    "train_anchor_mask = np.array(image_anchor_mask)\n",
    "\n",
    "image_index = []\n",
    "image_resized = []\n",
    "image_y_true = []\n",
    "image_anchor_mask = []\n",
    "\n",
    "for i in range(len(data_val)):\n",
    "    line = data_val[i]\n",
    "    \n",
    "    img_idx, img_resized, y_true, anchor_mask = parse_data(line, class_num, target_size, anchors)\n",
    "    image_index.append(img_idx)\n",
    "    image_resized.append(img_resized)\n",
    "    image_y_true.append(y_true)\n",
    "    image_anchor_mask.append(anchor_mask)\n",
    "val_image_index = image_index\n",
    "X_val = np.array(image_resized)\n",
    "Y_val = np.array(image_y_true)\n",
    "val_anchor_mask = np.array(image_anchor_mask)\n",
    "\n",
    "image_index = []\n",
    "image_resized = []\n",
    "image_y_true = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28 28 8 8 8\n",
      "[31, 1, 20, 35, 23, 25, 22, 18, 26, 3, 11, 7, 29, 8, 2, 19, 27, 17, 32, 15, 10, 14, 0, 30, 9, 24, 4, 13]\n",
      "(28, 480, 480, 3) (28, 15, 15, 3, 10)\n",
      "31 (480, 480, 3) (15, 15, 3, 10)\n",
      "(28, 15, 15, 3) and (15, 15, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(Y_train), len(train_image_index), len(X_val), len(Y_val), len(val_image_index))\n",
    "print(train_image_index)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(train_image_index[0], X_train[0].shape, Y_train[0].shape)\n",
    "print(train_anchor_mask.shape, 'and', train_anchor_mask[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31,\n",
       " '/home/scar3crow/Downloads/8-6-new-scan/59a.jpg',\n",
       " array([[  3.,   5., 160.,  56.],\n",
       "        [226.,   2., 291.,  24.],\n",
       "        [332.,   1., 388.,  24.],\n",
       "        [227.,  39., 312.,  65.],\n",
       "        [  3.,  58., 178., 120.]], dtype=float32),\n",
       " array([0, 1, 2, 3, 4]),\n",
       " 416,\n",
       " 200]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_line[7]  ## in x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.19591346, 0.1525    , 0.37740383, 0.255     , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0][2,2]   ## in x_center, y_center, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(logits, labels):\n",
    "    epsilon = 1e-7\n",
    "    logits = tf.clip_by_value(logits, epsilon, 1 - epsilon)\n",
    "    return -(labels * tf.math.log(logits) +\n",
    "             (1 - labels) * tf.math.log(1 - logits))\n",
    "\n",
    "def my_sigmoid(x):\n",
    "    result = 1/(1 + np.exp(-x))\n",
    "    return result\n",
    "\n",
    "def my_softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    result = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ethanyanjiali/deep-vision/blob/master/YOLO/tensorflow/utils.py\n",
    "\n",
    "def xywh_to_x1y1x2y2(box):\n",
    "    xy = box[..., 0:2]\n",
    "    wh = box[..., 2:4]\n",
    "\n",
    "    x1y1 = xy - wh / 2\n",
    "    x2y2 = xy + wh / 2\n",
    "\n",
    "    y_box = K.concatenate([x1y1, x2y2], axis=-1)\n",
    "    return y_box\n",
    "\n",
    "def broadcast_iou(box_a, box_b):\n",
    "    \"\"\"\n",
    "    calculate iou between box_a and multiple box_b in a broadcast way\n",
    "    inputs: box_a: a tensor full of boxes, eg. (B, N, 4), box is in x1y1x2y2\n",
    "            box_b: another tensor full of boxes, eg. (B, M, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    # (B, N, 1, 4)\n",
    "    box_a = tf.expand_dims(box_a, -2)\n",
    "    # (B, 1, M, 4)\n",
    "    box_b = tf.expand_dims(box_b, -3)\n",
    "    # (B, N, M, 4)\n",
    "    new_shape = tf.broadcast_dynamic_shape(tf.shape(box_a), tf.shape(box_b))\n",
    "\n",
    "    # (B, N, M, 4)\n",
    "    # (B, N, M, 4)\n",
    "    box_a = tf.broadcast_to(box_a, new_shape)\n",
    "    box_b = tf.broadcast_to(box_b, new_shape)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    al, at, ar, ab = tf.split(box_a, 4, -1)\n",
    "    bl, bt, br, bb = tf.split(box_b, 4, -1)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    left = tf.math.maximum(al, bl)\n",
    "    right = tf.math.minimum(ar, br)\n",
    "    top = tf.math.maximum(at, bt)\n",
    "    bot = tf.math.minimum(ab, bb)\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    iw = tf.clip_by_value(right - left, 0, 1)\n",
    "    ih = tf.clip_by_value(bot - top, 0, 1)\n",
    "    i = iw * ih\n",
    "\n",
    "    # (B, N, M, 1)\n",
    "    area_a = (ar - al) * (ab - at)\n",
    "    area_b = (br - bl) * (bb - bt)\n",
    "    union = area_a + area_b - i\n",
    "\n",
    "    # (B, N, M)\n",
    "    iou = tf.squeeze(i / (union + 1e-7), axis=-1)\n",
    "\n",
    "    return iou\n",
    "\n",
    "## https://github.com/ethanyanjiali/deep-vision/blob/master/YOLO/tensorflow/yolov3.py#L213\n",
    "\n",
    "def calc_ignore_mask(ignore_thresh, true_box, pred_box):\n",
    "    \n",
    "        # YOLOv3:\n",
    "        # \"If the bounding box prior is not the best but does overlap a ground\n",
    "        # truth object by more than some threshold we ignore the prediction,\n",
    "        # following [17]. We use the threshold of .5.\"\n",
    "        # calculate the iou for each pair of pred bbox and true bbox, then find the best among them\n",
    "\n",
    "        # (None, 13, 13, 3, 4)\n",
    "        \n",
    "        true_box_reorganised = xywh_to_x1y1x2y2(true_box)  # reorganised to x1, y1, x2, y2\n",
    "        pred_box_reorganised = xywh_to_x1y1x2y2(pred_box)\n",
    "        \n",
    "        true_box_shape = tf.shape(true_box_reorganised)  \n",
    "        # (None, 13, 13, 3, 4)\n",
    "        pred_box_shape = tf.shape(pred_box_reorganised)  \n",
    "        # (None, 507, 4)\n",
    "        true_box_reorganised = tf.reshape(true_box_reorganised, [true_box_shape[0], -1, 4])\n",
    "        # sort true_box to have non-zero boxes rank first\n",
    "        true_box_reorganised = tf.sort(true_box_reorganised, axis=1, direction=\"DESCENDING\")\n",
    "        # (None, 100, 4)\n",
    "        # only use maximum 100 boxes per groundtruth to calcualte IOU, otherwise\n",
    "        # GPU emory comsumption would explode for a matrix like (16, 52*52*3, 52*52*3, 4)\n",
    "        true_box_reorganised = true_box_reorganised[:, 0:100, :]\n",
    "        # (None, 507, 4)\n",
    "        pred_box_reorganised = tf.reshape(pred_box_reorganised, [pred_box_shape[0], -1, 4])\n",
    "\n",
    "        # https://github.com/dmlc/gluon-cv/blob/06bb7ec2044cdf3f433721be9362ab84b02c5a90/gluoncv/model_zoo/yolo/yolo_target.py#L198\n",
    "        # (None, 507, 507)\n",
    "        iou = broadcast_iou(pred_box_reorganised, true_box_reorganised)\n",
    "        # (None, 507)\n",
    "        best_iou = tf.reduce_max(iou, axis=-1)\n",
    "        # (None, 13, 13, 3)\n",
    "        best_iou = tf.reshape(best_iou, [pred_box_shape[0], pred_box_shape[1], pred_box_shape[2], pred_box_shape[3]])\n",
    "        # ignore_mask = 1 => don't ignore\n",
    "        # ignore_mask = 0 => should ignore\n",
    "        ignore_mask = tf.cast(best_iou < ignore_thresh, tf.float32)\n",
    "        # (None, 13, 13, 3, 1)\n",
    "        ignore_mask = tf.expand_dims(ignore_mask, axis=-1)\n",
    "        \n",
    "        return ignore_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use functools\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "anchors = anchors_wrt_target\n",
    "\n",
    "def my_yolo_loss(y_true, y_pred):\n",
    "    \n",
    "    def partial(my_yolo_loss, anchors):\n",
    "        \n",
    "# def yolo_my_loss(y_true, y_pred):\n",
    "    \n",
    "        '''Return yolo_loss\n",
    "        Parameters\n",
    "        ----------\n",
    "        yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\n",
    "        y_true: list of array, the output of preprocess_true_boxes\n",
    "        anchors: array, shape=(N, 2), wh ## w.r.t. target size\n",
    "        num_classes: integer\n",
    "        ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n",
    "        Returns\n",
    "        -------\n",
    "        loss: tensor, shape=(1,)\n",
    "        '''\n",
    "#        anchors = anchors_wrt_target\n",
    "        num_anchors = len(anchors)\n",
    "        num_classes = 5\n",
    "        ignore_thresh = 0.5\n",
    "        grid_size = [15., 15.]\n",
    "        grid_stride = 480. / grid_size[0]\n",
    "        batch_shape = y_pred.get_shape()\n",
    "        batch_size = batch_shape[0]\n",
    "    \n",
    "        scaled_anchors = anchors / grid_stride\n",
    "    \n",
    "        Lambda_Coord = 5.0\n",
    "        Lambda_no_obj = 0.5\n",
    "    \n",
    "        grid_x = np.arange(grid_size[1])\n",
    "        grid_y = np.arange(grid_size[0])\n",
    "    \n",
    "        a = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        b = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        c = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        d = np.concatenate((a,b,c), axis = 0)\n",
    "        e = d.transpose(2, 1, 0)\n",
    "        grid_final = np.reshape(e,[1,15,15,3,2])\n",
    "    \n",
    "#    grid_x = np.arange(grid_size[1])\n",
    "#    grid_y = np.arange(grid_size[0])\n",
    "#    grid_x, grid_y = np.meshgrid(grid_x, grid_y)\n",
    "#    grid_1st = np.dstack((grid_x, grid_y))\n",
    "#    grid_2nd = np.dstack((grid_1st, grid_x, grid_y))\n",
    "#    grid_3rd = np.dstack((grid_2nd, grid_x, grid_y)) \n",
    "#    grid_final = np.reshape(grid_3rd, [1,15,15,3,2])\n",
    "    \n",
    "        my_loss = tf.zeros(1, dtype='float32')\n",
    "    \n",
    "    \n",
    "#    m = y_true.shape[0]\n",
    "#    m = np.expand_dims(m, axis=-1)\n",
    "#    mf = K.cast(m, dtype='float32')\n",
    "\n",
    "    \n",
    "        obj_mask = y_true[..., 4:5]\n",
    "#    obj_mask = K.cast(obj_mask_y, dtype='float32')\n",
    "    \n",
    "#    true_box_wh = y_true[..., 2:4]\n",
    "#    weight = 2 - true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "#    weight = np.expand_dims(weight, axis=-1)\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++    \n",
    "        pred_box_xy = K.sigmoid(y_pred[..., :2]) + grid_final  # this gives x & y in no. of cells. x & y w.r.t. target\n",
    "                                                            # image = (x & y in no. of cells) / no. of cells\n",
    "#    pred_box_xy = my_sigmoid(y_pred[..., :2]) + grid_final  # this gives x & y in no. of cells. x & y w.r.t. target\n",
    "                                                            # image = (x & y in no. of cells) / no. of cells\n",
    "        \n",
    "#    pred_box_xy = K.eval(pred_box_xy)  \n",
    "        \n",
    "#    Lambda_Coord = K.cast(Lambda_Coord, dtype = 'float32')\n",
    "        pred_box_xy_wrt_target_image = (pred_box_xy * grid_stride) / 480.\n",
    "        true_box_xy_wrt_target_image = y_true[..., :2]\n",
    "    \n",
    "#    pred_box_xy_wrt_ti = K.cast(pred_box_xy_wrt_target_image, dtype = 'float32')\n",
    "#    true_box_xy_wrt_ti = K.cast(true_box_xy_wrt_target_image, dtype = 'float32')\n",
    "    \n",
    "#    xy_arr = K.cast((true_box_xy_wrt_ti - pred_box_xy_wrt_ti), dtype='float32')\n",
    "\n",
    "        xy_arr = np.power((true_box_xy_wrt_target_image - pred_box_xy_wrt_target_image), 2)\n",
    "    \n",
    "        xy_loss = K.cast((Lambda_Coord * np.sum(xy_arr * obj_mask)), dtype = 'float32')\n",
    "    \n",
    "#    xy_loss = K.cast(Lambda_Coord * tf.reduce_sum(tf.square(xy_arr) * obj_mask), dtype = 'float32')\n",
    "    \n",
    "#    xy_loss = (Lambda_Coord *np.sum(mean_squared_error(true_box_xy_wrt_ti, pred_box_xy_wrt_ti) * obj_mask*weight)) / m\n",
    "    \n",
    "#    xy_loss = (Lambda_Coord *np.sum(mean_squared_error(true_box_xy_wrt_ti, pred_box_xy_wrt_ti) * obj_mask*weight)) / m\n",
    "    \n",
    "#    xy_loss = (Lambda_Coord*tf.reduce_sum(tf.square(true_box_xy_wrt_ti - pred_box_xy_wrt_ti) *obj_mask))/m\n",
    "#    wh_loss = tf.reduce_sum(tf.square(true_tw_th - pred_tw_th) * object_mask * box_loss_scale * mix_w) / N\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++    \n",
    "        pred_box_wdht = K.exp(y_pred[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "    \n",
    "#    pred_box_wdht = np.exp(y_pred[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "    \n",
    "#    pred_box_wh = K.cast(pred_box_wdht, dtype = 'float32')\n",
    "        true_box_wdht = y_true[..., 2:4]\n",
    "#    true_box_wh = K.cast(true_box_wdht, dtype = 'float32')\n",
    "    \n",
    "#    wh_arr = K.cast((K.sqrt(true_box_wh) - K.sqrt(pred_box_wh)), dtype='float32')\n",
    "    \n",
    "        wh_arr = np.power((true_box_wdht - pred_box_wdht), 2)\n",
    "    \n",
    "        wh_loss = K.cast((Lambda_Coord * np.sum(wh_arr * obj_mask)), dtype = 'float32')\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_obj_mask = K.cast(K.sigmoid(y_pred[..., 4:5]), dtype = 'float32')  # shape = 28, 15, 15, 3, 1\n",
    "       \n",
    "        true_box_wrt_ti = K.concatenate([true_box_xy_wrt_target_image, true_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "        pred_box_wrt_ti = K.concatenate([pred_box_xy_wrt_target_image, pred_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#    true_box_wrt_ti = xywh_to_x1y1x2y2(true_box_wrt_ti)  ## converted to x1,y1,x2,y2 format\n",
    "#    pred_box_wrt_ti = xywh_to_x1y1x2y2(pred_box_wrt_ti)  ## converted to x1,y1,x2,y2 format\n",
    "    \n",
    "        ignore_mask = calc_ignore_mask(ignore_thresh, true_box_wrt_ti, pred_box_wrt_ti)\n",
    "    \n",
    "#    ignore_mask = K.cast(ignore_mask, dtype = 'float32') \n",
    "        \n",
    "        bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "\n",
    "###        obj_loss = K.cast((K.sum(binary_cross_entropy(obj_mask, pred_obj_mask) * obj_mask)), dtype = 'float32')\n",
    "        obj_loss = K.sum(bce(obj_mask, pred_obj_mask) * obj_mask)\n",
    "    \n",
    "        no_obj_mask = 1. - obj_mask\n",
    "    \n",
    "#    no_obj_mask = K.cast(no_obj_mask, dtype  = 'float32')\n",
    "    \n",
    "###        noobj_loss = K.cast((Lambda_no_obj * K.sum(binary_cross_entropy(obj_mask, pred_obj_mask) * no_obj_mask * ignore_mask)), dtype='float32')\n",
    "        \n",
    "        noobj_loss = Lambda_no_obj * K.sum(bce(obj_mask, pred_obj_mask) * no_obj_mask * ignore_mask)\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        true_classes = K.cast(y_true[..., 5:10], dtype = 'float32')\n",
    "    \n",
    "        pred_classes = K.cast(K.softmax(y_pred[..., 5:10]), dtype = 'float32')\n",
    "        \n",
    "        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "        class_loss = K.sum(cce(true_classes, pred_classes) * obj_mask)\n",
    "\n",
    "        my_loss = xy_loss + wh_loss + obj_loss + noobj_loss + class_loss\n",
    "    \n",
    "        return my_loss\n",
    "    \n",
    "    loss = partial(my_yolo_loss, anchors)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use functools\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "anchors = anchors_wrt_target\n",
    "\n",
    "def my_custom_loss(y_true, y_pred):\n",
    "    \n",
    "    def abcd(my_custom_loss, anchors):\n",
    "        \n",
    "        num_anchors = len(anchors)\n",
    "        num_classes = 5\n",
    "        ignore_thresh = 0.5\n",
    "        grid_size = [15., 15.]\n",
    "        grid_stride = 480. / grid_size[0]\n",
    "        batch_shape = y_pred.get_shape()\n",
    "        batch_size = batch_shape[0]\n",
    "    \n",
    "        scaled_anchors = anchors / grid_stride\n",
    "    \n",
    "        Lambda_Coord = 5.0\n",
    "        Lambda_no_obj = 0.5\n",
    "    \n",
    "        grid_x = np.arange(grid_size[1])\n",
    "        grid_y = np.arange(grid_size[0])\n",
    "    \n",
    "        a = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        b = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        c = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        d = np.concatenate((a,b,c), axis = 0)\n",
    "        e = d.transpose(2, 1, 0)\n",
    "        grid_final = np.reshape(e,[1,15,15,3,2])\n",
    "    \n",
    "        tot_loss = tf.zeros(1, dtype='float32')\n",
    "\n",
    "        obj_mask = y_true[..., 4:5]\n",
    "\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_box_xy = K.sigmoid(y_pred[..., :2]) + grid_final  # this gives x & y in no. of cells. x & y w.r.t. target\n",
    "                                                            # image = (x & y in no. of cells) / no. of cells\n",
    "        pred_box_xy_wrt_target_image = (pred_box_xy * grid_stride) / 480.\n",
    "        true_box_xy_wrt_target_image = y_true[..., :2]\n",
    "\n",
    "        xy_arr = np.power((true_box_xy_wrt_target_image - pred_box_xy_wrt_target_image), 2)\n",
    "    \n",
    "        xy_loss = K.cast((Lambda_Coord * np.sum(xy_arr * obj_mask)), dtype = 'float32')\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_box_wdht = K.exp(y_pred[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "    \n",
    "        true_box_wdht = y_true[..., 2:4]\n",
    "    \n",
    "        wh_arr = np.power((true_box_wdht - pred_box_wdht), 2)\n",
    "    \n",
    "        wh_loss = K.cast((Lambda_Coord * np.sum(wh_arr * obj_mask)), dtype = 'float32')\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_obj_mask = K.cast(K.sigmoid(y_pred[..., 4:5]), dtype = 'float32')  # shape = 28, 15, 15, 3, 1\n",
    "       \n",
    "        true_box_wrt_ti = K.concatenate([true_box_xy_wrt_target_image, true_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "        pred_box_wrt_ti = K.concatenate([pred_box_xy_wrt_target_image, pred_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "    \n",
    "        ignore_mask = calc_ignore_mask(ignore_thresh, true_box_wrt_ti, pred_box_wrt_ti)\n",
    "        \n",
    "        bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "        obj_loss = K.sum(bce(obj_mask, pred_obj_mask) * obj_mask)\n",
    "        obj_loss = K.cast(obj_loss, dtype = 'float32')\n",
    "    \n",
    "        no_obj_mask = 1. - obj_mask\n",
    "    \n",
    "        noobj_loss = Lambda_no_obj * K.sum(bce(obj_mask, pred_obj_mask) * no_obj_mask * ignore_mask)\n",
    "        noobj_loss = K.cast(noobj_loss, dtype = 'float32')\n",
    "        \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        true_classes = K.cast(y_true[..., 5:10], dtype = 'float32')\n",
    "    \n",
    "        pred_classes = K.cast(K.softmax(y_pred[..., 5:10]), dtype = 'float32')\n",
    "        \n",
    "        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "        class_loss = K.sum(cce(true_classes, pred_classes) * obj_mask)\n",
    "        class_loss = K.cast(class_loss, dtype = 'float32')\n",
    "\n",
    "        tot_loss = xy_loss + wh_loss + obj_loss + noobj_loss + class_loss\n",
    "        \n",
    "        return tot_loss\n",
    "    \n",
    "    loss = K.cast(abcd(my_custom_loss, anchors), dtype = 'float32')\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block(inp, convs, skip=True):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    \n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        \n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
    "        x = Conv2D(conv['filter'], \n",
    "                   conv['kernel'], \n",
    "                   strides=conv['stride'], \n",
    "                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
    "                   name='conv_' + str(conv['layer_idx']), \n",
    "                   use_bias=False if conv['bnorm'] else True)(x)\n",
    "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
    "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "\n",
    "    return add([skip_connection, x]) if skip else x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yolov3_model():\n",
    "    input_image = Input(shape=(480, 480, 3))\n",
    "\n",
    "    # Layer  0 => 4\n",
    "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
    "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
    "\n",
    "    # Layer  5 => 8\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
    "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
    "\n",
    "    # Layer  9 => 11\n",
    "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
    "\n",
    "    # Layer 12 => 15\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
    "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
    "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
    "\n",
    "    # Layer 16 => 36\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
    "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
    "        \n",
    "    skip_36 = x\n",
    "        \n",
    "    # Layer 37 => 40\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
    "\n",
    "    # Layer 41 => 61\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
    "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
    "        \n",
    "    skip_61 = x\n",
    "        \n",
    "#    # Layer 62 => 65\n",
    "#    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "#                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "#                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "    \n",
    "    # Layer 62 => 65\n",
    "    x = _conv_block(x, [{'filter':  256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "                        {'filter':  256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "\n",
    "\n",
    "#    # Layer 66 => 74\n",
    "#    for i in range(3):\n",
    "#        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "#                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "    \n",
    "     # Layer 66 => 74\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x, [{'filter':  128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "                            {'filter':  256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "\n",
    "#    # Layer 75 => 79\n",
    "#    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "#                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "#                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "#                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "#                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "    \n",
    "    # Layer 75 => 79\n",
    "    x = _conv_block(x, [{'filter':  128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "                        {'filter':  256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "                        {'filter':  128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "                        {'filter':  256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "                        {'filter':  128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "    \n",
    "    # Layer 80 => 82\n",
    "    yolo_82 = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
    "                              {'filter':  30, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
    "\n",
    "#    # Layer 83 => 86\n",
    "#    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
    "#    x = UpSampling2D(2)(x)\n",
    "#    x = concatenate([x, skip_61])\n",
    "\n",
    "#    # Layer 87 => 91\n",
    "#    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
    "#                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
    "#                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
    "#                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
    "#                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
    "\n",
    "#    # Layer 92 => 94\n",
    "#    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
    "#                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
    "\n",
    "#    # Layer 95 => 98\n",
    "#    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
    "#    x = UpSampling2D(2)(x)\n",
    "#    x = concatenate([x, skip_36])\n",
    "\n",
    "#    # Layer 99 => 106\n",
    "#    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
    "#                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
    "#                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
    "#                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
    "#                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
    "#                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
    "#                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
    "\n",
    "#    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n",
    "\n",
    "    final = Reshape((grid_y_axis,grid_x_axis,num_anchors,info))(yolo_82)\n",
    "    model = Model(input_image, final)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 480, 480, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv2D)                 (None, 480, 480, 32) 864         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_0 (BatchNormalization)    (None, 480, 480, 32) 128         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_0 (LeakyReLU)             (None, 480, 480, 32) 0           bnorm_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_86 (ZeroPadding2 (None, 481, 481, 32) 0           leaky_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 240, 240, 64) 18432       zero_padding2d_86[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_1 (BatchNormalization)    (None, 240, 240, 64) 256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_1 (LeakyReLU)             (None, 240, 240, 64) 0           bnorm_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 240, 240, 32) 2048        leaky_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_2 (BatchNormalization)    (None, 240, 240, 32) 128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_2 (LeakyReLU)             (None, 240, 240, 32) 0           bnorm_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 240, 240, 64) 18432       leaky_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_3 (BatchNormalization)    (None, 240, 240, 64) 256         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_3 (LeakyReLU)             (None, 240, 240, 64) 0           bnorm_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_392 (Add)                   (None, 240, 240, 64) 0           leaky_1[0][0]                    \n",
      "                                                                 leaky_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_87 (ZeroPadding2 (None, 241, 241, 64) 0           add_392[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 120, 120, 128 73728       zero_padding2d_87[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_5 (BatchNormalization)    (None, 120, 120, 128 512         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_5 (LeakyReLU)             (None, 120, 120, 128 0           bnorm_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 120, 120, 64) 8192        leaky_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_6 (BatchNormalization)    (None, 120, 120, 64) 256         conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_6 (LeakyReLU)             (None, 120, 120, 64) 0           bnorm_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, 120, 120, 128 73728       leaky_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_7 (BatchNormalization)    (None, 120, 120, 128 512         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_7 (LeakyReLU)             (None, 120, 120, 128 0           bnorm_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_393 (Add)                   (None, 120, 120, 128 0           leaky_5[0][0]                    \n",
      "                                                                 leaky_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, 120, 120, 64) 8192        add_393[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_9 (BatchNormalization)    (None, 120, 120, 64) 256         conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_9 (LeakyReLU)             (None, 120, 120, 64) 0           bnorm_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, 120, 120, 128 73728       leaky_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_10 (BatchNormalization)   (None, 120, 120, 128 512         conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_10 (LeakyReLU)            (None, 120, 120, 128 0           bnorm_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_394 (Add)                   (None, 120, 120, 128 0           add_393[0][0]                    \n",
      "                                                                 leaky_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_88 (ZeroPadding2 (None, 121, 121, 128 0           add_394[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_12 (Conv2D)                (None, 60, 60, 256)  294912      zero_padding2d_88[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_12 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_12 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_13 (Conv2D)                (None, 60, 60, 128)  32768       leaky_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_13 (BatchNormalization)   (None, 60, 60, 128)  512         conv_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_13 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_14 (Conv2D)                (None, 60, 60, 256)  294912      leaky_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_14 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_14 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_395 (Add)                   (None, 60, 60, 256)  0           leaky_12[0][0]                   \n",
      "                                                                 leaky_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 60, 60, 128)  32768       add_395[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_16 (BatchNormalization)   (None, 60, 60, 128)  512         conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_16 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 60, 60, 256)  294912      leaky_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_17 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_17 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_396 (Add)                   (None, 60, 60, 256)  0           add_395[0][0]                    \n",
      "                                                                 leaky_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 60, 60, 128)  32768       add_396[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_19 (BatchNormalization)   (None, 60, 60, 128)  512         conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_19 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 60, 60, 256)  294912      leaky_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_20 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_20 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_397 (Add)                   (None, 60, 60, 256)  0           add_396[0][0]                    \n",
      "                                                                 leaky_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 60, 60, 128)  32768       add_397[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_22 (BatchNormalization)   (None, 60, 60, 128)  512         conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_22 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 60, 60, 256)  294912      leaky_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_23 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_23 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_398 (Add)                   (None, 60, 60, 256)  0           add_397[0][0]                    \n",
      "                                                                 leaky_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_25 (Conv2D)                (None, 60, 60, 128)  32768       add_398[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_25 (BatchNormalization)   (None, 60, 60, 128)  512         conv_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_25 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_26 (Conv2D)                (None, 60, 60, 256)  294912      leaky_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_26 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_26 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_399 (Add)                   (None, 60, 60, 256)  0           add_398[0][0]                    \n",
      "                                                                 leaky_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_28 (Conv2D)                (None, 60, 60, 128)  32768       add_399[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_28 (BatchNormalization)   (None, 60, 60, 128)  512         conv_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_28 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_29 (Conv2D)                (None, 60, 60, 256)  294912      leaky_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_29 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_29 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_400 (Add)                   (None, 60, 60, 256)  0           add_399[0][0]                    \n",
      "                                                                 leaky_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_31 (Conv2D)                (None, 60, 60, 128)  32768       add_400[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_31 (BatchNormalization)   (None, 60, 60, 128)  512         conv_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_31 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_32 (Conv2D)                (None, 60, 60, 256)  294912      leaky_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_32 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_32 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_401 (Add)                   (None, 60, 60, 256)  0           add_400[0][0]                    \n",
      "                                                                 leaky_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_34 (Conv2D)                (None, 60, 60, 128)  32768       add_401[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_34 (BatchNormalization)   (None, 60, 60, 128)  512         conv_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_34 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_35 (Conv2D)                (None, 60, 60, 256)  294912      leaky_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_35 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_35 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_402 (Add)                   (None, 60, 60, 256)  0           add_401[0][0]                    \n",
      "                                                                 leaky_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_89 (ZeroPadding2 (None, 61, 61, 256)  0           add_402[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_37 (Conv2D)                (None, 30, 30, 512)  1179648     zero_padding2d_89[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_37 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_37 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_38 (Conv2D)                (None, 30, 30, 256)  131072      leaky_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_38 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_38 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_39 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_39 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_39 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_403 (Add)                   (None, 30, 30, 512)  0           leaky_37[0][0]                   \n",
      "                                                                 leaky_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_41 (Conv2D)                (None, 30, 30, 256)  131072      add_403[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_41 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_41 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_42 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_42 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_42 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_404 (Add)                   (None, 30, 30, 512)  0           add_403[0][0]                    \n",
      "                                                                 leaky_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_44 (Conv2D)                (None, 30, 30, 256)  131072      add_404[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_44 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_44 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_45 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_45 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_45 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_405 (Add)                   (None, 30, 30, 512)  0           add_404[0][0]                    \n",
      "                                                                 leaky_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_47 (Conv2D)                (None, 30, 30, 256)  131072      add_405[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_47 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_47 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_48 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_48 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_48 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_406 (Add)                   (None, 30, 30, 512)  0           add_405[0][0]                    \n",
      "                                                                 leaky_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_50 (Conv2D)                (None, 30, 30, 256)  131072      add_406[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_50 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_50 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_51 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_51 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_51 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_407 (Add)                   (None, 30, 30, 512)  0           add_406[0][0]                    \n",
      "                                                                 leaky_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_53 (Conv2D)                (None, 30, 30, 256)  131072      add_407[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_53 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_53 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_54 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_54 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_54 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_408 (Add)                   (None, 30, 30, 512)  0           add_407[0][0]                    \n",
      "                                                                 leaky_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_56 (Conv2D)                (None, 30, 30, 256)  131072      add_408[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_56 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_56 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_57 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_57 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_57[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_57 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_409 (Add)                   (None, 30, 30, 512)  0           add_408[0][0]                    \n",
      "                                                                 leaky_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_59 (Conv2D)                (None, 30, 30, 256)  131072      add_409[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_59 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_59 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_60 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_60 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_60 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_410 (Add)                   (None, 30, 30, 512)  0           add_409[0][0]                    \n",
      "                                                                 leaky_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_90 (ZeroPadding2 (None, 31, 31, 512)  0           add_410[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_62 (Conv2D)                (None, 15, 15, 256)  1179648     zero_padding2d_90[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_62 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_62 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_63 (Conv2D)                (None, 15, 15, 512)  131072      leaky_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_63 (BatchNormalization)   (None, 15, 15, 512)  2048        conv_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_63 (LeakyReLU)            (None, 15, 15, 512)  0           bnorm_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_64 (Conv2D)                (None, 15, 15, 256)  1179648     leaky_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_64 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_64 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_411 (Add)                   (None, 15, 15, 256)  0           leaky_62[0][0]                   \n",
      "                                                                 leaky_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_66 (Conv2D)                (None, 15, 15, 128)  32768       add_411[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_66 (BatchNormalization)   (None, 15, 15, 128)  512         conv_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_66 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_67 (Conv2D)                (None, 15, 15, 256)  294912      leaky_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_67 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_67 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_412 (Add)                   (None, 15, 15, 256)  0           add_411[0][0]                    \n",
      "                                                                 leaky_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_69 (Conv2D)                (None, 15, 15, 128)  32768       add_412[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_69 (BatchNormalization)   (None, 15, 15, 128)  512         conv_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_69 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_70 (Conv2D)                (None, 15, 15, 256)  294912      leaky_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_70 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_70[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_70 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_413 (Add)                   (None, 15, 15, 256)  0           add_412[0][0]                    \n",
      "                                                                 leaky_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_72 (Conv2D)                (None, 15, 15, 128)  32768       add_413[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_72 (BatchNormalization)   (None, 15, 15, 128)  512         conv_72[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_72 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_73 (Conv2D)                (None, 15, 15, 256)  294912      leaky_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_73 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_73[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_73 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_414 (Add)                   (None, 15, 15, 256)  0           add_413[0][0]                    \n",
      "                                                                 leaky_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_75 (Conv2D)                (None, 15, 15, 128)  32768       add_414[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_75 (BatchNormalization)   (None, 15, 15, 128)  512         conv_75[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_75 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_76 (Conv2D)                (None, 15, 15, 256)  294912      leaky_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_76 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_76[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_76 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_77 (Conv2D)                (None, 15, 15, 128)  32768       leaky_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_77 (BatchNormalization)   (None, 15, 15, 128)  512         conv_77[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_77 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_78 (Conv2D)                (None, 15, 15, 256)  294912      leaky_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_78 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_78[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_78 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_79 (Conv2D)                (None, 15, 15, 128)  32768       leaky_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_79 (BatchNormalization)   (None, 15, 15, 128)  512         conv_79[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_79 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_80 (Conv2D)                (None, 15, 15, 256)  294912      leaky_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_80 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_80[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_80 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_81 (Conv2D)                (None, 15, 15, 30)   7710        leaky_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 15, 15, 3, 10 0           conv_81[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,379,326\n",
      "Trainable params: 19,351,294\n",
      "Non-trainable params: 28,032\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_size = (target_w, target_h, 3)\n",
    "my_yolo_invoice_model = make_yolov3_model()\n",
    "\n",
    "\n",
    "my_model_1 = make_yolov3_model()\n",
    "my_model_2 = make_yolov3_model()\n",
    "\n",
    "print(my_model_1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "\n",
    "my_model_2.compile(optimizer= opt, loss = my_yolo_loss, metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/scar3crow/Dropbox/WorkStation-Subrata/python/venv1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 399s 14s/step - loss: 1068.3380 - accuracy: 0.1017 - val_loss: 936.7689 - val_accuracy: 0.3061\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 110s 4s/step - loss: 858.6869 - accuracy: 0.1208 - val_loss: 916.9739 - val_accuracy: 0.2322\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 94s 3s/step - loss: 649.1304 - accuracy: 0.1187 - val_loss: 878.8799 - val_accuracy: 0.3083\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 96s 3s/step - loss: 522.2300 - accuracy: 0.1347 - val_loss: 845.6731 - val_accuracy: 0.3109\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 105s 4s/step - loss: 416.4504 - accuracy: 0.1632 - val_loss: 802.2057 - val_accuracy: 0.0993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa53fe3f630>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 5, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/20\n",
      "28/28 [==============================] - 89s 3s/step - loss: 366.4440 - accuracy: 0.1521 - val_loss: 777.2081 - val_accuracy: 0.0622\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 93s 3s/step - loss: 309.6079 - accuracy: 0.1565 - val_loss: 756.9229 - val_accuracy: 0.0622\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 94s 3s/step - loss: 281.0435 - accuracy: 0.1567 - val_loss: 740.0048 - val_accuracy: 0.0637\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 96s 3s/step - loss: 256.0859 - accuracy: 0.1586 - val_loss: 688.2087 - val_accuracy: 0.0631\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 95s 3s/step - loss: 230.7691 - accuracy: 0.1599 - val_loss: 670.1775 - val_accuracy: 0.0622\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 97s 3s/step - loss: 213.6015 - accuracy: 0.1672 - val_loss: 636.2218 - val_accuracy: 0.0637\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 98s 3s/step - loss: 193.6117 - accuracy: 0.1703 - val_loss: 617.5971 - val_accuracy: 0.0622\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 98s 3s/step - loss: 184.6550 - accuracy: 0.1691 - val_loss: 553.1569 - val_accuracy: 0.0652\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 100s 4s/step - loss: 189.1522 - accuracy: 0.1591 - val_loss: 537.5282 - val_accuracy: 0.0654\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 97s 3s/step - loss: 178.7268 - accuracy: 0.1637 - val_loss: 497.7867 - val_accuracy: 0.0761\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 101s 4s/step - loss: 169.1699 - accuracy: 0.1645 - val_loss: 499.9265 - val_accuracy: 0.0622\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 100s 4s/step - loss: 164.2877 - accuracy: 0.1636 - val_loss: 451.1620 - val_accuracy: 0.0739\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 98s 4s/step - loss: 155.8418 - accuracy: 0.1642 - val_loss: 404.1055 - val_accuracy: 0.0606\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 98s 3s/step - loss: 151.9619 - accuracy: 0.1663 - val_loss: 385.8964 - val_accuracy: 0.0450\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 96s 3s/step - loss: 146.3220 - accuracy: 0.1595 - val_loss: 369.2182 - val_accuracy: 0.0500\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 99s 4s/step - loss: 139.3972 - accuracy: 0.1752 - val_loss: 329.3561 - val_accuracy: 0.0439\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 98s 4s/step - loss: 139.3254 - accuracy: 0.1848 - val_loss: 305.4432 - val_accuracy: 0.0591\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 98s 4s/step - loss: 131.9685 - accuracy: 0.2024 - val_loss: 296.1589 - val_accuracy: 0.0970\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 100s 4s/step - loss: 128.1917 - accuracy: 0.2343 - val_loss: 278.8938 - val_accuracy: 0.2481\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 100s 4s/step - loss: 129.1027 - accuracy: 0.2619 - val_loss: 258.3963 - val_accuracy: 0.2537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa54032b2b0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 20, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 89s 3s/step - loss: 121.2051 - accuracy: 0.2815 - val_loss: 243.7248 - val_accuracy: 0.2687\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 119.8802 - accuracy: 0.2922 - val_loss: 223.4276 - val_accuracy: 0.2870\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 125.4161 - accuracy: 0.2973 - val_loss: 220.6673 - val_accuracy: 0.2887\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 118.8083 - accuracy: 0.3033 - val_loss: 208.2060 - val_accuracy: 0.2891\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 117.0325 - accuracy: 0.3032 - val_loss: 176.9318 - val_accuracy: 0.2891\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 125.5975 - accuracy: 0.3066 - val_loss: 181.3711 - val_accuracy: 0.3069\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 116.3712 - accuracy: 0.3064 - val_loss: 174.5310 - val_accuracy: 0.3141\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 110.0129 - accuracy: 0.3100 - val_loss: 160.0054 - val_accuracy: 0.3276\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 110.0363 - accuracy: 0.3141 - val_loss: 158.6641 - val_accuracy: 0.3276\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 102.6856 - accuracy: 0.3177 - val_loss: 147.9160 - val_accuracy: 0.3276\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 100.1192 - accuracy: 0.3243 - val_loss: 140.7647 - val_accuracy: 0.3276\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 101.4781 - accuracy: 0.3258 - val_loss: 142.5407 - val_accuracy: 0.3291\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 98.0529 - accuracy: 0.3265 - val_loss: 133.5722 - val_accuracy: 0.3291\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 96.8801 - accuracy: 0.3276 - val_loss: 123.5556 - val_accuracy: 0.3291\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 94.7125 - accuracy: 0.3275 - val_loss: 120.0409 - val_accuracy: 0.3306\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 92.2155 - accuracy: 0.3283 - val_loss: 120.1580 - val_accuracy: 0.3306\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 89.6722 - accuracy: 0.3285 - val_loss: 116.7991 - val_accuracy: 0.3306\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 87.5066 - accuracy: 0.3284 - val_loss: 113.6084 - val_accuracy: 0.3306\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 86.8177 - accuracy: 0.3282 - val_loss: 111.6979 - val_accuracy: 0.3306\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 88.1230 - accuracy: 0.3283 - val_loss: 111.8061 - val_accuracy: 0.3306\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 85.1205 - accuracy: 0.3282 - val_loss: 106.2193 - val_accuracy: 0.3306\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 85.7799 - accuracy: 0.3286 - val_loss: 96.3149 - val_accuracy: 0.3306\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 84.8431 - accuracy: 0.3281 - val_loss: 98.6260 - val_accuracy: 0.3306\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 83.5724 - accuracy: 0.3284 - val_loss: 95.8002 - val_accuracy: 0.3306\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 80.5897 - accuracy: 0.3285 - val_loss: 89.6982 - val_accuracy: 0.3306\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 77.5899 - accuracy: 0.3288 - val_loss: 97.8653 - val_accuracy: 0.3306\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 78.7619 - accuracy: 0.3288 - val_loss: 90.5668 - val_accuracy: 0.3306\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 76.2089 - accuracy: 0.3285 - val_loss: 85.8475 - val_accuracy: 0.3306\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 74.7764 - accuracy: 0.3286 - val_loss: 83.7134 - val_accuracy: 0.3306\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 75.6328 - accuracy: 0.3287 - val_loss: 89.3610 - val_accuracy: 0.3306\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 72.6873 - accuracy: 0.3286 - val_loss: 89.6473 - val_accuracy: 0.3306\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 71.8783 - accuracy: 0.3289 - val_loss: 80.5927 - val_accuracy: 0.3306\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 69.5373 - accuracy: 0.3290 - val_loss: 83.6621 - val_accuracy: 0.3306\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 68.5942 - accuracy: 0.3290 - val_loss: 77.0604 - val_accuracy: 0.3306\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 67.4118 - accuracy: 0.3290 - val_loss: 78.0195 - val_accuracy: 0.3306\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 67.1365 - accuracy: 0.3289 - val_loss: 79.3730 - val_accuracy: 0.3306\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 65.4543 - accuracy: 0.3289 - val_loss: 79.4565 - val_accuracy: 0.3306\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 64.7265 - accuracy: 0.3291 - val_loss: 75.4297 - val_accuracy: 0.3306\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 63.2316 - accuracy: 0.3293 - val_loss: 76.6718 - val_accuracy: 0.3306\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 62.0954 - accuracy: 0.3293 - val_loss: 74.1816 - val_accuracy: 0.3306\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 60.4204 - accuracy: 0.3292 - val_loss: 74.2370 - val_accuracy: 0.3306\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 60.9842 - accuracy: 0.3293 - val_loss: 76.9900 - val_accuracy: 0.3306\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 57.6348 - accuracy: 0.3295 - val_loss: 76.3524 - val_accuracy: 0.3306\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 57.9956 - accuracy: 0.3294 - val_loss: 76.5699 - val_accuracy: 0.3306\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 55.6283 - accuracy: 0.3295 - val_loss: 74.5772 - val_accuracy: 0.3306\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 54.7658 - accuracy: 0.3293 - val_loss: 71.0756 - val_accuracy: 0.3306\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 51.5337 - accuracy: 0.3293 - val_loss: 74.2878 - val_accuracy: 0.3306\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 50.8951 - accuracy: 0.3295 - val_loss: 69.7309 - val_accuracy: 0.3306\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 52.3414 - accuracy: 0.3294 - val_loss: 68.4230 - val_accuracy: 0.3306\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 50.2485 - accuracy: 0.3294 - val_loss: 70.0772 - val_accuracy: 0.3306\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 49.3636 - accuracy: 0.3295 - val_loss: 65.2812 - val_accuracy: 0.3306\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 45.6077 - accuracy: 0.3295 - val_loss: 65.3552 - val_accuracy: 0.3306\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 46.4788 - accuracy: 0.3294 - val_loss: 67.2209 - val_accuracy: 0.3306\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 44.0986 - accuracy: 0.3295 - val_loss: 63.2169 - val_accuracy: 0.3306\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 44.2724 - accuracy: 0.3296 - val_loss: 63.5828 - val_accuracy: 0.3306\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 42.9496 - accuracy: 0.3296 - val_loss: 61.2769 - val_accuracy: 0.3306\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 109s 4s/step - loss: 41.2970 - accuracy: 0.3299 - val_loss: 61.3446 - val_accuracy: 0.3306\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 40.7304 - accuracy: 0.3299 - val_loss: 61.4812 - val_accuracy: 0.3306\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 38.4169 - accuracy: 0.3301 - val_loss: 60.6547 - val_accuracy: 0.3304\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 36.9804 - accuracy: 0.3303 - val_loss: 60.5418 - val_accuracy: 0.3302\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 37.9129 - accuracy: 0.3303 - val_loss: 59.9848 - val_accuracy: 0.3300\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 35.3771 - accuracy: 0.3305 - val_loss: 58.3519 - val_accuracy: 0.3300\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 34.7341 - accuracy: 0.3304 - val_loss: 57.2730 - val_accuracy: 0.3300\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 33.0508 - accuracy: 0.3305 - val_loss: 57.2025 - val_accuracy: 0.3300\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 35.5360 - accuracy: 0.3300 - val_loss: 57.9060 - val_accuracy: 0.3300\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 32.9449 - accuracy: 0.3307 - val_loss: 57.5316 - val_accuracy: 0.3298\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 32.0493 - accuracy: 0.3307 - val_loss: 57.5629 - val_accuracy: 0.3300\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 33.5118 - accuracy: 0.3303 - val_loss: 58.8515 - val_accuracy: 0.3298\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 30.6273 - accuracy: 0.3303 - val_loss: 57.0656 - val_accuracy: 0.3298\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 30.8826 - accuracy: 0.3311 - val_loss: 57.0349 - val_accuracy: 0.3298\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 32.6533 - accuracy: 0.3305 - val_loss: 58.0857 - val_accuracy: 0.3296\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 29.6956 - accuracy: 0.3307 - val_loss: 58.1686 - val_accuracy: 0.3298\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 30.4758 - accuracy: 0.3305 - val_loss: 57.9742 - val_accuracy: 0.3296\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 30.3547 - accuracy: 0.3302 - val_loss: 52.9026 - val_accuracy: 0.3298\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 29.5972 - accuracy: 0.3308 - val_loss: 53.2389 - val_accuracy: 0.3302\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 30.1387 - accuracy: 0.3312 - val_loss: 55.4589 - val_accuracy: 0.3300\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 27.8249 - accuracy: 0.3306 - val_loss: 54.8846 - val_accuracy: 0.3298\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 25.9898 - accuracy: 0.3310 - val_loss: 54.3725 - val_accuracy: 0.3300\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 26.5490 - accuracy: 0.3312 - val_loss: 55.7666 - val_accuracy: 0.3300\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 26.5840 - accuracy: 0.3310 - val_loss: 53.7465 - val_accuracy: 0.3302\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 25.4833 - accuracy: 0.3312 - val_loss: 56.4710 - val_accuracy: 0.3300\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 24.8833 - accuracy: 0.3311 - val_loss: 58.1906 - val_accuracy: 0.3300\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 23.8247 - accuracy: 0.3312 - val_loss: 56.0869 - val_accuracy: 0.3300\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 23.1678 - accuracy: 0.3314 - val_loss: nan - val_accuracy: 0.3300\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 23.3531 - accuracy: 0.3314 - val_loss: nan - val_accuracy: 0.3300\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 23.0978 - accuracy: 0.3312 - val_loss: nan - val_accuracy: 0.3298\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 22.1185 - accuracy: 0.3316 - val_loss: nan - val_accuracy: 0.3298\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 24.4257 - accuracy: 0.3315 - val_loss: nan - val_accuracy: 0.3300\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 22.2316 - accuracy: 0.3313 - val_loss: nan - val_accuracy: 0.3298\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 22.9600 - accuracy: 0.3312 - val_loss: nan - val_accuracy: 0.3298\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 21.1603 - accuracy: 0.3315 - val_loss: nan - val_accuracy: 0.3298\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 21.7752 - accuracy: 0.3315 - val_loss: nan - val_accuracy: 0.3298\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 21.1107 - accuracy: 0.3322 - val_loss: nan - val_accuracy: 0.3296\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 21.2374 - accuracy: 0.3317 - val_loss: nan - val_accuracy: 0.3309\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 20.3034 - accuracy: 0.3317 - val_loss: nan - val_accuracy: 0.3311\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 22.1796 - accuracy: 0.3320 - val_loss: nan - val_accuracy: 0.3309\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 20.8039 - accuracy: 0.3324 - val_loss: nan - val_accuracy: 0.3306\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 21.0913 - accuracy: 0.3317 - val_loss: nan - val_accuracy: 0.3307\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 19.4796 - accuracy: 0.3319 - val_loss: nan - val_accuracy: 0.3309\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 20.4219 - accuracy: 0.3326 - val_loss: 54.4556 - val_accuracy: 0.3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa53aa98400>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 100, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 89s 3s/step - loss: 21.8709 - accuracy: 0.3319 - val_loss: nan - val_accuracy: 0.3307\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 20.2223 - accuracy: 0.3328 - val_loss: nan - val_accuracy: 0.3306\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 18.9929 - accuracy: 0.3324 - val_loss: nan - val_accuracy: 0.3306\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 18.4190 - accuracy: 0.3323 - val_loss: nan - val_accuracy: 0.3307\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 19.7313 - accuracy: 0.3322 - val_loss: nan - val_accuracy: 0.3306\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 19.4605 - accuracy: 0.3323 - val_loss: 770.5505 - val_accuracy: 0.3306\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 18.4823 - accuracy: 0.3328 - val_loss: 62.2431 - val_accuracy: 0.3306\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 18.2107 - accuracy: 0.3323 - val_loss: 67.5464 - val_accuracy: 0.3306\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 17.8861 - accuracy: 0.3318 - val_loss: 65.2569 - val_accuracy: 0.3306\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 18.1924 - accuracy: 0.3322 - val_loss: 114.8766 - val_accuracy: 0.3307\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 17.8556 - accuracy: 0.3328 - val_loss: 401.4634 - val_accuracy: 0.3306\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 18.1409 - accuracy: 0.3322 - val_loss: 9916.6873 - val_accuracy: 0.3306\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 17.9018 - accuracy: 0.3325 - val_loss: 2299529.3756 - val_accuracy: 0.3306\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 18.1361 - accuracy: 0.3318 - val_loss: 585319.0019 - val_accuracy: 0.3306\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 18.5288 - accuracy: 0.3330 - val_loss: 1005.5031 - val_accuracy: 0.3306\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 18.0476 - accuracy: 0.3324 - val_loss: 76.0045 - val_accuracy: 0.3306\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 17.9120 - accuracy: 0.3325 - val_loss: 63.2440 - val_accuracy: 0.3306\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 17.0918 - accuracy: 0.3322 - val_loss: 50365.4477 - val_accuracy: 0.3306\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 17.4729 - accuracy: 0.3324 - val_loss: 10355.5661 - val_accuracy: 0.3306\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 17.5990 - accuracy: 0.3329 - val_loss: 18367609.9002 - val_accuracy: 0.3306\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 16.6045 - accuracy: 0.3321 - val_loss: nan - val_accuracy: 0.3306\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 16.3389 - accuracy: 0.3328 - val_loss: 5139972121.3345 - val_accuracy: 0.3306\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 16.3598 - accuracy: 0.3325 - val_loss: 15306766361.6228 - val_accuracy: 0.3306\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 16.7504 - accuracy: 0.3323 - val_loss: 9241098.7065 - val_accuracy: 0.3306\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 17.1027 - accuracy: 0.3332 - val_loss: 259.9985 - val_accuracy: 0.3306\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 17.3847 - accuracy: 0.3321 - val_loss: 219.1029 - val_accuracy: 0.3306\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 16.8440 - accuracy: 0.3331 - val_loss: 201.3019 - val_accuracy: 0.3306\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 16.1387 - accuracy: 0.3323 - val_loss: 6881532.9654 - val_accuracy: 0.3306\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 16.6145 - accuracy: 0.3319 - val_loss: 12031059.8754 - val_accuracy: 0.3306\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 16.2152 - accuracy: 0.3331 - val_loss: 2581779.0060 - val_accuracy: 0.3306\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 15.8062 - accuracy: 0.3321 - val_loss: 745729882.5034 - val_accuracy: 0.3306\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 16.8428 - accuracy: 0.3319 - val_loss: 72591506.3832 - val_accuracy: 0.3306\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 15.7387 - accuracy: 0.3334 - val_loss: 2309689882.3794 - val_accuracy: 0.3307\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 15.5676 - accuracy: 0.3328 - val_loss: 437128757274.5688 - val_accuracy: 0.3307\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 15.5460 - accuracy: 0.3326 - val_loss: 27322890266.9933 - val_accuracy: 0.3306\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 15.5622 - accuracy: 0.3326 - val_loss: 10339471386.9030 - val_accuracy: 0.3306\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 16.3564 - accuracy: 0.3331 - val_loss: 51440730.6888 - val_accuracy: 0.3306\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 15.1784 - accuracy: 0.3331 - val_loss: 299630428185.5980 - val_accuracy: 0.3306\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 15.1818 - accuracy: 0.3322 - val_loss: 3907510809.7271 - val_accuracy: 0.3306\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 15.1548 - accuracy: 0.3328 - val_loss: 349225386010.0385 - val_accuracy: 0.3306\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 14.5065 - accuracy: 0.3326 - val_loss: 10642826.4935 - val_accuracy: 0.3306\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 14.3331 - accuracy: 0.3330 - val_loss: 12494.6753 - val_accuracy: 0.3306\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 14.2431 - accuracy: 0.3324 - val_loss: 1269.8825 - val_accuracy: 0.3306\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 15.0410 - accuracy: 0.3320 - val_loss: 78.8520 - val_accuracy: 0.3306\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 13.3496 - accuracy: 0.3337 - val_loss: 61.0122 - val_accuracy: 0.3306\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 13.6114 - accuracy: 0.3331 - val_loss: 53.0751 - val_accuracy: 0.3306\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 13.3951 - accuracy: 0.3320 - val_loss: 98.5535 - val_accuracy: 0.3306\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 13.2055 - accuracy: 0.3320 - val_loss: 548.0964 - val_accuracy: 0.3306\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 12.2835 - accuracy: 0.3326 - val_loss: 3020.2330 - val_accuracy: 0.3306\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 12.2059 - accuracy: 0.3327 - val_loss: 8064.5844 - val_accuracy: 0.3306\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 12.4855 - accuracy: 0.3322 - val_loss: 174846.4899 - val_accuracy: 0.3306\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 12.2398 - accuracy: 0.3328 - val_loss: 63.4392 - val_accuracy: 0.3307\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 11.7907 - accuracy: 0.3327 - val_loss: 96518.5659 - val_accuracy: 0.3306\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 11.5198 - accuracy: 0.3321 - val_loss: 161519.6509 - val_accuracy: 0.3307\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 11.3030 - accuracy: 0.3332 - val_loss: 7843.4247 - val_accuracy: 0.3307\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 96s 3s/step - loss: 12.0250 - accuracy: 0.3322 - val_loss: 2475.5493 - val_accuracy: 0.3307\n",
      "Epoch 57/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 11.3195 - accuracy: 0.3325 - val_loss: 613.0974 - val_accuracy: 0.3306\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 10.9887 - accuracy: 0.3320 - val_loss: 207.5127 - val_accuracy: 0.3306\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 10.7220 - accuracy: 0.3334 - val_loss: 9152.7846 - val_accuracy: 0.3306\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 11.7614 - accuracy: 0.3324 - val_loss: 7852689.1628 - val_accuracy: 0.3306\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 10.5864 - accuracy: 0.3324 - val_loss: 353258.4508 - val_accuracy: 0.3306\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 10.3215 - accuracy: 0.3328 - val_loss: 1499.7399 - val_accuracy: 0.3306\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 9.9659 - accuracy: 0.3328 - val_loss: 2920.4169 - val_accuracy: 0.3306\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 10.7039 - accuracy: 0.3316 - val_loss: 61.8749 - val_accuracy: 0.3306\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 9.6304 - accuracy: 0.3329 - val_loss: 53.9866 - val_accuracy: 0.3306\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 9.6454 - accuracy: 0.3324 - val_loss: 57.7737 - val_accuracy: 0.3306\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 9.7345 - accuracy: 0.3320 - val_loss: 65.8012 - val_accuracy: 0.3306\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 9.5071 - accuracy: 0.3328 - val_loss: 183949.3995 - val_accuracy: 0.3306\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 9.3894 - accuracy: 0.3321 - val_loss: 35932.4704 - val_accuracy: 0.3306\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 9.8466 - accuracy: 0.3323 - val_loss: 116.0497 - val_accuracy: 0.3306\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 9.1562 - accuracy: 0.3322 - val_loss: 116.0512 - val_accuracy: 0.3306\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 8.3413 - accuracy: 0.3325 - val_loss: 61.1070 - val_accuracy: 0.3306\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 8.4434 - accuracy: 0.3322 - val_loss: 59.8177 - val_accuracy: 0.3306\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 8.0481 - accuracy: 0.3324 - val_loss: 59.2383 - val_accuracy: 0.3306\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 8.3622 - accuracy: 0.3323 - val_loss: 59.4093 - val_accuracy: 0.3306\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 7.8334 - accuracy: 0.3324 - val_loss: 58.7436 - val_accuracy: 0.3306\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 7.9802 - accuracy: 0.3320 - val_loss: 57.8063 - val_accuracy: 0.3306\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 7.6661 - accuracy: 0.3324 - val_loss: 51.7801 - val_accuracy: 0.3306\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 7.6758 - accuracy: 0.3321 - val_loss: 54.8783 - val_accuracy: 0.3306\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 7.4708 - accuracy: 0.3319 - val_loss: 51.4194 - val_accuracy: 0.3306\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 7.3497 - accuracy: 0.3328 - val_loss: 53.9765 - val_accuracy: 0.3307\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 7.2172 - accuracy: 0.3321 - val_loss: 57.1253 - val_accuracy: 0.3306\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 7.1744 - accuracy: 0.3319 - val_loss: 58.6348 - val_accuracy: 0.3307\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 6.8662 - accuracy: 0.3322 - val_loss: 57.7001 - val_accuracy: 0.3307\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 6.9433 - accuracy: 0.3325 - val_loss: 57.3284 - val_accuracy: 0.3307\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 6.9447 - accuracy: 0.3328 - val_loss: 57.6865 - val_accuracy: 0.3307\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 6.7794 - accuracy: 0.3324 - val_loss: 90.6755 - val_accuracy: 0.3306\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 6.8282 - accuracy: 0.3321 - val_loss: 63.2719 - val_accuracy: 0.3306\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 6.1584 - accuracy: 0.3334 - val_loss: 61.9947 - val_accuracy: 0.3306\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 6.3106 - accuracy: 0.3324 - val_loss: 61.1035 - val_accuracy: 0.3306\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 6.2171 - accuracy: 0.3324 - val_loss: 58.2990 - val_accuracy: 0.3306\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 6.9428 - accuracy: 0.3326 - val_loss: 57.6231 - val_accuracy: 0.3307\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 6.1157 - accuracy: 0.3329 - val_loss: 55.6903 - val_accuracy: 0.3307\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 5.7988 - accuracy: 0.3322 - val_loss: 56.0024 - val_accuracy: 0.3306\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 5.4804 - accuracy: 0.3326 - val_loss: 57.0804 - val_accuracy: 0.3306\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 5.5754 - accuracy: 0.3323 - val_loss: 60.6057 - val_accuracy: 0.3306\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 5.4638 - accuracy: 0.3322 - val_loss: 62.7793 - val_accuracy: 0.3307\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 5.4651 - accuracy: 0.3320 - val_loss: 63.8227 - val_accuracy: 0.3306\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 5.5467 - accuracy: 0.3320 - val_loss: 59.6600 - val_accuracy: 0.3306\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 5.2280 - accuracy: 0.3321 - val_loss: 58.6809 - val_accuracy: 0.3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa53aa98240>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 100, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_17 to have 4 dimensions, but got array with shape (480, 480, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-ed687acf8d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaaa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/WorkStation-Subrata/python/venv1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/WorkStation-Subrata/python/venv1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/WorkStation-Subrata/python/venv1/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_17 to have 4 dimensions, but got array with shape (480, 480, 3)"
     ]
    }
   ],
   "source": [
    "aaaa = my_model_2.predict(X_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "XXX = np.expand_dims(X_train[3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 480, 480, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XXX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15, 15, 3, 10)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbbb = my_model_2.predict(XXX)\n",
    "bbbb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.895462  ,  -7.1587195 ,  -0.4056387 ,  -0.19739641,\n",
       "         -5.5790863 , -23.71345   , -22.990238  , -23.47159   ,\n",
       "        -23.755758  , -23.471615  ],\n",
       "       [ -1.2802233 ,   0.44125468,  -0.51949126,   0.03848014,\n",
       "         -5.95801   , -23.36485   , -23.66184   , -23.05034   ,\n",
       "        -23.180973  , -22.777544  ],\n",
       "       [ -4.1403956 ,   4.549201  ,   0.7493741 ,   0.34761453,\n",
       "         -6.1389627 , -22.812275  , -23.515343  , -23.347775  ,\n",
       "        -23.445934  , -23.83998   ]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbbb[0,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5]],\n",
       "\n",
       "         [[ 6,  7],\n",
       "          [ 8,  9],\n",
       "          [10, 11]]],\n",
       "\n",
       "\n",
       "        [[[12, 13],\n",
       "          [14, 15],\n",
       "          [16, 17]],\n",
       "\n",
       "         [[18, 19],\n",
       "          [20, 21],\n",
       "          [22, 23]]]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = np.arange(24).reshape(1,2,2,3,2)\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 19],\n",
       "       [20, 21],\n",
       "       [22, 23]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz[0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15],\n",
       "        [16, 17],\n",
       "        [18, 19],\n",
       "        [20, 21],\n",
       "        [22, 23]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zzz = np.reshape(zz, [1,12,2])\n",
    "print(zzz.shape)\n",
    "zzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zzz[0,3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15, 15, 3, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  5.895462  ,  -7.1587195 ,  -0.4056387 ,  -0.19739641,\n",
       "         -5.5790863 , -23.71345   , -22.990238  , -23.47159   ,\n",
       "        -23.755758  , -23.471615  ],\n",
       "       [ -1.2802233 ,   0.44125468,  -0.51949126,   0.03848014,\n",
       "         -5.95801   , -23.36485   , -23.66184   , -23.05034   ,\n",
       "        -23.180973  , -22.777544  ],\n",
       "       [ -4.1403956 ,   4.549201  ,   0.7493741 ,   0.34761453,\n",
       "         -6.1389627 , -22.812275  , -23.515343  , -23.347775  ,\n",
       "        -23.445934  , -23.83998   ]], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bbbb.shape)\n",
    "bbbb[0,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.895462  ,  -7.1587195 ,  -0.4056387 ,  -0.19739641,\n",
       "        -5.5790863 , -23.71345   , -22.990238  , -23.47159   ,\n",
       "       -23.755758  , -23.471615  ], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cccc = np.reshape(bbbb, [1,675,10])\n",
    "cccc[0,99]  ## this gives pred or bbbb[0, 2, 3] starts with = (bbbb_x_box_number*15 + bbbb_y_box_number) * no. of anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x = np.arange(grid_size[1])\n",
    "grid_y = np.arange(grid_size[0])\n",
    "    \n",
    "a = np.array(np.meshgrid(grid_x, grid_y))\n",
    "b = np.array(np.meshgrid(grid_x, grid_y))\n",
    "c = np.array(np.meshgrid(grid_x, grid_y))\n",
    "d = np.concatenate((a,b,c), axis = 0)\n",
    "e = d.transpose(2, 1, 0)\n",
    "grid_final = np.reshape(e,[1,15,15,3,2])\n",
    "\n",
    "grid_size = [15., 15.]\n",
    "grid_stride = 480. / grid_size[0]\n",
    "\n",
    "\n",
    "bbbb[..., 0:2] = my_sigmoid(bbbb[..., 0:2]) + grid_final\n",
    "bbbb[..., 0:2] = (bbbb[..., 0:2] * grid_stride) / 480.\n",
    "bbbb[..., 2:4] = np.exp(bbbb[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "bbbb[..., 4:5] = my_sigmoid(bbbb[..., 4:5])\n",
    "bbbb[..., 5:] = my_softmax(bbbb[..., 5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1699348 , 0.16986041, 0.14679301, 0.14053045, 0.02341698,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.16991921, 0.1698899 , 0.29893446, 0.07986602, 0.01385964,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.16986115, 0.16993517, 0.5860102 , 0.43607962, 0.9536354 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbbb[0,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77933156, 0.10208584, 1.1346279 , 0.5012389 , 0.00278893,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.7793316 , 0.16985787, 0.15319575, 0.14992152, 0.00330623,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.77933156, 0.1699121 , 0.30411652, 0.08444142, 0.00347322,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.77933156, 0.16993633, 1.0899805 , 0.5016871 , 0.00342616,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.7793316 , 0.2376999 , 0.15367216, 0.15111324, 0.00436768,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.77933156, 0.23772322, 0.30931655, 0.08519414, 0.00349862,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.77933156, 0.23773183, 1.0461538 , 0.5344271 , 0.00239655,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.7793316 , 0.30548525, 0.15323152, 0.15064661, 0.00351464,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.77933156, 0.3054941 , 0.30625486, 0.08501402, 0.00326255,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.77933156, 0.30549738, 1.0473427 , 0.53028464, 0.00250799,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cccc[0,500:510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.17067307, 0.16230366, 0.33653846, 0.3036649 , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3][2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use functools\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "anchors = anchors_wrt_target\n",
    "\n",
    "def my_custom_loss(y_true, y_pred):\n",
    "    \n",
    "    def pre_loss(my_custom_loss, anchors):\n",
    "        \n",
    "        num_anchors = len(anchors)\n",
    "        num_classes = 5\n",
    "        ignore_thresh = 0.5\n",
    "        grid_size = [15., 15.]\n",
    "        grid_stride = 480. / grid_size[0]\n",
    "        batch_shape = y_pred.get_shape()\n",
    "        batch_size = batch_shape[0]\n",
    "    \n",
    "        scaled_anchors = anchors / grid_stride\n",
    "    \n",
    "        Lambda_Coord = 5.0\n",
    "        Lambda_no_obj = 0.5\n",
    "    \n",
    "        grid_x = np.arange(grid_size[1])\n",
    "        grid_y = np.arange(grid_size[0])\n",
    "    \n",
    "        a = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        b = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        c = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        d = np.concatenate((a,b,c), axis = 0)\n",
    "        e = d.transpose(2, 1, 0)\n",
    "        grid_final = np.reshape(e,[1,15,15,3,2])\n",
    "    \n",
    "        tot_loss = tf.zeros(1, dtype='float32')\n",
    "\n",
    "        obj_mask = y_true[..., 4:5]\n",
    "\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_box_xy = K.sigmoid(y_pred[..., :2]) + grid_final  # this gives x & y in no. of cells. x & y w.r.t. target\n",
    "                                                            # image = (x & y in no. of cells) / no. of cells\n",
    "        pred_box_xy_wrt_target_image = (pred_box_xy * grid_stride) / 480.\n",
    "        true_box_xy_wrt_target_image = y_true[..., :2]\n",
    "\n",
    "        xy_arr = np.power((true_box_xy_wrt_target_image - pred_box_xy_wrt_target_image), 2)\n",
    "    \n",
    "        xy_loss = K.cast((Lambda_Coord * np.sum(xy_arr * obj_mask)), dtype = 'float32')\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_box_wdht = K.exp(y_pred[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "    \n",
    "        true_box_wdht = y_true[..., 2:4]\n",
    "    \n",
    "        wh_arr = np.power((true_box_wdht - pred_box_wdht), 2)\n",
    "    \n",
    "        wh_loss = K.cast((Lambda_Coord * np.sum(wh_arr * obj_mask)), dtype = 'float32')\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_obj_mask = K.cast(K.sigmoid(y_pred[..., 4:5]), dtype = 'float32')  # shape = 28, 15, 15, 3, 1\n",
    "       \n",
    "        true_box_wrt_ti = K.concatenate([true_box_xy_wrt_target_image, true_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "        pred_box_wrt_ti = K.concatenate([pred_box_xy_wrt_target_image, pred_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "    \n",
    "        ignore_mask = calc_ignore_mask(ignore_thresh, true_box_wrt_ti, pred_box_wrt_ti)\n",
    "        \n",
    "        bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "        obj_loss = K.sum(bce(obj_mask, pred_obj_mask) * obj_mask)\n",
    "        obj_loss = K.cast(obj_loss, dtype = 'float32')\n",
    "    \n",
    "        no_obj_mask = 1. - obj_mask\n",
    "    \n",
    "        noobj_loss = Lambda_no_obj * K.sum(bce(obj_mask, pred_obj_mask) * no_obj_mask * ignore_mask)\n",
    "        noobj_loss = K.cast(noobj_loss, dtype = 'float32')\n",
    "        \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        true_classes = K.cast(y_true[..., 5:10], dtype = 'float32')\n",
    "    \n",
    "        pred_classes = K.cast(K.sigmoid(y_pred[..., 5:10]), dtype = 'float32')\n",
    "        \n",
    "        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "        class_loss = K.sum(bce(true_classes, pred_classes) * obj_mask)\n",
    "        class_loss = K.cast(class_loss, dtype = 'float32')\n",
    "\n",
    "        tot_loss = xy_loss + wh_loss + obj_loss + noobj_loss + class_loss\n",
    "        \n",
    "        return tot_loss\n",
    "    \n",
    "    loss = K.cast(pre_loss(my_custom_loss, anchors), dtype = 'float32')\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "my_model_1.compile(optimizer= opt, loss = my_yolo_loss, metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 293s 10s/step - loss: 957.9036 - accuracy: 0.1117 - val_loss: 931.7901 - val_accuracy: 0.3233\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 120s 4s/step - loss: 839.0445 - accuracy: 0.1275 - val_loss: 914.4023 - val_accuracy: 0.3285\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 92s 3s/step - loss: 729.1806 - accuracy: 0.1245 - val_loss: 889.7219 - val_accuracy: 0.3289\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 97s 3s/step - loss: 639.6839 - accuracy: 0.1281 - val_loss: 862.9572 - val_accuracy: 0.1370\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 94s 3s/step - loss: 534.8076 - accuracy: 0.1485 - val_loss: 825.4424 - val_accuracy: 0.0419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa492149b00>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_1.fit(X_train ,Y_train, epochs= 5, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 90s 3s/step - loss: 472.8064 - accuracy: 0.1704 - val_loss: 783.5237 - val_accuracy: 0.0593\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 411.6622 - accuracy: 0.1854 - val_loss: 752.9295 - val_accuracy: 0.0187\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 356.5063 - accuracy: 0.1896 - val_loss: 706.9615 - val_accuracy: 0.0354\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 340.9635 - accuracy: 0.1732 - val_loss: 679.0995 - val_accuracy: 0.1413\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 296.6083 - accuracy: 0.1853 - val_loss: 644.6458 - val_accuracy: 0.1667\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 276.6145 - accuracy: 0.1922 - val_loss: 611.4120 - val_accuracy: 0.1591\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 259.5459 - accuracy: 0.2007 - val_loss: 557.0526 - val_accuracy: 0.1854\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 241.1478 - accuracy: 0.1959 - val_loss: 562.9391 - val_accuracy: 1.8519e-04\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 214.6950 - accuracy: 0.1724 - val_loss: 518.0573 - val_accuracy: 0.0015\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 222.0183 - accuracy: 0.1304 - val_loss: 457.5370 - val_accuracy: 0.0044\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 200.4446 - accuracy: 0.1185 - val_loss: 421.9032 - val_accuracy: 0.0030\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 190.0992 - accuracy: 0.1263 - val_loss: 453.0831 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 175.4455 - accuracy: 0.1307 - val_loss: 391.5717 - val_accuracy: 0.0028\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 166.7859 - accuracy: 0.1410 - val_loss: 346.9754 - val_accuracy: 0.0044\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 165.3582 - accuracy: 0.1444 - val_loss: 341.8183 - val_accuracy: 0.0044\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 162.9191 - accuracy: 0.1646 - val_loss: 342.5286 - val_accuracy: 0.0289\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 147.7580 - accuracy: 0.1685 - val_loss: 328.8487 - val_accuracy: 0.0446\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 147.6815 - accuracy: 0.1899 - val_loss: 299.2638 - val_accuracy: 0.0772\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 144.8776 - accuracy: 0.1938 - val_loss: 273.8881 - val_accuracy: 0.1002\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 135.1585 - accuracy: 0.2058 - val_loss: 264.9570 - val_accuracy: 0.2511\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 135.4959 - accuracy: 0.2316 - val_loss: 250.7734 - val_accuracy: 0.2559\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 130.8939 - accuracy: 0.2550 - val_loss: 237.5883 - val_accuracy: 0.2515\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 125.0736 - accuracy: 0.2762 - val_loss: 229.3288 - val_accuracy: 0.2696\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 121.0225 - accuracy: 0.2884 - val_loss: 209.1653 - val_accuracy: 0.2728\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 119.0853 - accuracy: 0.3034 - val_loss: 208.1390 - val_accuracy: 0.2861\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 117.0567 - accuracy: 0.3193 - val_loss: 205.1084 - val_accuracy: 0.2930\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 111.3786 - accuracy: 0.3229 - val_loss: 180.4088 - val_accuracy: 0.2937\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 108.1148 - accuracy: 0.3208 - val_loss: 173.5910 - val_accuracy: 0.2981\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 105.6717 - accuracy: 0.3253 - val_loss: 179.0875 - val_accuracy: 0.3161\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 106.4629 - accuracy: 0.3326 - val_loss: 170.2954 - val_accuracy: 0.3306\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 100.5615 - accuracy: 0.3358 - val_loss: 165.4857 - val_accuracy: 0.3306\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 103.6404 - accuracy: 0.3342 - val_loss: 154.8922 - val_accuracy: 0.3306\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 101.1971 - accuracy: 0.3373 - val_loss: 149.9152 - val_accuracy: 0.3306\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 97.9288 - accuracy: 0.3331 - val_loss: 136.1611 - val_accuracy: 0.3306\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 93.8862 - accuracy: 0.3294 - val_loss: 135.2588 - val_accuracy: 0.3306\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 94.4371 - accuracy: 0.3292 - val_loss: 138.2865 - val_accuracy: 0.3306\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 91.9166 - accuracy: 0.3278 - val_loss: 127.9094 - val_accuracy: 0.3306\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 88.8301 - accuracy: 0.3266 - val_loss: 119.2636 - val_accuracy: 0.3306\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 88.9541 - accuracy: 0.3263 - val_loss: 121.5617 - val_accuracy: 0.3306\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 86.1670 - accuracy: 0.3261 - val_loss: 117.3173 - val_accuracy: 0.3306\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 86.5095 - accuracy: 0.3266 - val_loss: 114.8262 - val_accuracy: 0.3306\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 85.1257 - accuracy: 0.3264 - val_loss: 108.9441 - val_accuracy: 0.3306\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 81.8024 - accuracy: 0.3266 - val_loss: 105.6527 - val_accuracy: 0.3306\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 85.3065 - accuracy: 0.3263 - val_loss: 108.6580 - val_accuracy: 0.3306\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 79.5952 - accuracy: 0.3261 - val_loss: 98.5757 - val_accuracy: 0.3306\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 79.5249 - accuracy: 0.3261 - val_loss: 100.4770 - val_accuracy: 0.3306\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 75.4925 - accuracy: 0.3267 - val_loss: 100.3834 - val_accuracy: 0.3306\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 76.3200 - accuracy: 0.3268 - val_loss: 90.1972 - val_accuracy: 0.3306\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 75.2857 - accuracy: 0.3270 - val_loss: 88.9335 - val_accuracy: 0.3306\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 73.7467 - accuracy: 0.3270 - val_loss: 90.1680 - val_accuracy: 0.3306\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 71.1257 - accuracy: 0.3270 - val_loss: 89.8495 - val_accuracy: 0.3306\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 70.1657 - accuracy: 0.3268 - val_loss: 83.9199 - val_accuracy: 0.3306\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 67.3567 - accuracy: 0.3274 - val_loss: 84.7382 - val_accuracy: 0.3306\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 67.4149 - accuracy: 0.3273 - val_loss: 84.0030 - val_accuracy: 0.3306\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 67.1483 - accuracy: 0.3276 - val_loss: 81.5518 - val_accuracy: 0.3306\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 66.1785 - accuracy: 0.3279 - val_loss: 76.1349 - val_accuracy: 0.3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 62.9287 - accuracy: 0.3280 - val_loss: 74.9196 - val_accuracy: 0.3306\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 61.9291 - accuracy: 0.3283 - val_loss: 75.1461 - val_accuracy: 0.3306\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 63.9439 - accuracy: 0.3284 - val_loss: 71.7194 - val_accuracy: 0.3306\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 60.2151 - accuracy: 0.3282 - val_loss: 76.2033 - val_accuracy: 0.3306\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 58.5981 - accuracy: 0.3288 - val_loss: 75.0357 - val_accuracy: 0.3306\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 56.8788 - accuracy: 0.3286 - val_loss: 71.1936 - val_accuracy: 0.3306\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 56.7259 - accuracy: 0.3289 - val_loss: 73.3422 - val_accuracy: 0.3306\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 55.4230 - accuracy: 0.3287 - val_loss: 67.8186 - val_accuracy: 0.3306\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 53.8882 - accuracy: 0.3294 - val_loss: 67.9142 - val_accuracy: 0.3306\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 53.8380 - accuracy: 0.3296 - val_loss: 68.0693 - val_accuracy: 0.3306\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 52.8333 - accuracy: 0.3294 - val_loss: 66.5697 - val_accuracy: 0.3306\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 54.4735 - accuracy: 0.3296 - val_loss: 66.9599 - val_accuracy: 0.3306\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 50.5196 - accuracy: 0.3296 - val_loss: 65.4649 - val_accuracy: 0.3306\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 47.1166 - accuracy: 0.3297 - val_loss: 65.5456 - val_accuracy: 0.3306\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 49.3172 - accuracy: 0.3299 - val_loss: 63.4052 - val_accuracy: 0.3306\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 45.5073 - accuracy: 0.3298 - val_loss: 63.3984 - val_accuracy: 0.3306\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 44.5388 - accuracy: 0.3296 - val_loss: 64.4659 - val_accuracy: 0.3306\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 44.3072 - accuracy: 0.3302 - val_loss: 63.7272 - val_accuracy: 0.3306\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 45.3678 - accuracy: 0.3299 - val_loss: 61.3925 - val_accuracy: 0.3306\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 41.9759 - accuracy: 0.3298 - val_loss: 62.7698 - val_accuracy: 0.3306\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 42.0723 - accuracy: 0.3300 - val_loss: 60.9863 - val_accuracy: 0.3306\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 41.3460 - accuracy: 0.3302 - val_loss: 58.6593 - val_accuracy: 0.3306\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 41.4537 - accuracy: 0.3304 - val_loss: 59.6456 - val_accuracy: 0.3306\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 39.2835 - accuracy: 0.3301 - val_loss: 59.2489 - val_accuracy: 0.3306\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 42.7773 - accuracy: 0.3296 - val_loss: 59.6429 - val_accuracy: 0.3304\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 38.9646 - accuracy: 0.3294 - val_loss: 61.1635 - val_accuracy: 0.3300\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 39.5081 - accuracy: 0.3297 - val_loss: 60.9184 - val_accuracy: 0.3300\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 34.9876 - accuracy: 0.3296 - val_loss: 58.7216 - val_accuracy: 0.3304\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 39.4728 - accuracy: 0.3292 - val_loss: 60.6545 - val_accuracy: 0.3300\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 34.1603 - accuracy: 0.3289 - val_loss: 57.4948 - val_accuracy: 0.3304\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 35.4783 - accuracy: 0.3297 - val_loss: 57.3941 - val_accuracy: 0.3300\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 32.0500 - accuracy: 0.3295 - val_loss: 57.3230 - val_accuracy: 0.3302\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 31.8763 - accuracy: 0.3295 - val_loss: 58.0466 - val_accuracy: 0.3302\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 33.5260 - accuracy: 0.3297 - val_loss: 57.8398 - val_accuracy: 0.3302\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 31.6114 - accuracy: 0.3294 - val_loss: 57.7816 - val_accuracy: 0.3304\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 31.3860 - accuracy: 0.3299 - val_loss: 56.3673 - val_accuracy: 0.3304\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 29.7927 - accuracy: 0.3295 - val_loss: 56.1655 - val_accuracy: 0.3306\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 31.6795 - accuracy: 0.3301 - val_loss: 56.5831 - val_accuracy: 0.3306\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 30.4103 - accuracy: 0.3299 - val_loss: 55.8353 - val_accuracy: 0.3306\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 29.5203 - accuracy: 0.3299 - val_loss: 55.5822 - val_accuracy: 0.3306\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 29.4103 - accuracy: 0.3298 - val_loss: 55.7148 - val_accuracy: 0.3306\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 28.2610 - accuracy: 0.3302 - val_loss: 54.5826 - val_accuracy: 0.3306\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 27.2062 - accuracy: 0.3298 - val_loss: 54.0704 - val_accuracy: 0.3306\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 26.9310 - accuracy: 0.3299 - val_loss: 53.7218 - val_accuracy: 0.3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa486e9fc88>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_1.fit(X_train ,Y_train, epochs= 100, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 89s 3s/step - loss: 27.1186 - accuracy: 0.3302 - val_loss: 53.8945 - val_accuracy: 0.3306\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 27.0698 - accuracy: 0.3301 - val_loss: 53.8665 - val_accuracy: 0.3306\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 25.5921 - accuracy: 0.3299 - val_loss: 54.4546 - val_accuracy: 0.3306\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 28.1307 - accuracy: 0.3305 - val_loss: 53.9813 - val_accuracy: 0.3306\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 25.3371 - accuracy: 0.3299 - val_loss: 56.0053 - val_accuracy: 0.3306\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 94s 3s/step - loss: 25.8412 - accuracy: 0.3301 - val_loss: 55.1864 - val_accuracy: 0.3306\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 26.0287 - accuracy: 0.3299 - val_loss: 54.5803 - val_accuracy: 0.3306\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 24.3474 - accuracy: 0.3302 - val_loss: 55.3309 - val_accuracy: 0.3306\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 24.0311 - accuracy: 0.3300 - val_loss: 53.1251 - val_accuracy: 0.3306\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 23.1921 - accuracy: 0.3299 - val_loss: 53.0515 - val_accuracy: 0.3306\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 24.3715 - accuracy: 0.3303 - val_loss: 53.0659 - val_accuracy: 0.3306\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 23.6710 - accuracy: 0.3302 - val_loss: 52.8716 - val_accuracy: 0.3306\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 23.1264 - accuracy: 0.3304 - val_loss: 58.6037 - val_accuracy: 0.3306\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 24.1247 - accuracy: 0.3301 - val_loss: 235.9435 - val_accuracy: 0.3306\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 23.2430 - accuracy: 0.3303 - val_loss: 89.5811 - val_accuracy: 0.3306\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 22.8149 - accuracy: 0.3302 - val_loss: 57.7194 - val_accuracy: 0.3306\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 22.4959 - accuracy: 0.3302 - val_loss: 61.3490 - val_accuracy: 0.3306\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 22.1153 - accuracy: 0.3302 - val_loss: 56.4281 - val_accuracy: 0.3306\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 21.8916 - accuracy: 0.3303 - val_loss: 53.5152 - val_accuracy: 0.3306\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 21.2372 - accuracy: 0.3302 - val_loss: 53.9578 - val_accuracy: 0.3306\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 21.1713 - accuracy: 0.3302 - val_loss: 54.8713 - val_accuracy: 0.3306\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 21.4488 - accuracy: 0.3301 - val_loss: 55.9957 - val_accuracy: 0.3306\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 22.9511 - accuracy: 0.3303 - val_loss: 56.1887 - val_accuracy: 0.3306\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 20.7252 - accuracy: 0.3302 - val_loss: 54.2153 - val_accuracy: 0.3306\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 20.6275 - accuracy: 0.3302 - val_loss: 53.8993 - val_accuracy: 0.3306\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 20.1666 - accuracy: 0.3301 - val_loss: 59.7575 - val_accuracy: 0.3306\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 20.2984 - accuracy: 0.3301 - val_loss: 52.0329 - val_accuracy: 0.3306\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 19.6197 - accuracy: 0.3302 - val_loss: 52.5952 - val_accuracy: 0.3306\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 19.6919 - accuracy: 0.3301 - val_loss: 55.0973 - val_accuracy: 0.3306\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 19.1186 - accuracy: 0.3301 - val_loss: 85.6698 - val_accuracy: 0.3306\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 19.8489 - accuracy: 0.3302 - val_loss: 71.6915 - val_accuracy: 0.3306\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 19.3639 - accuracy: 0.3302 - val_loss: 76.5967 - val_accuracy: 0.3306\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 18.8138 - accuracy: 0.3301 - val_loss: 57.4283 - val_accuracy: 0.3306\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 19.2478 - accuracy: 0.3301 - val_loss: 51.9205 - val_accuracy: 0.3306\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 18.7153 - accuracy: 0.3303 - val_loss: 51.1421 - val_accuracy: 0.3306\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 19.2902 - accuracy: 0.3304 - val_loss: 52.1581 - val_accuracy: 0.3306\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 18.3822 - accuracy: 0.3302 - val_loss: 60.3221 - val_accuracy: 0.3306\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 18.5039 - accuracy: 0.3302 - val_loss: 51.6282 - val_accuracy: 0.3306\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 18.3679 - accuracy: 0.3303 - val_loss: 92.5138 - val_accuracy: 0.3306\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 17.9338 - accuracy: 0.3302 - val_loss: 2778.6624 - val_accuracy: 0.3306\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 17.2175 - accuracy: 0.3302 - val_loss: 538.5731 - val_accuracy: 0.3306\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 17.9715 - accuracy: 0.3301 - val_loss: 51.3062 - val_accuracy: 0.3306\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 16.8206 - accuracy: 0.3302 - val_loss: 60.0520 - val_accuracy: 0.3306\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 17.7488 - accuracy: 0.3301 - val_loss: 167.6606 - val_accuracy: 0.3306\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 17.2224 - accuracy: 0.3301 - val_loss: 86.6124 - val_accuracy: 0.3306\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 18.1132 - accuracy: 0.3303 - val_loss: 57.9766 - val_accuracy: 0.3306\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 17.2412 - accuracy: 0.3304 - val_loss: 59.6538 - val_accuracy: 0.3306\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 16.1724 - accuracy: 0.3301 - val_loss: 263.8440 - val_accuracy: 0.3306\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 16.2716 - accuracy: 0.3301 - val_loss: 179.9757 - val_accuracy: 0.3306\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 16.3124 - accuracy: 0.3304 - val_loss: 59.2078 - val_accuracy: 0.3306\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 15.5228 - accuracy: 0.3303 - val_loss: 51.8449 - val_accuracy: 0.3306\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 15.6412 - accuracy: 0.3301 - val_loss: 103.7368 - val_accuracy: 0.3306\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 14.9266 - accuracy: 0.3302 - val_loss: 151.6250 - val_accuracy: 0.3306\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 14.5051 - accuracy: 0.3302 - val_loss: 78.0569 - val_accuracy: 0.3306\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 14.1464 - accuracy: 0.3303 - val_loss: 53.6587 - val_accuracy: 0.3306\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 14.0167 - accuracy: 0.3302 - val_loss: 53.0102 - val_accuracy: 0.3306\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 112s 4s/step - loss: 14.1430 - accuracy: 0.3302 - val_loss: 52.9085 - val_accuracy: 0.3306\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 16.8428 - accuracy: 0.3302 - val_loss: 12030.7320 - val_accuracy: 0.3306\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 13.9633 - accuracy: 0.3303 - val_loss: 110.3701 - val_accuracy: 0.3306\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 14.1064 - accuracy: 0.3301 - val_loss: 63.9900 - val_accuracy: 0.3306\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 13.5128 - accuracy: 0.3301 - val_loss: 405.9123 - val_accuracy: 0.3306\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 13.4988 - accuracy: 0.3302 - val_loss: 1380.2514 - val_accuracy: 0.3306\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 12.4379 - accuracy: 0.3303 - val_loss: 60.1511 - val_accuracy: 0.3306\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 12.3633 - accuracy: 0.3301 - val_loss: 52.3358 - val_accuracy: 0.3306\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 12.3548 - accuracy: 0.3302 - val_loss: 50.3462 - val_accuracy: 0.3306\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 12.1179 - accuracy: 0.3302 - val_loss: 50.1797 - val_accuracy: 0.3306\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 12.8651 - accuracy: 0.3301 - val_loss: 51.7488 - val_accuracy: 0.3306\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 12.1310 - accuracy: 0.3301 - val_loss: 54.3583 - val_accuracy: 0.3306\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 11.5275 - accuracy: 0.3302 - val_loss: 61.6419 - val_accuracy: 0.3306\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 11.6013 - accuracy: 0.3303 - val_loss: 54.9931 - val_accuracy: 0.3306\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 11.5910 - accuracy: 0.3301 - val_loss: 51.9808 - val_accuracy: 0.3306\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 11.6041 - accuracy: 0.3302 - val_loss: 51.8943 - val_accuracy: 0.3306\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 11.3030 - accuracy: 0.3306 - val_loss: 361.0678 - val_accuracy: 0.3306\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 10.5109 - accuracy: 0.3302 - val_loss: 390.5593 - val_accuracy: 0.3306\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 10.6306 - accuracy: 0.3303 - val_loss: 250.6458 - val_accuracy: 0.3306\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 10.5965 - accuracy: 0.3301 - val_loss: 389.8594 - val_accuracy: 0.3306\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 10.4322 - accuracy: 0.3301 - val_loss: 92.5609 - val_accuracy: 0.3306\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 10.4638 - accuracy: 0.3301 - val_loss: 89.5967 - val_accuracy: 0.3306\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 10.6359 - accuracy: 0.3303 - val_loss: 55.8150 - val_accuracy: 0.3306\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 12.0407 - accuracy: 0.3303 - val_loss: 51.3245 - val_accuracy: 0.3306\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 9.8531 - accuracy: 0.3302 - val_loss: 50.2561 - val_accuracy: 0.3306\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 9.6812 - accuracy: 0.3303 - val_loss: 50.3578 - val_accuracy: 0.3306\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 9.5205 - accuracy: 0.3301 - val_loss: 51.3553 - val_accuracy: 0.3306\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 9.4484 - accuracy: 0.3302 - val_loss: 50.7860 - val_accuracy: 0.3306\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 9.7410 - accuracy: 0.3304 - val_loss: 49.9254 - val_accuracy: 0.3306\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 9.2426 - accuracy: 0.3303 - val_loss: 52.0086 - val_accuracy: 0.3306\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 9.0547 - accuracy: 0.3301 - val_loss: 53.7938 - val_accuracy: 0.3306\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 9.0720 - accuracy: 0.3302 - val_loss: 56.4775 - val_accuracy: 0.3306\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 8.6370 - accuracy: 0.3301 - val_loss: 53.6273 - val_accuracy: 0.3306\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 8.6134 - accuracy: 0.3304 - val_loss: 53.3950 - val_accuracy: 0.3306\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 8.4849 - accuracy: 0.3302 - val_loss: 54.4934 - val_accuracy: 0.3306\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 8.2272 - accuracy: 0.3302 - val_loss: 53.3244 - val_accuracy: 0.3306\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 8.7005 - accuracy: 0.3301 - val_loss: 51.4189 - val_accuracy: 0.3306\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 7.8954 - accuracy: 0.3303 - val_loss: 50.6344 - val_accuracy: 0.3306\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 8.9451 - accuracy: 0.3303 - val_loss: 51.2995 - val_accuracy: 0.3306\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 7.8663 - accuracy: 0.3301 - val_loss: 53.4574 - val_accuracy: 0.3306\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 7.5165 - accuracy: 0.3301 - val_loss: 53.6024 - val_accuracy: 0.3306\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 7.6994 - accuracy: 0.3302 - val_loss: 52.1757 - val_accuracy: 0.3306\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 7.3727 - accuracy: 0.3301 - val_loss: 50.9852 - val_accuracy: 0.3306\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 7.2275 - accuracy: 0.3303 - val_loss: 50.9279 - val_accuracy: 0.3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa486e4eb70>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_1.fit(X_train ,Y_train, epochs= 100, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 89s 3s/step - loss: 7.1162 - accuracy: 0.3302 - val_loss: 49.9227 - val_accuracy: 0.3306\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 95s 3s/step - loss: 7.1983 - accuracy: 0.3302 - val_loss: 51.0438 - val_accuracy: 0.3306\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 6.8234 - accuracy: 0.3302 - val_loss: 50.2813 - val_accuracy: 0.3306\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 6.7974 - accuracy: 0.3303 - val_loss: 52.5311 - val_accuracy: 0.3306\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 7.1179 - accuracy: 0.3301 - val_loss: 53.5125 - val_accuracy: 0.3306\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 6.6386 - accuracy: 0.3301 - val_loss: 51.1172 - val_accuracy: 0.3306\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 6.7435 - accuracy: 0.3302 - val_loss: 51.2411 - val_accuracy: 0.3306\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 6.8698 - accuracy: 0.3304 - val_loss: 52.2050 - val_accuracy: 0.3306\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 6.5420 - accuracy: 0.3302 - val_loss: 53.9653 - val_accuracy: 0.3306\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 6.4414 - accuracy: 0.3302 - val_loss: 56.3458 - val_accuracy: 0.3306\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 6.1341 - accuracy: 0.3303 - val_loss: 54.7740 - val_accuracy: 0.3306\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 5.9995 - accuracy: 0.3302 - val_loss: 51.5545 - val_accuracy: 0.3306\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 6.4145 - accuracy: 0.3301 - val_loss: 51.3090 - val_accuracy: 0.3306\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 6.2937 - accuracy: 0.3301 - val_loss: 54.4316 - val_accuracy: 0.3306\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 6.1514 - accuracy: 0.3303 - val_loss: 54.5784 - val_accuracy: 0.3306\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 5.7678 - accuracy: 0.3301 - val_loss: 52.6868 - val_accuracy: 0.3306\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 5.5535 - accuracy: 0.3301 - val_loss: 52.1988 - val_accuracy: 0.3306\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 5.6768 - accuracy: 0.3304 - val_loss: 52.7153 - val_accuracy: 0.3306\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 5.5315 - accuracy: 0.3301 - val_loss: 54.2109 - val_accuracy: 0.3306\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 5.4572 - accuracy: 0.3304 - val_loss: 53.7457 - val_accuracy: 0.3306\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 5.4179 - accuracy: 0.3302 - val_loss: 52.8578 - val_accuracy: 0.3306\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 6.0732 - accuracy: 0.3302 - val_loss: 52.9975 - val_accuracy: 0.3306\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 5.2964 - accuracy: 0.3302 - val_loss: 55.0973 - val_accuracy: 0.3306\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 5.1850 - accuracy: 0.3301 - val_loss: 54.1745 - val_accuracy: 0.3306\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 5.1246 - accuracy: 0.3303 - val_loss: 55.0160 - val_accuracy: 0.3306\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 4.8787 - accuracy: 0.3302 - val_loss: 55.9909 - val_accuracy: 0.3306\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 4.8835 - accuracy: 0.3301 - val_loss: 55.0376 - val_accuracy: 0.3306\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 5.0726 - accuracy: 0.3301 - val_loss: 54.4343 - val_accuracy: 0.3306\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 4.7989 - accuracy: 0.3303 - val_loss: 55.0461 - val_accuracy: 0.3306\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 4.8318 - accuracy: 0.3304 - val_loss: 55.0587 - val_accuracy: 0.3306\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 107s 4s/step - loss: 4.8310 - accuracy: 0.3302 - val_loss: 58.2278 - val_accuracy: 0.3306\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 4.7586 - accuracy: 0.3304 - val_loss: 58.3957 - val_accuracy: 0.3306\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 4.8917 - accuracy: 0.3308 - val_loss: 54.7937 - val_accuracy: 0.3306\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 4.5111 - accuracy: 0.3303 - val_loss: 53.0246 - val_accuracy: 0.3306\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 4.8022 - accuracy: 0.3301 - val_loss: 54.4757 - val_accuracy: 0.3306\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 4.6240 - accuracy: 0.3301 - val_loss: 58.5376 - val_accuracy: 0.3306\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 5.7284 - accuracy: 0.3303 - val_loss: 55.8318 - val_accuracy: 0.3306\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 4.6676 - accuracy: 0.3302 - val_loss: 55.4882 - val_accuracy: 0.3306\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 4.7036 - accuracy: 0.3302 - val_loss: 54.4390 - val_accuracy: 0.3306\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 4.3535 - accuracy: 0.3302 - val_loss: 54.4318 - val_accuracy: 0.3306\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 4.2041 - accuracy: 0.3301 - val_loss: 55.4122 - val_accuracy: 0.3306\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 4.2118 - accuracy: 0.3303 - val_loss: 54.8443 - val_accuracy: 0.3306\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 4.1376 - accuracy: 0.3302 - val_loss: 53.2849 - val_accuracy: 0.3306\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.8876 - accuracy: 0.3303 - val_loss: 53.6708 - val_accuracy: 0.3306\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 4.1129 - accuracy: 0.3301 - val_loss: 56.6291 - val_accuracy: 0.3306\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 4.0511 - accuracy: 0.3302 - val_loss: 61.8185 - val_accuracy: 0.3306\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 4.2091 - accuracy: 0.3303 - val_loss: 60.3641 - val_accuracy: 0.3306\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 114s 4s/step - loss: 4.4389 - accuracy: 0.3305 - val_loss: 58.6629 - val_accuracy: 0.3306\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 3.7323 - accuracy: 0.3301 - val_loss: 60.3582 - val_accuracy: 0.3306\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 4.3267 - accuracy: 0.3302 - val_loss: 59.4991 - val_accuracy: 0.3306\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 117s 4s/step - loss: 3.8565 - accuracy: 0.3302 - val_loss: 55.5936 - val_accuracy: 0.3306\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 3.6518 - accuracy: 0.3303 - val_loss: 55.2076 - val_accuracy: 0.3306\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.8625 - accuracy: 0.3304 - val_loss: 52.5116 - val_accuracy: 0.3306\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 114s 4s/step - loss: 3.5709 - accuracy: 0.3302 - val_loss: 54.4139 - val_accuracy: 0.3306\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.5956 - accuracy: 0.3302 - val_loss: 54.6962 - val_accuracy: 0.3306\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.9506 - accuracy: 0.3302 - val_loss: 54.2553 - val_accuracy: 0.3306\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 112s 4s/step - loss: 3.9100 - accuracy: 0.3304 - val_loss: 53.5743 - val_accuracy: 0.3306\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 3.6815 - accuracy: 0.3307 - val_loss: 55.3715 - val_accuracy: 0.3306\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.7866 - accuracy: 0.3305 - val_loss: 55.8193 - val_accuracy: 0.3306\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 117s 4s/step - loss: 3.3329 - accuracy: 0.3303 - val_loss: 54.6202 - val_accuracy: 0.3306\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.5062 - accuracy: 0.3301 - val_loss: 55.5047 - val_accuracy: 0.3306\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.3674 - accuracy: 0.3303 - val_loss: 57.6111 - val_accuracy: 0.3306\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 114s 4s/step - loss: 3.5602 - accuracy: 0.3303 - val_loss: 60.2342 - val_accuracy: 0.3306\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.2881 - accuracy: 0.3301 - val_loss: 61.7143 - val_accuracy: 0.3306\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.2192 - accuracy: 0.3304 - val_loss: 60.0778 - val_accuracy: 0.3306\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.1587 - accuracy: 0.3303 - val_loss: 59.8083 - val_accuracy: 0.3306\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.4302 - accuracy: 0.3302 - val_loss: 58.7638 - val_accuracy: 0.3306\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 3.3166 - accuracy: 0.3301 - val_loss: 58.9320 - val_accuracy: 0.3306\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 3.3019 - accuracy: 0.3302 - val_loss: 54.6774 - val_accuracy: 0.3306\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.2889 - accuracy: 0.3303 - val_loss: 54.0451 - val_accuracy: 0.3306\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.1503 - accuracy: 0.3303 - val_loss: 58.7350 - val_accuracy: 0.3306\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.1438 - accuracy: 0.3302 - val_loss: 70.0095 - val_accuracy: 0.3306\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.3791 - accuracy: 0.3304 - val_loss: 66.6929 - val_accuracy: 0.3306\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.0643 - accuracy: 0.3307 - val_loss: 72.6989 - val_accuracy: 0.3306\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.1844 - accuracy: 0.3303 - val_loss: 78.2397 - val_accuracy: 0.3306\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.1992 - accuracy: 0.3303 - val_loss: 73.0195 - val_accuracy: 0.3306\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 2.9372 - accuracy: 0.3305 - val_loss: 72.2679 - val_accuracy: 0.3306\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.9293 - accuracy: 0.3303 - val_loss: 71.5236 - val_accuracy: 0.3306\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 3.0128 - accuracy: 0.3303 - val_loss: 69.5276 - val_accuracy: 0.3306\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 2.6831 - accuracy: 0.3305 - val_loss: 66.7664 - val_accuracy: 0.3306\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.7172 - accuracy: 0.3304 - val_loss: 67.6742 - val_accuracy: 0.3306\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 109s 4s/step - loss: 2.9849 - accuracy: 0.3301 - val_loss: 65.3419 - val_accuracy: 0.3306\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 4.8693 - accuracy: 0.3306 - val_loss: 60.4647 - val_accuracy: 0.3306\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.8804 - accuracy: 0.3309 - val_loss: 53.3638 - val_accuracy: 0.3306\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 2.9952 - accuracy: 0.3306 - val_loss: 60.1265 - val_accuracy: 0.3306\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.8636 - accuracy: 0.3305 - val_loss: 60.6978 - val_accuracy: 0.3306\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.7143 - accuracy: 0.3304 - val_loss: 60.7779 - val_accuracy: 0.3306\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 3.0485 - accuracy: 0.3305 - val_loss: 65.1577 - val_accuracy: 0.3306\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 2.5955 - accuracy: 0.3303 - val_loss: 64.5961 - val_accuracy: 0.3306\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.7568 - accuracy: 0.3304 - val_loss: 62.0573 - val_accuracy: 0.3306\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 2.5997 - accuracy: 0.3305 - val_loss: 61.0519 - val_accuracy: 0.3306\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 2.5680 - accuracy: 0.3304 - val_loss: 62.8533 - val_accuracy: 0.3306\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 2.6392 - accuracy: 0.3304 - val_loss: 62.2700 - val_accuracy: 0.3306\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.4865 - accuracy: 0.3305 - val_loss: 65.1582 - val_accuracy: 0.3306\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.6912 - accuracy: 0.3303 - val_loss: 66.8789 - val_accuracy: 0.3306\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 2.3644 - accuracy: 0.3305 - val_loss: 65.4340 - val_accuracy: 0.3306\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 113s 4s/step - loss: 2.8283 - accuracy: 0.3304 - val_loss: 67.2407 - val_accuracy: 0.3306\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 110s 4s/step - loss: 2.4509 - accuracy: 0.3306 - val_loss: 70.1560 - val_accuracy: 0.3306\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 112s 4s/step - loss: 2.2921 - accuracy: 0.3303 - val_loss: 70.7267 - val_accuracy: 0.3306\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 111s 4s/step - loss: 3.5712 - accuracy: 0.3306 - val_loss: 71.3608 - val_accuracy: 0.3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa53a985240>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_1.fit(X_train ,Y_train, epochs= 100, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15, 15, 3, 10)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_2 = my_model_1.predict(XXX)\n",
    "pred_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15, 15, 3, 10)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.zeros((675,10))\n",
    "out = np.reshape(out, [1,15,15,3,10])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16685548, 0.1668554 , 0.16714459, 0.1591167 , 0.62383407,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756],\n",
       "       [0.16685547, 0.16685543, 0.38706994, 0.08607214, 0.62327319,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756],\n",
       "       [0.1668554 , 0.16685548, 0.8047214 , 0.56012524, 0.67301333,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[..., 0:2] = my_sigmoid(pred_2[..., 0:2]) + grid_final\n",
    "out[..., 0:2] = (out[..., 0:2] * grid_stride) / 480.\n",
    "out[..., 2:4] = np.exp(pred_2[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "out[..., 4:5] = my_sigmoid(pred_2[..., 4:5])\n",
    "out[..., 5:] = my_sigmoid(pred_2[..., 5:])\n",
    "\n",
    "out[0,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.17067307, 0.16230366, 0.33653846, 0.3036649 , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3][2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58894235, 0.06806283, 0.16826923, 0.12565444, 1.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3][1,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10011343, 0.56730701, 0.16758227, 0.15945843, 0.67374051,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756],\n",
       "       [0.10011333, 0.56730701, 0.38946299, 0.08609107, 0.62353903,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756],\n",
       "       [0.10011329, 0.56730701, 0.80983375, 0.74956209, 0.62388235,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0,1,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59855765, 0.40052354, 0.1826923 , 0.13089006, 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3][6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43382327, 0.56730701, 0.16754375, 0.15948865, 0.67105365,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756],\n",
       "       [0.43382327, 0.56730701, 0.38975628, 0.08609491, 0.62348527,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756],\n",
       "       [0.43382326, 0.56730701, 0.8492662 , 0.63354522, 0.62390208,\n",
       "        0.67503756, 0.67503756, 0.67503756, 0.67503756, 0.67503756]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0,6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "## made on 16/7/2020 at 8:48 pm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "anchors = anchors_wrt_target\n",
    "\n",
    "def my_custom_loss(y_true, y_pred):\n",
    "    \n",
    "    def pre_loss(my_custom_loss, anchors):\n",
    "        \n",
    "        num_anchors = len(anchors)\n",
    "        num_classes = 5\n",
    "        ignore_thresh = 0.5\n",
    "        grid_size = [15., 15.]\n",
    "        grid_stride = 480. / grid_size[0]\n",
    "        batch_shape = y_pred.get_shape()\n",
    "        batch_size = batch_shape[0]\n",
    "    \n",
    "        scaled_anchors = anchors / grid_stride\n",
    "    \n",
    "        Lambda_Coord = 5.0\n",
    "        Lambda_no_obj = 0.5\n",
    "    \n",
    "        grid_x = np.arange(grid_size[1])\n",
    "        grid_y = np.arange(grid_size[0])\n",
    "    \n",
    "        a = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        b = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        c = np.array(np.meshgrid(grid_x, grid_y))\n",
    "        d = np.concatenate((a,b,c), axis = 0)\n",
    "        e = d.transpose(2, 1, 0)\n",
    "        grid_final = np.reshape(e,[1,15,15,3,2])\n",
    "    \n",
    "        tot_loss = tf.zeros(1, dtype='float32')\n",
    "\n",
    "        obj_mask = y_true[..., 4:5]\n",
    "\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_box_xy = K.sigmoid(y_pred[..., :2]) + grid_final  # this gives x & y in no. of cells. x & y w.r.t. target\n",
    "                                                            # image = (x & y in no. of cells) / no. of cells\n",
    "        pred_box_xy_wrt_target_image = (pred_box_xy * grid_stride) / 480.\n",
    "        true_box_xy_wrt_target_image = y_true[..., :2]\n",
    "\n",
    "        xy_arr = Lambda_Coord * K.square(true_box_xy_wrt_target_image - pred_box_xy_wrt_target_image)\n",
    "    \n",
    "        xy_loss = K.sum(xy_arr * obj_mask)\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_box_wdht = K.exp(y_pred[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "    \n",
    "        true_box_wdht = y_true[..., 2:4]\n",
    "    \n",
    "        wh_arr = Lambda_Coord * K.square(true_box_wdht - pred_box_wdht)\n",
    "    \n",
    "        wh_loss = K.sum(wh_arr * obj_mask)\n",
    "    \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \n",
    "\n",
    "        pred_obj_mask = K.cast(K.sigmoid(y_pred[..., 4:5]), dtype = 'float32')  # shape = 28, 15, 15, 3, 1\n",
    "       \n",
    "        true_box_wrt_ti = K.concatenate([true_box_xy_wrt_target_image, true_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "        pred_box_wrt_ti = K.concatenate([pred_box_xy_wrt_target_image, pred_box_wdht], axis = -1)  ## in x,y,w,h format\n",
    "    \n",
    "        ignore_mask = calc_ignore_mask(ignore_thresh, true_box_wrt_ti, pred_box_wrt_ti)\n",
    "        \n",
    "#        bce = tf.keras.losses.BinaryCrossentropy()        \n",
    "#        obj_loss = K.sum(bce(obj_mask, pred_obj_mask) * obj_mask)\n",
    "\n",
    "        obj_loss_arr = K.square(obj_mask - pred_obj_mask)\n",
    "        obj_loss = K.sum(obj_loss_arr * obj_mask)\n",
    "    \n",
    "        no_obj_mask = 1. - obj_mask\n",
    "                \n",
    "        noobj_loss_arr = Lambda_no_obj * K.square(obj_mask - pred_obj_mask)\n",
    "        noobj_loss = K.sum(noobj_loss_arr * no_obj_mask * ignore_mask)\n",
    "        \n",
    "        \n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        true_classes = K.cast(y_true[..., 5:10], dtype = 'float32')\n",
    "    \n",
    "        pred_classes = K.cast(K.sigmoid(y_pred[..., 5:10]), dtype = 'float32')\n",
    "        \n",
    "#        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "#        class_loss = K.sum(bce(true_classes, pred_classes) * obj_mask)\n",
    "\n",
    "        class_loss_arr = K.square(true_classes - pred_classes)\n",
    "        class_loss = K.sum(class_loss_arr * obj_mask)\n",
    "\n",
    "        tot_loss = xy_loss + wh_loss + obj_loss + noobj_loss + class_loss\n",
    "        \n",
    "        return tot_loss\n",
    "    \n",
    "    loss = K.cast(pre_loss(my_custom_loss, anchors), dtype = 'float32')\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "my_model_2.compile(optimizer= opt, loss = my_yolo_loss, metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 380s 14s/step - loss: 1153.0500 - accuracy: 0.0619 - val_loss: 937.5840 - val_accuracy: 0.2835\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 139s 5s/step - loss: 976.0923 - accuracy: 0.0782 - val_loss: 935.9561 - val_accuracy: 0.2767\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 92s 3s/step - loss: 790.3050 - accuracy: 0.0877 - val_loss: 924.1128 - val_accuracy: 0.2844\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 91s 3s/step - loss: 668.5525 - accuracy: 0.0969 - val_loss: 910.4016 - val_accuracy: 0.0881\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 92s 3s/step - loss: 552.3350 - accuracy: 0.1041 - val_loss: 880.7205 - val_accuracy: 0.0767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa486dab080>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 5, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 480, 480, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv2D)                 (None, 480, 480, 32) 864         input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_0 (BatchNormalization)    (None, 480, 480, 32) 128         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_0 (LeakyReLU)             (None, 480, 480, 32) 0           bnorm_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_101 (ZeroPadding (None, 481, 481, 32) 0           leaky_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 240, 240, 64) 18432       zero_padding2d_101[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_1 (BatchNormalization)    (None, 240, 240, 64) 256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_1 (LeakyReLU)             (None, 240, 240, 64) 0           bnorm_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 240, 240, 32) 2048        leaky_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_2 (BatchNormalization)    (None, 240, 240, 32) 128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_2 (LeakyReLU)             (None, 240, 240, 32) 0           bnorm_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 240, 240, 64) 18432       leaky_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_3 (BatchNormalization)    (None, 240, 240, 64) 256         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_3 (LeakyReLU)             (None, 240, 240, 64) 0           bnorm_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_461 (Add)                   (None, 240, 240, 64) 0           leaky_1[0][0]                    \n",
      "                                                                 leaky_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_102 (ZeroPadding (None, 241, 241, 64) 0           add_461[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 120, 120, 128 73728       zero_padding2d_102[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_5 (BatchNormalization)    (None, 120, 120, 128 512         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_5 (LeakyReLU)             (None, 120, 120, 128 0           bnorm_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 120, 120, 64) 8192        leaky_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_6 (BatchNormalization)    (None, 120, 120, 64) 256         conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_6 (LeakyReLU)             (None, 120, 120, 64) 0           bnorm_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, 120, 120, 128 73728       leaky_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_7 (BatchNormalization)    (None, 120, 120, 128 512         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_7 (LeakyReLU)             (None, 120, 120, 128 0           bnorm_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_462 (Add)                   (None, 120, 120, 128 0           leaky_5[0][0]                    \n",
      "                                                                 leaky_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, 120, 120, 64) 8192        add_462[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_9 (BatchNormalization)    (None, 120, 120, 64) 256         conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_9 (LeakyReLU)             (None, 120, 120, 64) 0           bnorm_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, 120, 120, 128 73728       leaky_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_10 (BatchNormalization)   (None, 120, 120, 128 512         conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_10 (LeakyReLU)            (None, 120, 120, 128 0           bnorm_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_463 (Add)                   (None, 120, 120, 128 0           add_462[0][0]                    \n",
      "                                                                 leaky_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_103 (ZeroPadding (None, 121, 121, 128 0           add_463[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_12 (Conv2D)                (None, 60, 60, 256)  294912      zero_padding2d_103[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_12 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_12 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_13 (Conv2D)                (None, 60, 60, 128)  32768       leaky_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_13 (BatchNormalization)   (None, 60, 60, 128)  512         conv_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_13 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_14 (Conv2D)                (None, 60, 60, 256)  294912      leaky_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_14 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_14 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_464 (Add)                   (None, 60, 60, 256)  0           leaky_12[0][0]                   \n",
      "                                                                 leaky_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 60, 60, 128)  32768       add_464[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_16 (BatchNormalization)   (None, 60, 60, 128)  512         conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_16 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 60, 60, 256)  294912      leaky_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_17 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_17 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_465 (Add)                   (None, 60, 60, 256)  0           add_464[0][0]                    \n",
      "                                                                 leaky_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 60, 60, 128)  32768       add_465[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_19 (BatchNormalization)   (None, 60, 60, 128)  512         conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_19 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 60, 60, 256)  294912      leaky_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_20 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_20 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_466 (Add)                   (None, 60, 60, 256)  0           add_465[0][0]                    \n",
      "                                                                 leaky_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 60, 60, 128)  32768       add_466[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_22 (BatchNormalization)   (None, 60, 60, 128)  512         conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_22 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 60, 60, 256)  294912      leaky_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_23 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_23 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_467 (Add)                   (None, 60, 60, 256)  0           add_466[0][0]                    \n",
      "                                                                 leaky_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_25 (Conv2D)                (None, 60, 60, 128)  32768       add_467[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_25 (BatchNormalization)   (None, 60, 60, 128)  512         conv_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_25 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_26 (Conv2D)                (None, 60, 60, 256)  294912      leaky_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_26 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_26 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_468 (Add)                   (None, 60, 60, 256)  0           add_467[0][0]                    \n",
      "                                                                 leaky_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_28 (Conv2D)                (None, 60, 60, 128)  32768       add_468[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_28 (BatchNormalization)   (None, 60, 60, 128)  512         conv_28[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_28 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_29 (Conv2D)                (None, 60, 60, 256)  294912      leaky_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_29 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_29 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_469 (Add)                   (None, 60, 60, 256)  0           add_468[0][0]                    \n",
      "                                                                 leaky_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_31 (Conv2D)                (None, 60, 60, 128)  32768       add_469[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_31 (BatchNormalization)   (None, 60, 60, 128)  512         conv_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_31 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_32 (Conv2D)                (None, 60, 60, 256)  294912      leaky_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_32 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_32 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_470 (Add)                   (None, 60, 60, 256)  0           add_469[0][0]                    \n",
      "                                                                 leaky_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_34 (Conv2D)                (None, 60, 60, 128)  32768       add_470[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_34 (BatchNormalization)   (None, 60, 60, 128)  512         conv_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_34 (LeakyReLU)            (None, 60, 60, 128)  0           bnorm_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_35 (Conv2D)                (None, 60, 60, 256)  294912      leaky_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_35 (BatchNormalization)   (None, 60, 60, 256)  1024        conv_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_35 (LeakyReLU)            (None, 60, 60, 256)  0           bnorm_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_471 (Add)                   (None, 60, 60, 256)  0           add_470[0][0]                    \n",
      "                                                                 leaky_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_104 (ZeroPadding (None, 61, 61, 256)  0           add_471[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_37 (Conv2D)                (None, 30, 30, 512)  1179648     zero_padding2d_104[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_37 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_37 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_38 (Conv2D)                (None, 30, 30, 256)  131072      leaky_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_38 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_38 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_39 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_39 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_39 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_472 (Add)                   (None, 30, 30, 512)  0           leaky_37[0][0]                   \n",
      "                                                                 leaky_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_41 (Conv2D)                (None, 30, 30, 256)  131072      add_472[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_41 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_41 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_42 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_42 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_42 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_473 (Add)                   (None, 30, 30, 512)  0           add_472[0][0]                    \n",
      "                                                                 leaky_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_44 (Conv2D)                (None, 30, 30, 256)  131072      add_473[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_44 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_44 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_45 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_45 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_45[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_45 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_474 (Add)                   (None, 30, 30, 512)  0           add_473[0][0]                    \n",
      "                                                                 leaky_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_47 (Conv2D)                (None, 30, 30, 256)  131072      add_474[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_47 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_47[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_47 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_48 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_48 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_48[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_48 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_475 (Add)                   (None, 30, 30, 512)  0           add_474[0][0]                    \n",
      "                                                                 leaky_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_50 (Conv2D)                (None, 30, 30, 256)  131072      add_475[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_50 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_50[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_50 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_51 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_51 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_51[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_51 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_476 (Add)                   (None, 30, 30, 512)  0           add_475[0][0]                    \n",
      "                                                                 leaky_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_53 (Conv2D)                (None, 30, 30, 256)  131072      add_476[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_53 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_53[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_53 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_54 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_54 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_54[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_54 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_477 (Add)                   (None, 30, 30, 512)  0           add_476[0][0]                    \n",
      "                                                                 leaky_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_56 (Conv2D)                (None, 30, 30, 256)  131072      add_477[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_56 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_56[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_56 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_57 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_57 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_57[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_57 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_478 (Add)                   (None, 30, 30, 512)  0           add_477[0][0]                    \n",
      "                                                                 leaky_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_59 (Conv2D)                (None, 30, 30, 256)  131072      add_478[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_59 (BatchNormalization)   (None, 30, 30, 256)  1024        conv_59[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_59 (LeakyReLU)            (None, 30, 30, 256)  0           bnorm_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_60 (Conv2D)                (None, 30, 30, 512)  1179648     leaky_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_60 (BatchNormalization)   (None, 30, 30, 512)  2048        conv_60[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_60 (LeakyReLU)            (None, 30, 30, 512)  0           bnorm_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_479 (Add)                   (None, 30, 30, 512)  0           add_478[0][0]                    \n",
      "                                                                 leaky_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_105 (ZeroPadding (None, 31, 31, 512)  0           add_479[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_62 (Conv2D)                (None, 15, 15, 256)  1179648     zero_padding2d_105[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_62 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_62[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_62 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_63 (Conv2D)                (None, 15, 15, 512)  131072      leaky_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_63 (BatchNormalization)   (None, 15, 15, 512)  2048        conv_63[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_63 (LeakyReLU)            (None, 15, 15, 512)  0           bnorm_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_64 (Conv2D)                (None, 15, 15, 256)  1179648     leaky_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_64 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_64 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_480 (Add)                   (None, 15, 15, 256)  0           leaky_62[0][0]                   \n",
      "                                                                 leaky_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_66 (Conv2D)                (None, 15, 15, 128)  32768       add_480[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_66 (BatchNormalization)   (None, 15, 15, 128)  512         conv_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_66 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_67 (Conv2D)                (None, 15, 15, 256)  294912      leaky_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_67 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_67[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_67 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_481 (Add)                   (None, 15, 15, 256)  0           add_480[0][0]                    \n",
      "                                                                 leaky_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_69 (Conv2D)                (None, 15, 15, 128)  32768       add_481[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_69 (BatchNormalization)   (None, 15, 15, 128)  512         conv_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_69 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_70 (Conv2D)                (None, 15, 15, 256)  294912      leaky_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_70 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_70[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_70 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_482 (Add)                   (None, 15, 15, 256)  0           add_481[0][0]                    \n",
      "                                                                 leaky_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_72 (Conv2D)                (None, 15, 15, 128)  32768       add_482[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_72 (BatchNormalization)   (None, 15, 15, 128)  512         conv_72[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_72 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_73 (Conv2D)                (None, 15, 15, 256)  294912      leaky_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_73 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_73[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_73 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_483 (Add)                   (None, 15, 15, 256)  0           add_482[0][0]                    \n",
      "                                                                 leaky_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_75 (Conv2D)                (None, 15, 15, 128)  32768       add_483[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_75 (BatchNormalization)   (None, 15, 15, 128)  512         conv_75[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_75 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_76 (Conv2D)                (None, 15, 15, 256)  294912      leaky_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_76 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_76[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_76 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_77 (Conv2D)                (None, 15, 15, 128)  32768       leaky_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_77 (BatchNormalization)   (None, 15, 15, 128)  512         conv_77[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_77 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_78 (Conv2D)                (None, 15, 15, 256)  294912      leaky_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_78 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_78[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_78 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_79 (Conv2D)                (None, 15, 15, 128)  32768       leaky_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_79 (BatchNormalization)   (None, 15, 15, 128)  512         conv_79[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_79 (LeakyReLU)            (None, 15, 15, 128)  0           bnorm_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_80 (Conv2D)                (None, 15, 15, 256)  294912      leaky_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bnorm_80 (BatchNormalization)   (None, 15, 15, 256)  1024        conv_80[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_80 (LeakyReLU)            (None, 15, 15, 256)  0           bnorm_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_81 (Conv2D)                (None, 15, 15, 30)   7710        leaky_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 15, 15, 3, 10 0           conv_81[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,379,326\n",
      "Trainable params: 19,351,294\n",
      "Non-trainable params: 28,032\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_size = (target_w, target_h, 3)\n",
    "my_yolo_invoice_model = make_yolov3_model()\n",
    "\n",
    "\n",
    "my_model_1 = make_yolov3_model()\n",
    "my_model_2 = make_yolov3_model()\n",
    "\n",
    "print(my_model_1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "my_model_2.compile(optimizer= opt, loss = my_custom_loss, metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/5\n",
      "28/28 [==============================] - 318s 11s/step - loss: 1201.3670 - accuracy: 0.2725 - val_loss: 392.2430 - val_accuracy: 0.4102\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 104s 4s/step - loss: 481.2516 - accuracy: 0.2820 - val_loss: 388.7625 - val_accuracy: 0.3335\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 96s 3s/step - loss: 455.7300 - accuracy: 0.2858 - val_loss: 383.4540 - val_accuracy: 0.3150\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 97s 3s/step - loss: 443.7589 - accuracy: 0.2959 - val_loss: 376.6940 - val_accuracy: 0.3172\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 100s 4s/step - loss: 424.5925 - accuracy: 0.3052 - val_loss: 369.1475 - val_accuracy: 0.3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa47d019518>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 5, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "28/28 [==============================] - 93s 3s/step - loss: 409.0058 - accuracy: 0.3077 - val_loss: 360.1932 - val_accuracy: 0.3306\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 388.7526 - accuracy: 0.3212 - val_loss: 350.9710 - val_accuracy: 0.3320\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 98s 3s/step - loss: 373.9669 - accuracy: 0.3312 - val_loss: 341.0386 - val_accuracy: 0.3320\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 356.0425 - accuracy: 0.3316 - val_loss: 330.7975 - val_accuracy: 0.3320\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 342.2878 - accuracy: 0.3438 - val_loss: 320.8164 - val_accuracy: 0.3320\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 325.8933 - accuracy: 0.3429 - val_loss: 310.5037 - val_accuracy: 0.3320\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 312.0668 - accuracy: 0.3512 - val_loss: 300.5641 - val_accuracy: 0.3320\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 302.0387 - accuracy: 0.3593 - val_loss: 290.7509 - val_accuracy: 0.3320\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 287.1370 - accuracy: 0.3639 - val_loss: 280.9113 - val_accuracy: 0.3320\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 274.9065 - accuracy: 0.3599 - val_loss: 270.7860 - val_accuracy: 0.3317\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 264.9527 - accuracy: 0.3656 - val_loss: 261.1107 - val_accuracy: 0.3309\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 251.9358 - accuracy: 0.3682 - val_loss: 251.5772 - val_accuracy: 0.3307\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 243.2165 - accuracy: 0.3795 - val_loss: 242.4114 - val_accuracy: 0.3306\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 233.1520 - accuracy: 0.3752 - val_loss: 233.4079 - val_accuracy: 0.3306\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 223.4976 - accuracy: 0.3787 - val_loss: 224.8582 - val_accuracy: 0.3306\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 215.6850 - accuracy: 0.3813 - val_loss: 216.6039 - val_accuracy: 0.3306\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 205.9515 - accuracy: 0.3792 - val_loss: 207.9951 - val_accuracy: 0.3306\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 203.0999 - accuracy: 0.3821 - val_loss: 199.9991 - val_accuracy: 0.3306\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 193.8206 - accuracy: 0.3867 - val_loss: 191.9352 - val_accuracy: 0.3306\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 186.2335 - accuracy: 0.3896 - val_loss: 184.4387 - val_accuracy: 0.3306\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 175.6369 - accuracy: 0.3816 - val_loss: 176.9880 - val_accuracy: 0.3306\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 176.1368 - accuracy: 0.3813 - val_loss: 170.5783 - val_accuracy: 0.3306\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 169.0686 - accuracy: 0.3809 - val_loss: 164.0227 - val_accuracy: 0.3306\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 161.5060 - accuracy: 0.3804 - val_loss: 157.6176 - val_accuracy: 0.3306\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 158.0904 - accuracy: 0.3798 - val_loss: 152.3145 - val_accuracy: 0.3306\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 152.6203 - accuracy: 0.3821 - val_loss: 146.1650 - val_accuracy: 0.3306\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 144.6152 - accuracy: 0.3815 - val_loss: 140.9740 - val_accuracy: 0.3306\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 141.7041 - accuracy: 0.3783 - val_loss: 136.5263 - val_accuracy: 0.3306\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 140.9873 - accuracy: 0.3812 - val_loss: 132.5189 - val_accuracy: 0.3306\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 140.7381 - accuracy: 0.3784 - val_loss: 128.7918 - val_accuracy: 0.3306\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 131.0969 - accuracy: 0.3766 - val_loss: 125.0425 - val_accuracy: 0.3306\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 131.4829 - accuracy: 0.3765 - val_loss: 122.0388 - val_accuracy: 0.3306\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 126.4535 - accuracy: 0.3745 - val_loss: 119.1501 - val_accuracy: 0.3306\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 121.6295 - accuracy: 0.3749 - val_loss: 116.5219 - val_accuracy: 0.3306\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 120.3309 - accuracy: 0.3730 - val_loss: 114.1140 - val_accuracy: 0.3306\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 119.0262 - accuracy: 0.3714 - val_loss: 111.7545 - val_accuracy: 0.3306\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 112.9945 - accuracy: 0.3685 - val_loss: 109.2390 - val_accuracy: 0.3306\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 111.7481 - accuracy: 0.3735 - val_loss: 107.3874 - val_accuracy: 0.3306\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 107.6554 - accuracy: 0.3692 - val_loss: 105.1895 - val_accuracy: 0.3306\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 109.5626 - accuracy: 0.3663 - val_loss: 103.6298 - val_accuracy: 0.3306\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 107.9324 - accuracy: 0.3627 - val_loss: 102.0561 - val_accuracy: 0.3306\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 103.9027 - accuracy: 0.3644 - val_loss: 100.8209 - val_accuracy: 0.3306\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 102.9423 - accuracy: 0.3623 - val_loss: 99.4676 - val_accuracy: 0.3306\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 104.1769 - accuracy: 0.3612 - val_loss: 98.2465 - val_accuracy: 0.3306\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 100.1061 - accuracy: 0.3619 - val_loss: 96.8762 - val_accuracy: 0.3306\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 96.2234 - accuracy: 0.3608 - val_loss: 95.3056 - val_accuracy: 0.3306\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 96.5160 - accuracy: 0.3599 - val_loss: 94.0605 - val_accuracy: 0.3306\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 96.8293 - accuracy: 0.3577 - val_loss: 92.6409 - val_accuracy: 0.3306\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 93.3361 - accuracy: 0.3572 - val_loss: 91.2435 - val_accuracy: 0.3306\n",
      "Epoch 50/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 92.8870 - accuracy: 0.3560 - val_loss: 89.8140 - val_accuracy: 0.3306\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 94.5351 - accuracy: 0.3556 - val_loss: 89.1852 - val_accuracy: 0.3306\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 91.6697 - accuracy: 0.3532 - val_loss: 89.0485 - val_accuracy: 0.3309\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 88.6023 - accuracy: 0.3525 - val_loss: 88.1233 - val_accuracy: 0.3307\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 88.6226 - accuracy: 0.3522 - val_loss: 87.1813 - val_accuracy: 0.3307\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 86.8196 - accuracy: 0.3513 - val_loss: 86.0718 - val_accuracy: 0.3313\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - 97s 3s/step - loss: 86.3756 - accuracy: 0.3505 - val_loss: 85.4592 - val_accuracy: 0.3311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 86.9555 - accuracy: 0.3502 - val_loss: 85.0596 - val_accuracy: 0.3313\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 87.6438 - accuracy: 0.3498 - val_loss: 84.9869 - val_accuracy: 0.3313\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 85.5595 - accuracy: 0.3469 - val_loss: 84.6956 - val_accuracy: 0.3311\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 82.3567 - accuracy: 0.3494 - val_loss: 84.4618 - val_accuracy: 0.3313\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 82.8682 - accuracy: 0.3455 - val_loss: 83.8605 - val_accuracy: 0.3315\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - 100s 4s/step - loss: 83.1325 - accuracy: 0.3449 - val_loss: 82.9691 - val_accuracy: 0.3313\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - 108s 4s/step - loss: 80.5971 - accuracy: 0.3442 - val_loss: 81.9000 - val_accuracy: 0.3313\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 80.5663 - accuracy: 0.3426 - val_loss: 81.3718 - val_accuracy: 0.3319\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 81.8457 - accuracy: 0.3439 - val_loss: 80.9853 - val_accuracy: 0.3309\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 79.1300 - accuracy: 0.3415 - val_loss: 80.0954 - val_accuracy: 0.3309\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 78.9708 - accuracy: 0.3419 - val_loss: 79.4047 - val_accuracy: 0.3319\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 77.6405 - accuracy: 0.3420 - val_loss: 78.5908 - val_accuracy: 0.3313\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 78.1773 - accuracy: 0.3410 - val_loss: 78.2111 - val_accuracy: 0.3313\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 77.4232 - accuracy: 0.3424 - val_loss: 78.1811 - val_accuracy: 0.3315\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 74.5900 - accuracy: 0.3404 - val_loss: 77.6565 - val_accuracy: 0.3313\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 81.2511 - accuracy: 0.3416 - val_loss: 77.6580 - val_accuracy: 0.3320\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 75.4268 - accuracy: 0.3389 - val_loss: 77.7738 - val_accuracy: 0.3324\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 73.0048 - accuracy: 0.3407 - val_loss: 77.5735 - val_accuracy: 0.3319\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 74.9614 - accuracy: 0.3407 - val_loss: 77.1446 - val_accuracy: 0.3324\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - 96s 3s/step - loss: 72.9275 - accuracy: 0.3388 - val_loss: 76.7832 - val_accuracy: 0.3330\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 72.8749 - accuracy: 0.3394 - val_loss: 76.4824 - val_accuracy: 0.3337\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 71.8194 - accuracy: 0.3397 - val_loss: 76.2859 - val_accuracy: 0.3344\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 73.2270 - accuracy: 0.3381 - val_loss: 75.9849 - val_accuracy: 0.3335\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 71.8124 - accuracy: 0.3383 - val_loss: 75.6372 - val_accuracy: 0.3330\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 73.4570 - accuracy: 0.3381 - val_loss: 75.4279 - val_accuracy: 0.3331\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 71.8173 - accuracy: 0.3381 - val_loss: 75.9329 - val_accuracy: 0.3333\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 71.4522 - accuracy: 0.3368 - val_loss: 75.8637 - val_accuracy: 0.3343\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 71.3407 - accuracy: 0.3363 - val_loss: 75.4234 - val_accuracy: 0.3339\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 69.3292 - accuracy: 0.3378 - val_loss: 75.8124 - val_accuracy: 0.3354\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 68.1206 - accuracy: 0.3367 - val_loss: 75.5000 - val_accuracy: 0.3350\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - 106s 4s/step - loss: 68.1697 - accuracy: 0.3358 - val_loss: 75.0795 - val_accuracy: 0.3343\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - 98s 4s/step - loss: 68.2522 - accuracy: 0.3364 - val_loss: 74.5024 - val_accuracy: 0.3337\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 66.9636 - accuracy: 0.3364 - val_loss: 73.0382 - val_accuracy: 0.3343\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 66.7285 - accuracy: 0.3352 - val_loss: 72.0603 - val_accuracy: 0.3344\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 67.3572 - accuracy: 0.3359 - val_loss: 71.1443 - val_accuracy: 0.3350\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - 104s 4s/step - loss: 64.9426 - accuracy: 0.3360 - val_loss: 70.8516 - val_accuracy: 0.3350\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - 99s 4s/step - loss: 67.5355 - accuracy: 0.3355 - val_loss: 70.6701 - val_accuracy: 0.3344\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 63.9859 - accuracy: 0.3346 - val_loss: 70.3450 - val_accuracy: 0.3346\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 66.8215 - accuracy: 0.3352 - val_loss: 69.9057 - val_accuracy: 0.3346\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - 105s 4s/step - loss: 65.2695 - accuracy: 0.3347 - val_loss: 69.3585 - val_accuracy: 0.3343\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - 102s 4s/step - loss: 64.3798 - accuracy: 0.3349 - val_loss: 68.8388 - val_accuracy: 0.3337\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 63.7332 - accuracy: 0.3349 - val_loss: 68.4903 - val_accuracy: 0.3339\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - 103s 4s/step - loss: 64.7252 - accuracy: 0.3337 - val_loss: 68.2795 - val_accuracy: 0.3337\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - 101s 4s/step - loss: 64.5475 - accuracy: 0.3342 - val_loss: 68.0645 - val_accuracy: 0.3341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa47d02cdd8>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 100, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 90s 3s/step - loss: 64.4096 - accuracy: 0.3345 - val_loss: 67.9865 - val_accuracy: 0.3339\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 94s 3s/step - loss: 65.5626 - accuracy: 0.3341 - val_loss: 67.9462 - val_accuracy: 0.3343\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 94s 3s/step - loss: 62.1514 - accuracy: 0.3340 - val_loss: 67.8561 - val_accuracy: 0.3346\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 63.2877 - accuracy: 0.3342 - val_loss: 67.5814 - val_accuracy: 0.3344\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 64.0227 - accuracy: 0.3338 - val_loss: 67.4809 - val_accuracy: 0.3339\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 62.8490 - accuracy: 0.3339 - val_loss: 67.5425 - val_accuracy: 0.3341\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 62.9015 - accuracy: 0.3332 - val_loss: 67.5880 - val_accuracy: 0.3341\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 62.2485 - accuracy: 0.3327 - val_loss: 67.6800 - val_accuracy: 0.3344\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 60.5010 - accuracy: 0.3331 - val_loss: 67.7192 - val_accuracy: 0.3346\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 61.2100 - accuracy: 0.3339 - val_loss: 67.6240 - val_accuracy: 0.3348\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 60.7557 - accuracy: 0.3332 - val_loss: 67.3729 - val_accuracy: 0.3348\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 59.7494 - accuracy: 0.3333 - val_loss: 67.1533 - val_accuracy: 0.3339\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 59.3145 - accuracy: 0.3332 - val_loss: 66.9133 - val_accuracy: 0.3335\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 60.3499 - accuracy: 0.3338 - val_loss: 66.7011 - val_accuracy: 0.3331\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 57.5911 - accuracy: 0.3330 - val_loss: 66.4066 - val_accuracy: 0.3335\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 58.0619 - accuracy: 0.3332 - val_loss: 66.1088 - val_accuracy: 0.3337\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 107s 4s/step - loss: 61.8990 - accuracy: 0.3330 - val_loss: 65.8026 - val_accuracy: 0.3333\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 60.0166 - accuracy: 0.3329 - val_loss: 65.8055 - val_accuracy: 0.3335\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 59.9875 - accuracy: 0.3328 - val_loss: 66.1190 - val_accuracy: 0.3337\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 58.3102 - accuracy: 0.3324 - val_loss: 66.0346 - val_accuracy: 0.3335\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 59.4444 - accuracy: 0.3330 - val_loss: 65.8331 - val_accuracy: 0.3337\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 58.6679 - accuracy: 0.3333 - val_loss: 65.4089 - val_accuracy: 0.3330\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 57.8392 - accuracy: 0.3331 - val_loss: 65.0753 - val_accuracy: 0.3328\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 58.1178 - accuracy: 0.3328 - val_loss: 64.8791 - val_accuracy: 0.3331\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 107s 4s/step - loss: 57.9225 - accuracy: 0.3331 - val_loss: 64.7218 - val_accuracy: 0.3331\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 58.6853 - accuracy: 0.3326 - val_loss: 64.6269 - val_accuracy: 0.3330\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 57.3617 - accuracy: 0.3325 - val_loss: 64.5239 - val_accuracy: 0.3328\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 59.3191 - accuracy: 0.3320 - val_loss: 64.4813 - val_accuracy: 0.3335\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 55.8663 - accuracy: 0.3322 - val_loss: 64.3705 - val_accuracy: 0.3339\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 111s 4s/step - loss: 56.9243 - accuracy: 0.3325 - val_loss: 64.1937 - val_accuracy: 0.3335\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 55.9074 - accuracy: 0.3334 - val_loss: 63.9798 - val_accuracy: 0.3331\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 59.1650 - accuracy: 0.3327 - val_loss: 63.8950 - val_accuracy: 0.3335\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 56.1613 - accuracy: 0.3327 - val_loss: 63.6929 - val_accuracy: 0.3343\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 55.2354 - accuracy: 0.3328 - val_loss: 63.4142 - val_accuracy: 0.3337\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 55.7710 - accuracy: 0.3324 - val_loss: 63.2016 - val_accuracy: 0.3339\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 55.7053 - accuracy: 0.3325 - val_loss: 63.0837 - val_accuracy: 0.3331\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 55.6513 - accuracy: 0.3322 - val_loss: 63.1402 - val_accuracy: 0.3337\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 107s 4s/step - loss: 56.8530 - accuracy: 0.3327 - val_loss: 62.9742 - val_accuracy: 0.3337\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 54.1840 - accuracy: 0.3321 - val_loss: 62.9372 - val_accuracy: 0.3333\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 57.5094 - accuracy: 0.3324 - val_loss: 63.0171 - val_accuracy: 0.3335\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 54.8911 - accuracy: 0.3323 - val_loss: 62.9897 - val_accuracy: 0.3337\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 53.9937 - accuracy: 0.3326 - val_loss: 62.8759 - val_accuracy: 0.3337\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 54.7168 - accuracy: 0.3319 - val_loss: 62.7112 - val_accuracy: 0.3339\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 107s 4s/step - loss: 54.0596 - accuracy: 0.3328 - val_loss: 62.4628 - val_accuracy: 0.3344\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 111s 4s/step - loss: 56.0273 - accuracy: 0.3324 - val_loss: 62.3859 - val_accuracy: 0.3343\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 53.7433 - accuracy: 0.3333 - val_loss: 62.3288 - val_accuracy: 0.3331\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 54.0453 - accuracy: 0.3326 - val_loss: 62.2112 - val_accuracy: 0.3331\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 54.4880 - accuracy: 0.3327 - val_loss: 62.0386 - val_accuracy: 0.3333\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 55.9006 - accuracy: 0.3332 - val_loss: 61.7024 - val_accuracy: 0.3335\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 53.6941 - accuracy: 0.3317 - val_loss: 61.3347 - val_accuracy: 0.3331\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 52.0539 - accuracy: 0.3321 - val_loss: 61.1694 - val_accuracy: 0.3324\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 55.0535 - accuracy: 0.3321 - val_loss: 61.3002 - val_accuracy: 0.3326\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 52.6161 - accuracy: 0.3320 - val_loss: 61.2284 - val_accuracy: 0.3326\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 52.2009 - accuracy: 0.3320 - val_loss: 61.2229 - val_accuracy: 0.3322\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 52.2693 - accuracy: 0.3316 - val_loss: 61.2463 - val_accuracy: 0.3322\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 112s 4s/step - loss: 55.4505 - accuracy: 0.3324 - val_loss: 61.4135 - val_accuracy: 0.3326\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 108s 4s/step - loss: 52.1025 - accuracy: 0.3322 - val_loss: 61.5644 - val_accuracy: 0.3322\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 107s 4s/step - loss: 53.2221 - accuracy: 0.3320 - val_loss: 61.5771 - val_accuracy: 0.3322\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 111s 4s/step - loss: 52.0022 - accuracy: 0.3317 - val_loss: 61.4037 - val_accuracy: 0.3322\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 54.3801 - accuracy: 0.3320 - val_loss: 61.4678 - val_accuracy: 0.3322\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 107s 4s/step - loss: 50.6507 - accuracy: 0.3325 - val_loss: 61.2524 - val_accuracy: 0.3324\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 111s 4s/step - loss: 52.1054 - accuracy: 0.3321 - val_loss: 61.0602 - val_accuracy: 0.3326\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 50.9652 - accuracy: 0.3321 - val_loss: 60.9876 - val_accuracy: 0.3328\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 108s 4s/step - loss: 50.6869 - accuracy: 0.3320 - val_loss: 60.9107 - val_accuracy: 0.3333\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 107s 4s/step - loss: 52.6703 - accuracy: 0.3326 - val_loss: 60.7263 - val_accuracy: 0.3331\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 110s 4s/step - loss: 50.6220 - accuracy: 0.3323 - val_loss: 60.7907 - val_accuracy: 0.3333\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 52.1512 - accuracy: 0.3328 - val_loss: 60.9472 - val_accuracy: 0.3333\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 109s 4s/step - loss: 50.3544 - accuracy: 0.3326 - val_loss: 61.1836 - val_accuracy: 0.3333\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 110s 4s/step - loss: 51.3204 - accuracy: 0.3322 - val_loss: 61.1134 - val_accuracy: 0.3333\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 50.6790 - accuracy: 0.3321 - val_loss: 61.2901 - val_accuracy: 0.3333\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 50.2106 - accuracy: 0.3327 - val_loss: 61.1433 - val_accuracy: 0.3333\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 50.0919 - accuracy: 0.3326 - val_loss: 60.8680 - val_accuracy: 0.3335\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 50.4352 - accuracy: 0.3327 - val_loss: 60.6697 - val_accuracy: 0.3330\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 51.0421 - accuracy: 0.3323 - val_loss: 60.6501 - val_accuracy: 0.3331\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 50.5219 - accuracy: 0.3325 - val_loss: 60.7786 - val_accuracy: 0.3343\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 49.5920 - accuracy: 0.3329 - val_loss: 60.8376 - val_accuracy: 0.3348\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 49.1052 - accuracy: 0.3326 - val_loss: 60.7913 - val_accuracy: 0.3346\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 49.8666 - accuracy: 0.3325 - val_loss: 60.5191 - val_accuracy: 0.3344\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 48.2688 - accuracy: 0.3325 - val_loss: 60.3814 - val_accuracy: 0.3346\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 48.1497 - accuracy: 0.3329 - val_loss: 60.2732 - val_accuracy: 0.3344\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 49.3807 - accuracy: 0.3330 - val_loss: 60.0661 - val_accuracy: 0.3337\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 50.8799 - accuracy: 0.3326 - val_loss: 59.8750 - val_accuracy: 0.3344\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 51.6207 - accuracy: 0.3323 - val_loss: 59.5990 - val_accuracy: 0.3346\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 49.9761 - accuracy: 0.3330 - val_loss: 59.4140 - val_accuracy: 0.3348\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 49.6014 - accuracy: 0.3329 - val_loss: 59.1435 - val_accuracy: 0.3350\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 49.9671 - accuracy: 0.3325 - val_loss: 59.8124 - val_accuracy: 0.3352\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 49.3374 - accuracy: 0.3324 - val_loss: 59.8273 - val_accuracy: 0.3348\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 50.1804 - accuracy: 0.3326 - val_loss: 59.6090 - val_accuracy: 0.3348\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 50.0795 - accuracy: 0.3326 - val_loss: 59.4182 - val_accuracy: 0.3350\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 49.7665 - accuracy: 0.3322 - val_loss: 59.2920 - val_accuracy: 0.3346\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 49.0536 - accuracy: 0.3331 - val_loss: 59.2801 - val_accuracy: 0.3348\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 49.3556 - accuracy: 0.3324 - val_loss: 59.2845 - val_accuracy: 0.3343\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 49.1714 - accuracy: 0.3330 - val_loss: 59.5090 - val_accuracy: 0.3350\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 47.1632 - accuracy: 0.3331 - val_loss: 59.8614 - val_accuracy: 0.3359\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 47.6016 - accuracy: 0.3336 - val_loss: 60.1489 - val_accuracy: 0.3357\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 48.4843 - accuracy: 0.3322 - val_loss: 60.2895 - val_accuracy: 0.3359\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 48.0167 - accuracy: 0.3330 - val_loss: 60.9387 - val_accuracy: 0.3357\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 48.6973 - accuracy: 0.3326 - val_loss: 61.7036 - val_accuracy: 0.3363\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 48.3710 - accuracy: 0.3330 - val_loss: 63.6057 - val_accuracy: 0.3361\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 47.3847 - accuracy: 0.3331 - val_loss: 62.3725 - val_accuracy: 0.3356\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 47.6941 - accuracy: 0.3326 - val_loss: 61.2222 - val_accuracy: 0.3350\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 110s 4s/step - loss: 48.0071 - accuracy: 0.3323 - val_loss: 61.0430 - val_accuracy: 0.3346\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 116s 4s/step - loss: 47.3204 - accuracy: 0.3332 - val_loss: 60.2740 - val_accuracy: 0.3350\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 47.0634 - accuracy: 0.3335 - val_loss: 59.3252 - val_accuracy: 0.3352\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 46.8085 - accuracy: 0.3335 - val_loss: 58.9068 - val_accuracy: 0.3354\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 47.3213 - accuracy: 0.3332 - val_loss: 58.7105 - val_accuracy: 0.3350\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 48.6203 - accuracy: 0.3331 - val_loss: 58.9488 - val_accuracy: 0.3348\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 47.7339 - accuracy: 0.3335 - val_loss: 59.0530 - val_accuracy: 0.3350\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 49.5611 - accuracy: 0.3330 - val_loss: 59.1104 - val_accuracy: 0.3357\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 46.6713 - accuracy: 0.3335 - val_loss: 58.9242 - val_accuracy: 0.3354\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 46.5633 - accuracy: 0.3330 - val_loss: 58.8823 - val_accuracy: 0.3357\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 45.6806 - accuracy: 0.3336 - val_loss: 59.0665 - val_accuracy: 0.3350\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 99s 4s/step - loss: 46.5712 - accuracy: 0.3341 - val_loss: 59.0673 - val_accuracy: 0.3356\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 47.2225 - accuracy: 0.3331 - val_loss: 58.5310 - val_accuracy: 0.3359\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 47.1449 - accuracy: 0.3335 - val_loss: 58.1480 - val_accuracy: 0.3365\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 47.8747 - accuracy: 0.3337 - val_loss: 57.8837 - val_accuracy: 0.3354\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 45.5246 - accuracy: 0.3330 - val_loss: 57.8751 - val_accuracy: 0.3350\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 46.6463 - accuracy: 0.3334 - val_loss: 58.2938 - val_accuracy: 0.3344\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 47.0984 - accuracy: 0.3332 - val_loss: 58.8350 - val_accuracy: 0.3350\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 46.3351 - accuracy: 0.3340 - val_loss: 59.4398 - val_accuracy: 0.3350\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 47.0128 - accuracy: 0.3331 - val_loss: 59.5898 - val_accuracy: 0.3346\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 107s 4s/step - loss: 47.0471 - accuracy: 0.3338 - val_loss: 59.5848 - val_accuracy: 0.3350\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 47.2694 - accuracy: 0.3344 - val_loss: 59.6102 - val_accuracy: 0.3354\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 47.1814 - accuracy: 0.3337 - val_loss: 59.1664 - val_accuracy: 0.3354\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 45.8287 - accuracy: 0.3335 - val_loss: 58.7871 - val_accuracy: 0.3348\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 45.9500 - accuracy: 0.3341 - val_loss: 58.5482 - val_accuracy: 0.3348\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 48.5153 - accuracy: 0.3334 - val_loss: 58.2364 - val_accuracy: 0.3333\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 45.7510 - accuracy: 0.3333 - val_loss: 57.9162 - val_accuracy: 0.3330\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 46.7059 - accuracy: 0.3334 - val_loss: 57.6603 - val_accuracy: 0.3330\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 46.8024 - accuracy: 0.3334 - val_loss: 57.6784 - val_accuracy: 0.3335\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 47.2629 - accuracy: 0.3330 - val_loss: 57.8396 - val_accuracy: 0.3333\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 46.3883 - accuracy: 0.3334 - val_loss: 58.0559 - val_accuracy: 0.3337\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 46.3297 - accuracy: 0.3346 - val_loss: 58.0989 - val_accuracy: 0.3343\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 45.8134 - accuracy: 0.3342 - val_loss: 58.0160 - val_accuracy: 0.3346\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 46.2331 - accuracy: 0.3341 - val_loss: 58.1514 - val_accuracy: 0.3350\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 45.3310 - accuracy: 0.3342 - val_loss: 58.1753 - val_accuracy: 0.3344\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 45.7569 - accuracy: 0.3340 - val_loss: 58.1139 - val_accuracy: 0.3350\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 45.4277 - accuracy: 0.3335 - val_loss: 58.0997 - val_accuracy: 0.3350\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 46.0431 - accuracy: 0.3339 - val_loss: 58.1843 - val_accuracy: 0.3352\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 44.6381 - accuracy: 0.3339 - val_loss: 57.7312 - val_accuracy: 0.3352\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 45.3356 - accuracy: 0.3339 - val_loss: 57.7243 - val_accuracy: 0.3350\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 45.2563 - accuracy: 0.3344 - val_loss: 57.5829 - val_accuracy: 0.3346\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 45.8561 - accuracy: 0.3348 - val_loss: 57.7711 - val_accuracy: 0.3348\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 44.5651 - accuracy: 0.3335 - val_loss: 57.7839 - val_accuracy: 0.3357\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 45.8033 - accuracy: 0.3348 - val_loss: 57.4532 - val_accuracy: 0.3359\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 44.9352 - accuracy: 0.3346 - val_loss: 56.8687 - val_accuracy: 0.3363\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 45.2812 - accuracy: 0.3342 - val_loss: 56.9628 - val_accuracy: 0.3354\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 44.1558 - accuracy: 0.3343 - val_loss: 56.9842 - val_accuracy: 0.3350\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 109s 4s/step - loss: 43.7587 - accuracy: 0.3343 - val_loss: 56.9748 - val_accuracy: 0.3348\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 45.6175 - accuracy: 0.3350 - val_loss: 56.9437 - val_accuracy: 0.3354\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 44.2902 - accuracy: 0.3350 - val_loss: 56.9280 - val_accuracy: 0.3359\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 44.2233 - accuracy: 0.3343 - val_loss: 56.9387 - val_accuracy: 0.3365\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 42.7158 - accuracy: 0.3350 - val_loss: 56.8119 - val_accuracy: 0.3363\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 44.6192 - accuracy: 0.3350 - val_loss: 56.5191 - val_accuracy: 0.3361\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 44.8384 - accuracy: 0.3352 - val_loss: 56.3611 - val_accuracy: 0.3359\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 43.4647 - accuracy: 0.3350 - val_loss: 56.1297 - val_accuracy: 0.3357\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 44.9083 - accuracy: 0.3351 - val_loss: 56.2006 - val_accuracy: 0.3363\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 43.5565 - accuracy: 0.3351 - val_loss: 56.0649 - val_accuracy: 0.3359\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 42.2144 - accuracy: 0.3352 - val_loss: 56.0495 - val_accuracy: 0.3354\n",
      "Epoch 160/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 42.7117 - accuracy: 0.3344 - val_loss: 55.9776 - val_accuracy: 0.3350\n",
      "Epoch 161/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 42.7952 - accuracy: 0.3365 - val_loss: 55.9479 - val_accuracy: 0.3352\n",
      "Epoch 162/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 42.1462 - accuracy: 0.3360 - val_loss: 55.7330 - val_accuracy: 0.3352\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 43.1285 - accuracy: 0.3354 - val_loss: 55.5772 - val_accuracy: 0.3357\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 43.5836 - accuracy: 0.3351 - val_loss: 55.5650 - val_accuracy: 0.3365\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 42.4295 - accuracy: 0.3358 - val_loss: 55.7123 - val_accuracy: 0.3374\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 43.9286 - accuracy: 0.3357 - val_loss: 55.8592 - val_accuracy: 0.3372\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 41.8864 - accuracy: 0.3350 - val_loss: 55.7359 - val_accuracy: 0.3374\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 41.8227 - accuracy: 0.3360 - val_loss: 55.7077 - val_accuracy: 0.3372\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 100s 4s/step - loss: 42.9269 - accuracy: 0.3366 - val_loss: 55.7064 - val_accuracy: 0.3365\n",
      "Epoch 170/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 43.0360 - accuracy: 0.3358 - val_loss: 55.8136 - val_accuracy: 0.3374\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 42.0951 - accuracy: 0.3353 - val_loss: 55.6282 - val_accuracy: 0.3381\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 41.2299 - accuracy: 0.3359 - val_loss: 55.5643 - val_accuracy: 0.3383\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 42.0055 - accuracy: 0.3363 - val_loss: 55.6160 - val_accuracy: 0.3378\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 42.4551 - accuracy: 0.3371 - val_loss: 55.2652 - val_accuracy: 0.3383\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 42.9260 - accuracy: 0.3353 - val_loss: 55.0444 - val_accuracy: 0.3383\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 42.1948 - accuracy: 0.3356 - val_loss: 54.8860 - val_accuracy: 0.3391\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 42.2387 - accuracy: 0.3364 - val_loss: 54.9110 - val_accuracy: 0.3389\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 42.4512 - accuracy: 0.3362 - val_loss: 55.1724 - val_accuracy: 0.3393\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 42.3143 - accuracy: 0.3360 - val_loss: 55.3402 - val_accuracy: 0.3394\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 42.4120 - accuracy: 0.3357 - val_loss: 55.3970 - val_accuracy: 0.3394\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 41.1401 - accuracy: 0.3361 - val_loss: 55.3208 - val_accuracy: 0.3393\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 42.2306 - accuracy: 0.3365 - val_loss: 55.3092 - val_accuracy: 0.3391\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 41.2724 - accuracy: 0.3367 - val_loss: 55.5143 - val_accuracy: 0.3389\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 41.6759 - accuracy: 0.3365 - val_loss: 56.2795 - val_accuracy: 0.3387\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 40.8894 - accuracy: 0.3368 - val_loss: 56.8524 - val_accuracy: 0.3394\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 42.6422 - accuracy: 0.3368 - val_loss: 57.4789 - val_accuracy: 0.3400\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 40.7922 - accuracy: 0.3372 - val_loss: 57.7933 - val_accuracy: 0.3394\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 41.2472 - accuracy: 0.3363 - val_loss: 57.5301 - val_accuracy: 0.3393\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 40.7120 - accuracy: 0.3370 - val_loss: 57.5036 - val_accuracy: 0.3393\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 41.4826 - accuracy: 0.3363 - val_loss: 57.4742 - val_accuracy: 0.3389\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 41.3523 - accuracy: 0.3377 - val_loss: 57.0980 - val_accuracy: 0.3394\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 40.8678 - accuracy: 0.3367 - val_loss: 56.9814 - val_accuracy: 0.3393\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 42.0244 - accuracy: 0.3365 - val_loss: 56.9944 - val_accuracy: 0.3394\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 109s 4s/step - loss: 41.6322 - accuracy: 0.3364 - val_loss: 58.1047 - val_accuracy: 0.3393\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 42.2466 - accuracy: 0.3377 - val_loss: 57.4883 - val_accuracy: 0.3391\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 40.7880 - accuracy: 0.3372 - val_loss: 57.1625 - val_accuracy: 0.3400\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 40.8327 - accuracy: 0.3371 - val_loss: 57.4600 - val_accuracy: 0.3402\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 42.0721 - accuracy: 0.3376 - val_loss: 57.4767 - val_accuracy: 0.3404\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 41.4674 - accuracy: 0.3381 - val_loss: 57.7104 - val_accuracy: 0.3409\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 40.3246 - accuracy: 0.3377 - val_loss: 57.1137 - val_accuracy: 0.3409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa486dc5a20>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 200, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28 samples, validate on 8 samples\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 89s 3s/step - loss: 41.1463 - accuracy: 0.3373 - val_loss: 56.6062 - val_accuracy: 0.3404\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 95s 3s/step - loss: 40.3220 - accuracy: 0.3383 - val_loss: 56.3051 - val_accuracy: 0.3406\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 40.6956 - accuracy: 0.3373 - val_loss: 56.1673 - val_accuracy: 0.3409\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 43.0726 - accuracy: 0.3374 - val_loss: 56.1546 - val_accuracy: 0.3402\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 40.1665 - accuracy: 0.3383 - val_loss: 56.1911 - val_accuracy: 0.3400\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 41.3749 - accuracy: 0.3378 - val_loss: 56.1147 - val_accuracy: 0.3407\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 40.3413 - accuracy: 0.3377 - val_loss: 56.2063 - val_accuracy: 0.3409\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 40.8812 - accuracy: 0.3378 - val_loss: 56.0267 - val_accuracy: 0.3407\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 41.2506 - accuracy: 0.3392 - val_loss: 55.8961 - val_accuracy: 0.3409\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 39.9117 - accuracy: 0.3368 - val_loss: 55.9086 - val_accuracy: 0.3407\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 44.5305 - accuracy: 0.3379 - val_loss: 57.2656 - val_accuracy: 0.3383\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 41.7563 - accuracy: 0.3366 - val_loss: 58.4322 - val_accuracy: 0.3363\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 42.9847 - accuracy: 0.3368 - val_loss: 57.9858 - val_accuracy: 0.3363\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 40.8113 - accuracy: 0.3361 - val_loss: 57.5417 - val_accuracy: 0.3367\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 41.7749 - accuracy: 0.3365 - val_loss: 57.2559 - val_accuracy: 0.3370\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 42.2580 - accuracy: 0.3365 - val_loss: 57.0246 - val_accuracy: 0.3374\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 41.0090 - accuracy: 0.3363 - val_loss: 56.7985 - val_accuracy: 0.3376\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 40.9564 - accuracy: 0.3372 - val_loss: 56.6856 - val_accuracy: 0.3372\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 42.1791 - accuracy: 0.3359 - val_loss: 56.0888 - val_accuracy: 0.3369\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 40.4932 - accuracy: 0.3367 - val_loss: 55.5514 - val_accuracy: 0.3369\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 41.3954 - accuracy: 0.3379 - val_loss: 55.2041 - val_accuracy: 0.3376\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 40.4798 - accuracy: 0.3385 - val_loss: 54.9691 - val_accuracy: 0.3376\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 39.3735 - accuracy: 0.3375 - val_loss: 54.7437 - val_accuracy: 0.3383\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 39.4588 - accuracy: 0.3371 - val_loss: 54.7594 - val_accuracy: 0.3391\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 40.5736 - accuracy: 0.3372 - val_loss: 54.5924 - val_accuracy: 0.3393\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 40.2582 - accuracy: 0.3386 - val_loss: 54.5236 - val_accuracy: 0.3398\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 40.4171 - accuracy: 0.3381 - val_loss: 54.0372 - val_accuracy: 0.3396\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 41.3469 - accuracy: 0.3383 - val_loss: 53.8077 - val_accuracy: 0.3407\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 40.0913 - accuracy: 0.3394 - val_loss: 53.7831 - val_accuracy: 0.3413\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 40.4332 - accuracy: 0.3385 - val_loss: 53.7261 - val_accuracy: 0.3420\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 39.6770 - accuracy: 0.3385 - val_loss: 53.5975 - val_accuracy: 0.3415\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 39.4578 - accuracy: 0.3395 - val_loss: 53.5076 - val_accuracy: 0.3411\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 39.0434 - accuracy: 0.3381 - val_loss: 53.4222 - val_accuracy: 0.3413\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 40.4505 - accuracy: 0.3396 - val_loss: 53.4748 - val_accuracy: 0.3409\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 39.2104 - accuracy: 0.3391 - val_loss: 53.6491 - val_accuracy: 0.3417\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 39.6321 - accuracy: 0.3373 - val_loss: 53.7249 - val_accuracy: 0.3419\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 39.9576 - accuracy: 0.3386 - val_loss: 53.7620 - val_accuracy: 0.3419\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 105s 4s/step - loss: 39.0389 - accuracy: 0.3392 - val_loss: 53.7058 - val_accuracy: 0.3420\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 38.0942 - accuracy: 0.3376 - val_loss: 53.8398 - val_accuracy: 0.3417\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 39.6888 - accuracy: 0.3392 - val_loss: 53.7608 - val_accuracy: 0.3420\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 38.3724 - accuracy: 0.3396 - val_loss: 53.6948 - val_accuracy: 0.3413\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 40.6303 - accuracy: 0.3386 - val_loss: 53.7785 - val_accuracy: 0.3428\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 40.6979 - accuracy: 0.3388 - val_loss: 53.9347 - val_accuracy: 0.3426\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 38.0652 - accuracy: 0.3387 - val_loss: 54.1423 - val_accuracy: 0.3428\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 40.1776 - accuracy: 0.3396 - val_loss: 54.4655 - val_accuracy: 0.3431\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 38.8896 - accuracy: 0.3404 - val_loss: 55.0935 - val_accuracy: 0.3430\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 40.1789 - accuracy: 0.3411 - val_loss: 55.2887 - val_accuracy: 0.3443\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 41.0214 - accuracy: 0.3404 - val_loss: 55.2081 - val_accuracy: 0.3446\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 40.3456 - accuracy: 0.3385 - val_loss: 55.1198 - val_accuracy: 0.3448\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 38.5461 - accuracy: 0.3405 - val_loss: 54.9998 - val_accuracy: 0.3439\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 106s 4s/step - loss: 38.8815 - accuracy: 0.3394 - val_loss: 55.0190 - val_accuracy: 0.3435\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 38.7951 - accuracy: 0.3405 - val_loss: 54.9383 - val_accuracy: 0.3435\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 39.3586 - accuracy: 0.3399 - val_loss: 54.8885 - val_accuracy: 0.3428\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.5530 - accuracy: 0.3412 - val_loss: 54.8594 - val_accuracy: 0.3424\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 38.2647 - accuracy: 0.3408 - val_loss: 54.8791 - val_accuracy: 0.3424\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 38.9549 - accuracy: 0.3416 - val_loss: 54.7171 - val_accuracy: 0.3424\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 100s 4s/step - loss: 37.7908 - accuracy: 0.3411 - val_loss: 54.4676 - val_accuracy: 0.3428\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 37.8046 - accuracy: 0.3401 - val_loss: 54.4853 - val_accuracy: 0.3435\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 37.3708 - accuracy: 0.3406 - val_loss: 54.3609 - val_accuracy: 0.3439\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 38.0410 - accuracy: 0.3398 - val_loss: 54.2515 - val_accuracy: 0.3443\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 38.5030 - accuracy: 0.3411 - val_loss: 54.1021 - val_accuracy: 0.3428\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 38.6330 - accuracy: 0.3419 - val_loss: 54.2110 - val_accuracy: 0.3433\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 39.0422 - accuracy: 0.3418 - val_loss: 54.3544 - val_accuracy: 0.3437\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 40.3509 - accuracy: 0.3429 - val_loss: 54.5809 - val_accuracy: 0.3433\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 37.3123 - accuracy: 0.3422 - val_loss: 54.5991 - val_accuracy: 0.3433\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 37.3254 - accuracy: 0.3424 - val_loss: 54.6471 - val_accuracy: 0.3430\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 38.4156 - accuracy: 0.3425 - val_loss: 54.7390 - val_accuracy: 0.3428\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 38.3017 - accuracy: 0.3420 - val_loss: 54.5296 - val_accuracy: 0.3424\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 37.2470 - accuracy: 0.3426 - val_loss: 54.3361 - val_accuracy: 0.3439\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 37.7901 - accuracy: 0.3410 - val_loss: 54.1252 - val_accuracy: 0.3444\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 38.0646 - accuracy: 0.3422 - val_loss: 53.6888 - val_accuracy: 0.3441\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 37.3508 - accuracy: 0.3435 - val_loss: 53.4254 - val_accuracy: 0.3443\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 36.9444 - accuracy: 0.3420 - val_loss: 53.3848 - val_accuracy: 0.3454\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 38.0479 - accuracy: 0.3420 - val_loss: 53.3952 - val_accuracy: 0.3456\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 37.8957 - accuracy: 0.3420 - val_loss: 53.4585 - val_accuracy: 0.3459\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 37.5671 - accuracy: 0.3435 - val_loss: 53.5909 - val_accuracy: 0.3454\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.8245 - accuracy: 0.3433 - val_loss: 53.7275 - val_accuracy: 0.3448\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 39.3870 - accuracy: 0.3423 - val_loss: 53.7982 - val_accuracy: 0.3450\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 37.9243 - accuracy: 0.3448 - val_loss: 54.3295 - val_accuracy: 0.3459\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 37.5000 - accuracy: 0.3435 - val_loss: 55.2680 - val_accuracy: 0.3472\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.8174 - accuracy: 0.3420 - val_loss: 55.3233 - val_accuracy: 0.3467\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 37.6192 - accuracy: 0.3431 - val_loss: 55.2967 - val_accuracy: 0.3465\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.4692 - accuracy: 0.3428 - val_loss: 54.9783 - val_accuracy: 0.3465\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 38.5381 - accuracy: 0.3439 - val_loss: 54.8656 - val_accuracy: 0.3463\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.7611 - accuracy: 0.3430 - val_loss: 54.9049 - val_accuracy: 0.3463\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 37.0271 - accuracy: 0.3438 - val_loss: 54.8042 - val_accuracy: 0.3463\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.6616 - accuracy: 0.3437 - val_loss: 54.5092 - val_accuracy: 0.3457\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 37.9372 - accuracy: 0.3430 - val_loss: 54.1708 - val_accuracy: 0.3454\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 36.9056 - accuracy: 0.3431 - val_loss: 54.0892 - val_accuracy: 0.3441\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 38.7885 - accuracy: 0.3434 - val_loss: 54.0873 - val_accuracy: 0.3437\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.3360 - accuracy: 0.3422 - val_loss: 54.0381 - val_accuracy: 0.3448\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 36.6615 - accuracy: 0.3443 - val_loss: 53.7230 - val_accuracy: 0.3444\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.5656 - accuracy: 0.3435 - val_loss: 53.6461 - val_accuracy: 0.3448\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 37.7328 - accuracy: 0.3430 - val_loss: 53.5336 - val_accuracy: 0.3448\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 36.2880 - accuracy: 0.3434 - val_loss: 53.3660 - val_accuracy: 0.3452\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 36.8102 - accuracy: 0.3435 - val_loss: 53.3288 - val_accuracy: 0.3444\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.4913 - accuracy: 0.3451 - val_loss: 53.2732 - val_accuracy: 0.3444\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.1544 - accuracy: 0.3441 - val_loss: 53.7838 - val_accuracy: 0.3433\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 38.4773 - accuracy: 0.3443 - val_loss: 54.1160 - val_accuracy: 0.3456\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 37.6455 - accuracy: 0.3449 - val_loss: 54.2701 - val_accuracy: 0.3454\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.8123 - accuracy: 0.3455 - val_loss: 54.2005 - val_accuracy: 0.3456\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 37.6632 - accuracy: 0.3447 - val_loss: 54.2203 - val_accuracy: 0.3444\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.7067 - accuracy: 0.3440 - val_loss: 54.1274 - val_accuracy: 0.3448\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 40.0597 - accuracy: 0.3446 - val_loss: 54.0699 - val_accuracy: 0.3456\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 38.3249 - accuracy: 0.3446 - val_loss: 54.0250 - val_accuracy: 0.3459\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.6501 - accuracy: 0.3463 - val_loss: 53.9617 - val_accuracy: 0.3465\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 38.3044 - accuracy: 0.3443 - val_loss: 53.8173 - val_accuracy: 0.3461\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 38.4103 - accuracy: 0.3445 - val_loss: 53.8286 - val_accuracy: 0.3452\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 38.2274 - accuracy: 0.3469 - val_loss: 53.7892 - val_accuracy: 0.3452\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 37.5573 - accuracy: 0.3456 - val_loss: 53.6548 - val_accuracy: 0.3457\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 36.2786 - accuracy: 0.3452 - val_loss: 53.4434 - val_accuracy: 0.3459\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.9036 - accuracy: 0.3452 - val_loss: 53.2570 - val_accuracy: 0.3457\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 35.9276 - accuracy: 0.3444 - val_loss: 53.1236 - val_accuracy: 0.3456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 36.2388 - accuracy: 0.3452 - val_loss: 53.0512 - val_accuracy: 0.3450\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 37.4532 - accuracy: 0.3454 - val_loss: 53.0248 - val_accuracy: 0.3450\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 37.9799 - accuracy: 0.3449 - val_loss: 53.1558 - val_accuracy: 0.3469\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 36.2412 - accuracy: 0.3462 - val_loss: 53.1552 - val_accuracy: 0.3469\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 36.3176 - accuracy: 0.3455 - val_loss: 53.0554 - val_accuracy: 0.3463\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 35.5877 - accuracy: 0.3462 - val_loss: 53.0067 - val_accuracy: 0.3465\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 36.0355 - accuracy: 0.3454 - val_loss: 53.0515 - val_accuracy: 0.3459\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 36.4021 - accuracy: 0.3457 - val_loss: 52.9681 - val_accuracy: 0.3446\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.1917 - accuracy: 0.3460 - val_loss: 52.9769 - val_accuracy: 0.3456\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 36.5668 - accuracy: 0.3448 - val_loss: 53.0046 - val_accuracy: 0.3456\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 35.6040 - accuracy: 0.3454 - val_loss: 52.9613 - val_accuracy: 0.3456\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.0445 - accuracy: 0.3461 - val_loss: 52.9259 - val_accuracy: 0.3454\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.3940 - accuracy: 0.3456 - val_loss: 52.8646 - val_accuracy: 0.3456\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 35.6352 - accuracy: 0.3458 - val_loss: 52.9383 - val_accuracy: 0.3446\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.8904 - accuracy: 0.3445 - val_loss: 53.1301 - val_accuracy: 0.3443\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 36.9446 - accuracy: 0.3470 - val_loss: 53.6370 - val_accuracy: 0.3446\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 36.1866 - accuracy: 0.3468 - val_loss: 53.9771 - val_accuracy: 0.3448\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 36.9114 - accuracy: 0.3456 - val_loss: 54.4493 - val_accuracy: 0.3478\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 36.7748 - accuracy: 0.3464 - val_loss: 54.6778 - val_accuracy: 0.3515\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.1205 - accuracy: 0.3454 - val_loss: 54.4497 - val_accuracy: 0.3544\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 36.4535 - accuracy: 0.3463 - val_loss: 54.2819 - val_accuracy: 0.3524\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.9564 - accuracy: 0.3455 - val_loss: 54.1401 - val_accuracy: 0.3522\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 37.5868 - accuracy: 0.3446 - val_loss: 54.1348 - val_accuracy: 0.3498\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.1825 - accuracy: 0.3451 - val_loss: 54.1656 - val_accuracy: 0.3496\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.5098 - accuracy: 0.3458 - val_loss: 54.1865 - val_accuracy: 0.3496\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 36.9744 - accuracy: 0.3465 - val_loss: 54.1758 - val_accuracy: 0.3487\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 36.7874 - accuracy: 0.3471 - val_loss: 53.9932 - val_accuracy: 0.3494\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 35.7368 - accuracy: 0.3455 - val_loss: 53.7376 - val_accuracy: 0.3494\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.8032 - accuracy: 0.3466 - val_loss: 53.6830 - val_accuracy: 0.3483\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 37.4600 - accuracy: 0.3440 - val_loss: 53.6599 - val_accuracy: 0.3474\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 36.8209 - accuracy: 0.3463 - val_loss: 53.8229 - val_accuracy: 0.3467\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.5678 - accuracy: 0.3465 - val_loss: 54.0086 - val_accuracy: 0.3452\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 35.7655 - accuracy: 0.3458 - val_loss: 54.0773 - val_accuracy: 0.3448\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 36.0365 - accuracy: 0.3459 - val_loss: 54.0466 - val_accuracy: 0.3450\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 35.9831 - accuracy: 0.3467 - val_loss: 54.0049 - val_accuracy: 0.3448\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.3729 - accuracy: 0.3467 - val_loss: 53.8941 - val_accuracy: 0.3465\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 98s 3s/step - loss: 36.3413 - accuracy: 0.3466 - val_loss: 53.6232 - val_accuracy: 0.3472\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 35.6334 - accuracy: 0.3469 - val_loss: 53.4851 - val_accuracy: 0.3469\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.5215 - accuracy: 0.3473 - val_loss: 53.4065 - val_accuracy: 0.3470\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 36.4185 - accuracy: 0.3481 - val_loss: 53.1062 - val_accuracy: 0.3485\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 36.5833 - accuracy: 0.3481 - val_loss: 52.8794 - val_accuracy: 0.3502\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 36.4903 - accuracy: 0.3478 - val_loss: 52.8405 - val_accuracy: 0.3509\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.8096 - accuracy: 0.3484 - val_loss: 52.8724 - val_accuracy: 0.3498\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.0696 - accuracy: 0.3486 - val_loss: 52.7925 - val_accuracy: 0.3507\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 35.4618 - accuracy: 0.3477 - val_loss: 52.8130 - val_accuracy: 0.3498\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.9857 - accuracy: 0.3483 - val_loss: 52.9617 - val_accuracy: 0.3507\n",
      "Epoch 160/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 36.3122 - accuracy: 0.3489 - val_loss: 53.0712 - val_accuracy: 0.3515\n",
      "Epoch 161/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 35.4067 - accuracy: 0.3476 - val_loss: 53.0696 - val_accuracy: 0.3520\n",
      "Epoch 162/200\n",
      "28/28 [==============================] - 98s 4s/step - loss: 35.5375 - accuracy: 0.3489 - val_loss: 53.3123 - val_accuracy: 0.3522\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 35.4389 - accuracy: 0.3488 - val_loss: 53.5291 - val_accuracy: 0.3526\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 35.4758 - accuracy: 0.3457 - val_loss: 53.5890 - val_accuracy: 0.3522\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 35.6469 - accuracy: 0.3488 - val_loss: 53.5799 - val_accuracy: 0.3513\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 35.5100 - accuracy: 0.3483 - val_loss: 53.3047 - val_accuracy: 0.3511\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 35.8276 - accuracy: 0.3479 - val_loss: 53.1872 - val_accuracy: 0.3507\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 35.3951 - accuracy: 0.3504 - val_loss: 52.8930 - val_accuracy: 0.3504\n",
      "Epoch 169/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.2099 - accuracy: 0.3483 - val_loss: 52.7410 - val_accuracy: 0.3500\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 100s 4s/step - loss: 34.9123 - accuracy: 0.3495 - val_loss: 52.8003 - val_accuracy: 0.3500\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 34.9833 - accuracy: 0.3499 - val_loss: 52.9131 - val_accuracy: 0.3511\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.5044 - accuracy: 0.3484 - val_loss: 52.9215 - val_accuracy: 0.3507\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.2121 - accuracy: 0.3493 - val_loss: 52.8753 - val_accuracy: 0.3504\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.6083 - accuracy: 0.3489 - val_loss: 52.7838 - val_accuracy: 0.3504\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 34.8114 - accuracy: 0.3481 - val_loss: 52.8453 - val_accuracy: 0.3509\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.9940 - accuracy: 0.3483 - val_loss: 52.8064 - val_accuracy: 0.3496\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 35.5517 - accuracy: 0.3487 - val_loss: 52.9179 - val_accuracy: 0.3500\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 35.1268 - accuracy: 0.3503 - val_loss: 52.9981 - val_accuracy: 0.3496\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 34.3767 - accuracy: 0.3493 - val_loss: 53.0607 - val_accuracy: 0.3489\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 104s 4s/step - loss: 35.1612 - accuracy: 0.3481 - val_loss: 52.9408 - val_accuracy: 0.3483\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 35.1799 - accuracy: 0.3493 - val_loss: 52.9431 - val_accuracy: 0.3478\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.2003 - accuracy: 0.3499 - val_loss: 52.9381 - val_accuracy: 0.3483\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 34.6923 - accuracy: 0.3485 - val_loss: 52.8365 - val_accuracy: 0.3480\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 35.5556 - accuracy: 0.3486 - val_loss: 53.0321 - val_accuracy: 0.3485\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 34.5369 - accuracy: 0.3486 - val_loss: 53.1302 - val_accuracy: 0.3485\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 34.9921 - accuracy: 0.3496 - val_loss: 52.9751 - val_accuracy: 0.3481\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 34.7912 - accuracy: 0.3496 - val_loss: 52.8255 - val_accuracy: 0.3489\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.8040 - accuracy: 0.3491 - val_loss: 52.9738 - val_accuracy: 0.3500\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 35.4827 - accuracy: 0.3487 - val_loss: 53.1463 - val_accuracy: 0.3500\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 35.1394 - accuracy: 0.3483 - val_loss: 53.4677 - val_accuracy: 0.3498\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 34.8524 - accuracy: 0.3496 - val_loss: 53.5772 - val_accuracy: 0.3489\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 103s 4s/step - loss: 35.8589 - accuracy: 0.3483 - val_loss: 53.7029 - val_accuracy: 0.3496\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 97s 3s/step - loss: 34.9892 - accuracy: 0.3489 - val_loss: 53.5440 - val_accuracy: 0.3494\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 34.8468 - accuracy: 0.3502 - val_loss: 53.3259 - val_accuracy: 0.3502\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 102s 4s/step - loss: 34.6947 - accuracy: 0.3493 - val_loss: 53.1039 - val_accuracy: 0.3500\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 34.8693 - accuracy: 0.3505 - val_loss: 52.9239 - val_accuracy: 0.3502\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 100s 4s/step - loss: 37.4244 - accuracy: 0.3496 - val_loss: 53.0228 - val_accuracy: 0.3500\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 96s 3s/step - loss: 35.3830 - accuracy: 0.3510 - val_loss: 53.1712 - val_accuracy: 0.3500\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 99s 4s/step - loss: 35.0767 - accuracy: 0.3502 - val_loss: 53.1489 - val_accuracy: 0.3487\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 101s 4s/step - loss: 35.5630 - accuracy: 0.3497 - val_loss: 53.1598 - val_accuracy: 0.3489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa47cb3d6a0>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_2.fit(X_train ,Y_train, epochs= 200, batch_size = 4, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15, 15, 3, 10)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_3 = my_model_2.predict(XXX)\n",
    "pred_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19988801, 0.13472565, 0.22146013, 0.169499  , 0.00538603,\n",
       "        0.02993557, 0.43830889, 0.04455022, 0.57029241, 0.03293376],\n",
       "       [0.13593299, 0.19317172, 0.41259327, 0.06906526, 0.01900874,\n",
       "        0.049185  , 0.30527437, 0.55680817, 0.06997597, 0.01398845],\n",
       "       [0.1519369 , 0.18540917, 0.38039782, 0.28331454, 0.94108653,\n",
       "        0.99037021, 0.00797337, 0.00147398, 0.00673689, 0.00604247]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1 = np.zeros((675,10))\n",
    "out_1 = np.reshape(out_1, [1,15,15,3,10])\n",
    "\n",
    "out_1[..., 0:2] = my_sigmoid(pred_3[..., 0:2]) + grid_final\n",
    "out_1[..., 0:2] = (out_1[..., 0:2] * grid_stride) / 480.\n",
    "out_1[..., 2:4] = np.exp(pred_3[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "out_1[..., 4:5] = my_sigmoid(pred_3[..., 4:5])\n",
    "out_1[..., 5:] = my_sigmoid(pred_3[..., 5:])\n",
    "\n",
    "out_1[0,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.17067307, 0.16230366, 0.33653846, 0.3036649 , 1.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3][2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59855765, 0.40052354, 0.1826923 , 0.13089006, 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[3][6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46658366, 0.53344806, 0.19101982, 0.14533059, 0.87939739,\n",
       "        0.01671863, 0.06185059, 0.02398217, 0.95252073, 0.00713944],\n",
       "       [0.44096431, 0.58441422, 0.33026338, 0.0944004 , 0.0118835 ,\n",
       "        0.00960593, 0.19067493, 0.08527617, 0.52204525, 0.0124633 ],\n",
       "       [0.40352687, 0.59840792, 0.56907531, 0.50901692, 0.00310088,\n",
       "        0.70681721, 0.04354021, 0.03912443, 0.02527317, 0.8524372 ]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_1[0,6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_4 = my_model_2.predict(X_train)\n",
    "\n",
    "out_2 = np.zeros((28, 6750))\n",
    "out_2 = np.reshape(out_2, [28,15,15,3,10])\n",
    "\n",
    "out_2[..., 0:2] = my_sigmoid(pred_4[..., 0:2]) + grid_final\n",
    "out_2[..., 0:2] = (out_2[..., 0:2] * grid_stride) / 480.\n",
    "out_2[..., 2:4] = np.exp(pred_4[..., 2:4]) * (anchors_wrt_target / 480.)\n",
    "out_2[..., 4:5] = my_sigmoid(pred_4[..., 4:5])\n",
    "out_2[..., 5:] = my_sigmoid(pred_4[..., 5:])\n",
    "\n",
    "obj_mask = Y_train[..., 4:5]\n",
    "\n",
    "xy_arr_1 = 5 * np.power((Y_train[..., 0:2] - out_2[...,0:2]),2)\n",
    "    \n",
    "xy_loss_1 = np.sum(xy_arr * obj_mask)\n",
    "\n",
    "xy_loss_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
